{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database creations using pytorch Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageQualityDataset(Dataset):\n",
    "    \"\"\"Dataset for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its labels by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (image, labels) where:\n",
    "                image (PIL.Image): The image.\n",
    "                labels (torch.Tensor): Tensor containing quality and authenticity scores.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(os.getcwd(), self.data.iloc[idx, 3])  # image_path column\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        quality = self.data.iloc[idx, 0]  # Quality column\n",
    "        authenticity = self.data.iloc[idx, 1]  # Authenticity column\n",
    "        labels = torch.tensor([quality, authenticity], dtype=torch.float)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    \"\"\"VGG16 model for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, num_outputs=2):\n",
    "        \"\"\"\n",
    "        Initializes the VGG16 model.\n",
    "\n",
    "        Args:\n",
    "            num_outputs (int): Number of output features. Defaults to 2 (quality and authenticity).\n",
    "        \"\"\"\n",
    "        super(VGG16, self).__init__()\n",
    "        # Load pre-trained VGG16 model\n",
    "        self.vgg16 = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "\n",
    "        # Freeze all layers\n",
    "        for param in self.vgg16.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Modify the classifier\n",
    "        num_features = self.vgg16.classifier[6].in_features\n",
    "        self.vgg16.classifier = nn.Sequential(\n",
    "            *list(self.vgg16.classifier.children())[:-1],  # Remove last layer with 1000 outputs\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_outputs)  # Add new layer with num_out outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.vgg16(x)\n",
    "    \n",
    "class QualityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in vgg.features.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Extract features up to fc2\n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        self.fc1 = vgg.classifier[:-1]  # Up to fc2 (4096 -> 128)\n",
    "        \n",
    "        # New regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2)  # Predict quality and realness\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        features = self.fc1(x)\n",
    "        predictions = self.regression_head(features)\n",
    "        return predictions, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility funcitons for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Trains the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloaders (dict): A dictionary containing the training and validation data loaders.\n",
    "        criterion (nn.Module): The loss function.\n",
    "        optimizer (optim.Optimizer): The optimizer.\n",
    "        num_epochs (int): Number of epochs to train for. Defaults to 10.\n",
    "        device (str): Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The trained model.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:  # Iterate over training and validation phases\n",
    "            print(f'{phase} phase')\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:  # Iterate over data in the current phase\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):  # Enable gradients only during training\n",
    "                    outputs, _ = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}') # Print loss for the current phase\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return model\n",
    "\n",
    "def test_model(model, dataloader, criterion, device='cuda'):\n",
    "\n",
    "    \"\"\"\n",
    "    Tests the model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        dataloader (DataLoader): The test data loader.\n",
    "        criterion (nn.Module): The loss function.\n",
    "        device (str): Device to use for testing ('cuda' or 'cpu'). Defaults to 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss on the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs, _ = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    test_loss = running_loss / len(dataloader.dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    return test_loss\n",
    "\n",
    "def get_predictions(model, dataloader, device)-> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Get predictions from the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        dataloader (DataLoader): The data loader.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (predictions, labels) where:\n",
    "            predictions (torch.Tensor): Predictions from the model.\n",
    "            labels (torch.Tensor): Ground truth labels.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, target in dataloader:\n",
    "            outputs, _ = model(inputs.to(device))\n",
    "            predictions.append(outputs)\n",
    "            labels.append(target)\n",
    "\n",
    "    #move to cpu and concatenate\n",
    "    predictions = torch.cat(predictions).cpu()\n",
    "    labels = torch.cat(labels).cpu()\n",
    "\n",
    "    return predictions, labels\n",
    "\n",
    "def get_regression_errors(tuple: tuple[torch.Tensor, torch.Tensor]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Get regression errors.\n",
    "\n",
    "    Args:\n",
    "        tuple: A tuple (predictions, labels) where:\n",
    "            predictions (torch.Tensor): Predictions from the model.\n",
    "            labels (torch.Tensor): Ground truth labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (quality_errors, authenticity_errors) where:\n",
    "            quality_errors (torch.Tensor): Quality errors.\n",
    "            authenticity_errors (torch.Tensor): Authenticity errors.\n",
    "    \"\"\"\n",
    "    predictions, labels = tuple\n",
    "    quality_errors = predictions[:, 0] - labels[:, 0]\n",
    "    authenticity_errors = predictions[:, 1] - labels[:, 1]\n",
    "    return quality_errors, authenticity_errors\n",
    "\n",
    "def get_rmse(errors: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get the root mean squared error.\n",
    "\n",
    "    Args:\n",
    "        errors (torch.Tensor): Errors.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Root mean squared error.\n",
    "    \"\"\"\n",
    "    return torch.sqrt(torch.mean(errors ** 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations for the ImageNet dataset\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "annotations_file = 'Dataset/AIGCIQA2023/mos_data.csv'\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageQualityDataset(csv_file=annotations_file, transform=data_transforms)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Create a dictionary containing the data loaders\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader,\n",
    "    'test': test_dataloader\n",
    "}\n",
    "\n",
    "model = QualityPredictor()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss (regression)\n",
    "optimizer = optim.Adam(model.regression_head.parameters(), lr=0.001)\n",
    "\n",
    "model_path = 'Models/VGG-16_finetuned_regression.pth'\n",
    "\n",
    "quality_predictor_trained= train_model(model, dataloaders, criterion, optimizer, EPOCHS, device)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(quality_predictor_trained.state_dict(), model_path)\n",
    "\n",
    "# Load the trained model\n",
    "quality_predictor_trained = QualityPredictor()\n",
    "quality_predictor_trained.load_state_dict(torch.load(model_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities for pruning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_map_importance(model, dataloader, device, layer_name) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Computes the importance of each feature map in a convolution\n",
    "    layer by measuring the change in predictions when the feature map is zero\n",
    "    out.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (indices, importance_scores) where both are numpy arrays\n",
    "    \"\"\"\n",
    "    #if importance_scores.npy exists, load it\n",
    "    if os.path.exists('importance_scores.npy'):\n",
    "        return np.load('importance_scores.npy')\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    importance_scores = []\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    baseline_predictions = get_predictions(model, dataloader, device)\n",
    "    regression_errors = get_regression_errors(baseline_predictions)\n",
    "    quality_errors, authenticity_errors = regression_errors\n",
    "    baseline_quality_rmse = get_rmse(quality_errors)\n",
    "    baseline_authenticity_rmse = get_rmse(authenticity_errors)\n",
    "    average_baseline_rmse = (baseline_quality_rmse + baseline_authenticity_rmse) / 2\n",
    "\n",
    "    \n",
    "    print(f'Average baseline RMSE: {average_baseline_rmse:.4f}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(layer.out_channels):\n",
    "            # Create a backup of the weights and bias\n",
    "            backup_weights = layer.weight[i, ...].clone()\n",
    "            backup_bias = layer.bias[i].clone() if layer.bias is not None else None\n",
    "\n",
    "            # Zero out the i-th output channel\n",
    "            layer.weight[i, ...] = 0\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[i] = 0\n",
    "\n",
    "            # Get predictions with the pruned feature map\n",
    "            pruned_predictions = get_predictions(model, dataloader, device)\n",
    "            pruned_regression_errors = get_regression_errors(pruned_predictions)\n",
    "            pruned_quality_errors, pruned_authenticity_errors = pruned_regression_errors\n",
    "            pruned_quality_rmse = get_rmse(pruned_quality_errors)\n",
    "            pruned_authenticity_rmse = get_rmse(pruned_authenticity_errors)\n",
    "            average_pruned_rmse = (pruned_quality_rmse + pruned_authenticity_rmse) / 2\n",
    "    \n",
    "            # Compute importance score\n",
    "            importance_score = average_baseline_rmse - average_pruned_rmse\n",
    "            importance_scores.append([i, importance_score])\n",
    "            \n",
    "\n",
    "            print(f'Feature map {i}: Importance score: {importance_score:.4f}')\n",
    "            \n",
    "            # After computing importance, restore weights and bias\n",
    "            layer.weight[i, ...] = backup_weights\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[i] = backup_bias \n",
    "\n",
    "    sorted_importance_scores = sorted(importance_scores, key=lambda x: x[1], reverse=True)\n",
    "    # save np array \n",
    "    np.save('importance_scores.npy', sorted_importance_scores)\n",
    "    return np.array(sorted_importance_scores)\n",
    "\n",
    "def find_optimal_feature_subset(model, dataloader, device, layer_name, sorted_importance_scores, model_path='Models/pruned_model.pth'):\n",
    "    \"\"\"\n",
    "    Find an optimal subset of feature maps by iteratively adding features in order of importance\n",
    "    and tracking model performance, keeping the subset that maximizes performance.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        dataloader: DataLoader for evaluation\n",
    "        device: Device to run the model on (cuda/cpu)\n",
    "        layer_name: Name of the layer to optimize\n",
    "        sorted_importance_scores: List of tuples (channel_index, importance_score) sorted by importance\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with optimal subset and performance metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Reverse the sorted importance scores\n",
    "    sorted_importance_scores = sorted_importance_scores[::-1]\n",
    "    \n",
    "    # Get the target layer\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    \n",
    "    # Create a backup of the original weights and bias\n",
    "    original_weights = layer.weight.clone()\n",
    "    original_bias = layer.bias.clone() if layer.bias is not None else None\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_rmse = float('inf')\n",
    "    best_subset = []\n",
    "    rmse_history = []\n",
    "    current_subset = []\n",
    "    \n",
    "    # Get baseline with no features (all zeroed out)\n",
    "    layer.weight.data.fill_(0)\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.fill_(0)\n",
    "        \n",
    "    baseline_predictions = get_predictions(model, dataloader, device)\n",
    "    baseline_regression_errors = get_regression_errors(baseline_predictions)\n",
    "    baseline_quality_errors, baseline_authenticity_errors = baseline_regression_errors\n",
    "    baseline_quality_rmse = get_rmse(baseline_quality_errors)\n",
    "    baseline_authenticity_rmse = get_rmse(baseline_authenticity_errors)\n",
    "    baseline_rmse = (baseline_quality_rmse + baseline_authenticity_rmse) / 2\n",
    "    \n",
    "    print(f\"Baseline RMSE (no features): {baseline_rmse:.4f}\")\n",
    "    print(\"------------------\")\n",
    "    \n",
    "    # Track performance with empty set\n",
    "    rmse_history.append(([], baseline_rmse))\n",
    "    \n",
    "    # Iteratively add feature maps in order of importance\n",
    "    for idx, (channel_idx, _) in enumerate(sorted_importance_scores):\n",
    "        channel_idx = int(channel_idx)\n",
    "        \n",
    "        # Add this feature map to the current subset\n",
    "        current_subset.append(channel_idx)\n",
    "        \n",
    "        # Reset all weights to zero first\n",
    "        layer.weight.data.fill_(0)\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0)\n",
    "        \n",
    "        # Enable only the feature maps in the current subset\n",
    "        for ch_idx in current_subset:\n",
    "            layer.weight[ch_idx, ...] = original_weights[ch_idx, ...]\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[ch_idx] = original_bias[ch_idx]\n",
    "        \n",
    "        # Evaluate model with current subset\n",
    "        predictions = get_predictions(model, dataloader, device)\n",
    "        regression_errors = get_regression_errors(predictions)\n",
    "        quality_errors, authenticity_errors = regression_errors\n",
    "        quality_rmse = get_rmse(quality_errors)\n",
    "        authenticity_rmse = get_rmse(authenticity_errors)\n",
    "        current_rmse = (quality_rmse + authenticity_rmse) / 2\n",
    "        \n",
    "        # Record performance\n",
    "        rmse_history.append((current_subset.copy(), current_rmse))\n",
    "        \n",
    "        print(f\"Iteration {idx+1}/{len(sorted_importance_scores)}: \" +\n",
    "              f\"Added channel {channel_idx}, \" +\n",
    "              f\"Subset size: {len(current_subset)}, \" +\n",
    "              f\"RMSE: {current_rmse:.4f}\")\n",
    "        \n",
    "        # Update best subset if this one is better\n",
    "        if current_rmse < best_rmse:\n",
    "            best_rmse = current_rmse\n",
    "            best_subset = current_subset.copy()\n",
    "            print(f\"  ✓ New best subset found! RMSE: {best_rmse:.4f}\")\n",
    "    \n",
    "    print(\"\\n------------------\")\n",
    "    print(f\"Best RMSE: {best_rmse:.4f} with {len(best_subset)} features\")\n",
    "    print(f\"Improvement over baseline: {baseline_rmse - best_rmse:.4f}\")\n",
    "    print(f\"Feature reduction: {(1 - len(best_subset)/len(sorted_importance_scores))*100:.1f}%\")\n",
    "    \n",
    "    # Apply the best subset to the model\n",
    "    layer.weight.data.fill_(0)\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.fill_(0)\n",
    "        \n",
    "    for ch_idx in best_subset:\n",
    "        layer.weight[ch_idx, ...] = original_weights[ch_idx, ...]\n",
    "        if layer.bias is not None:\n",
    "            layer.bias[ch_idx] = original_bias[ch_idx]\n",
    "    \n",
    "    # Save the pruned model\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Restore original weights for future use\n",
    "    layer.weight.data.copy_(original_weights)\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.copy_(original_bias)\n",
    "    \n",
    "    return {\n",
    "        'best_subset': best_subset,\n",
    "        'best_rmse': best_rmse,\n",
    "        'baseline_rmse': baseline_rmse,\n",
    "        'improvement': baseline_rmse - best_rmse,\n",
    "        'reduction_percentage': (1 - len(best_subset)/len(sorted_importance_scores))*100,\n",
    "        'rmse_history': rmse_history\n",
    "    }\n",
    "\n",
    "def remove_noisy_feature_maps(model, dataloader, device, layer_name, sorted_importance_scores, model_path='Models/pruned_model.pth'):\n",
    "    \"\"\"\n",
    "    Remove noisy feature maps from a convolutional layer based on importance scores.\n",
    "    Feature maps are zeroed out one by one and kept zeroed only if model performance improves.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        dataloader: DataLoader for evaluation\n",
    "        device: Device to run the model on (cuda/cpu)\n",
    "        layer_name: Name of the layer to optimize\n",
    "        sorted_importance_scores: List of tuples (channel_index, importance_score) sorted by importance\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with pruning results and performance metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Get the target layer\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    \n",
    "    # Create a backup of the original weights and bias\n",
    "    original_weights = layer.weight.clone()\n",
    "    original_bias = layer.bias.clone() if layer.bias is not None else None\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    removed_features = []\n",
    "    rmse_history = []\n",
    "    \n",
    "    # Get baseline performance\n",
    "    baseline_predictions = get_predictions(model, dataloader, device)\n",
    "    baseline_regression_errors = get_regression_errors(baseline_predictions)\n",
    "    baseline_quality_errors, baseline_authenticity_errors = baseline_regression_errors\n",
    "    baseline_quality_rmse = get_rmse(baseline_quality_errors)\n",
    "    baseline_authenticity_rmse = get_rmse(baseline_authenticity_errors)\n",
    "    average_baseline_rmse = (baseline_quality_rmse + baseline_authenticity_rmse) / 2\n",
    "    \n",
    "    print(f\"Baseline RMSE: {average_baseline_rmse:.4f}\")\n",
    "    print(\"------------------\")\n",
    "    \n",
    "    # Track initial performance\n",
    "    rmse_history.append(([], average_baseline_rmse))\n",
    "    baseline_rmse = average_baseline_rmse\n",
    "    \n",
    "    # Iterate over the sorted indices and if removing a feature map improves performance, keep it removed\n",
    "    for idx, (channel_idx, importance_score) in enumerate(sorted_importance_scores):\n",
    "        channel_idx = int(channel_idx)\n",
    "        \n",
    "        # Temporarily zero out this feature map\n",
    "        layer.weight[channel_idx, ...] = 0\n",
    "        if layer.bias is not None:\n",
    "            layer.bias[channel_idx] = 0\n",
    "        \n",
    "        # Evaluate model with feature map removed\n",
    "        predictions = get_predictions(model, dataloader, device)\n",
    "        regression_errors = get_regression_errors(predictions)\n",
    "        quality_errors, authenticity_errors = regression_errors\n",
    "        quality_rmse = get_rmse(quality_errors)\n",
    "        authenticity_rmse = get_rmse(authenticity_errors)\n",
    "        average_new_rmse = (quality_rmse + authenticity_rmse) / 2\n",
    "        \n",
    "        print(f\"Iteration {idx+1}/{len(sorted_importance_scores)}: \" +\n",
    "              f\"Testing removal of channel {channel_idx}, \" +\n",
    "              f\"Importance: {importance_score:.4f}, \" +\n",
    "              f\"RMSE: {average_new_rmse:.4f}\")\n",
    "        \n",
    "        # Decide whether to keep this feature map removed\n",
    "        if average_new_rmse < baseline_rmse:\n",
    "            baseline_rmse = average_new_rmse if average_new_rmse < average_baseline_rmse else average_baseline_rmse\n",
    "            removed_features.append(channel_idx)\n",
    "            rmse_history.append((removed_features.copy(), baseline_rmse))\n",
    "            print(f\"  ✓ IMPROVING: Zeroing out feature map {channel_idx}\")\n",
    "        else:\n",
    "            # Restore the feature map\n",
    "            layer.weight[channel_idx, ...] = original_weights[channel_idx, ...]\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[channel_idx] = original_bias[channel_idx]\n",
    "            print(f\"  ✗ NOT IMPROVING: Keeping feature map {channel_idx}\")\n",
    "        \n",
    "        print(f\"  Current best RMSE: {baseline_rmse:.4f}\")\n",
    "        print(\"------------------\")\n",
    "    \n",
    "    # Final statistics\n",
    "    print(\"\\n------------------\")\n",
    "    print(f\"Final RMSE: {baseline_rmse:.4f} after removing {len(removed_features)} feature maps\")\n",
    "    print(f\"Improvement over baseline: {average_baseline_rmse - baseline_rmse:.4f}\")\n",
    "    print(f\"Feature reduction: {(len(removed_features)/len(sorted_importance_scores))*100:.1f}%\")\n",
    "    \n",
    "    # Save the pruned model\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    return {\n",
    "        'removed_features': removed_features,\n",
    "        'baseline_rmse': average_baseline_rmse,\n",
    "        'final_rmse': baseline_rmse,\n",
    "        'improvement': average_baseline_rmse - baseline_rmse,\n",
    "        'reduction_percentage': (len(removed_features)/len(sorted_importance_scores))*100,\n",
    "        'rmse_history': rmse_history\n",
    "    }\n",
    "\n",
    "def remove_negative_impact_feature_maps(model, dataloader, device, layer_name, sorted_importance_scores, model_path='Models/negative_impact_pruned_model.pth'):\n",
    "    \"\"\"\n",
    "    Remove feature maps that have a negative impact on model performance based on importance scores (impotance score < 0).\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        dataloader: DataLoader for evaluation\n",
    "        device: Device to run the model on (cuda/cpu)\n",
    "        layer_name: Name of the layer to optimize\n",
    "        sorted_importance_scores: List of tuples (channel_index, importance_score) sorted by importance\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with pruning results and performance metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Get the target layer\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    \n",
    "    # Create a backup of the original weights and bias\n",
    "    original_weights = layer.weight.clone()\n",
    "    original_bias = layer.bias.clone() if layer.bias is not None else None\n",
    "    \n",
    "    # Get baseline performance\n",
    "    predictions = get_predictions(model, dataloader, device)\n",
    "    regression_errors = get_regression_errors(predictions)\n",
    "    quality_errors, authenticity_errors = regression_errors\n",
    "    quality_rmse = get_rmse(quality_errors)\n",
    "    authenticity_rmse = get_rmse(authenticity_errors)\n",
    "    baseline_rmse = (quality_rmse + authenticity_rmse) / 2\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    removed_features = []\n",
    "    \n",
    "    # Iterate over the sorted indices and zero out all the feature maps that have a negative impact (importance < 0)\n",
    "\n",
    "    for idx, (channel_idx, importance_score) in enumerate(sorted_importance_scores):\n",
    "        print(f\"Iteration {idx} - Channel {channel_idx}: Importance score: {importance_score:.4f}\")\n",
    "        if importance_score > 0:\n",
    "            channel_idx = int(channel_idx)\n",
    "            layer.weight[channel_idx, ...] = 0\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[channel_idx] = 0\n",
    "            removed_features.append(channel_idx)\n",
    "\n",
    "    # Evaluate model with feature maps removed\n",
    "    new_predictions = get_predictions(model, dataloader, device)\n",
    "    new_regression_errors = get_regression_errors(new_predictions)\n",
    "    new_quality_errors, new_authenticity_errors = new_regression_errors\n",
    "    new_quality_rmse = get_rmse(new_quality_errors)\n",
    "    new_authenticity_rmse = get_rmse(new_authenticity_errors)\n",
    "    new_rmse = (new_quality_rmse + new_authenticity_rmse) / 2\n",
    "\n",
    "    # Save the pruned model\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # Restore original weights for future use\n",
    "    layer.weight.data.copy_(original_weights)\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.copy_(original_bias)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'removed_features': removed_features,\n",
    "        'baseline_rmse': baseline_rmse,\n",
    "        'final_rmse': new_rmse,\n",
    "        'improvement': baseline_rmse - new_rmse,\n",
    "        'reduction_percentage': (len(removed_features)/len(sorted_importance_scores))*100\n",
    "    }\n",
    "\n",
    "def remove_channels(model,device,layer_name,channels_indexes)->QualityPredictor:\n",
    "    \"\"\"\n",
    "    Remove channels, using an index list, from a convolutional layer in a model.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        device: Device to run the model on (cuda/cpu)\n",
    "        layer_name: Name of the layer to optimize\n",
    "        channels_indexes: List of channel indexes to remove\n",
    "        \n",
    "    Returns:\n",
    "        The pruned model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Get the target layer\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    \n",
    "    # Create a backup of the original weights and bias\n",
    "    original_weights = layer.weight.clone()\n",
    "    original_bias = layer.bias.clone() if layer.bias is not None else None\n",
    "    \n",
    "    # Zero out the specified channels\n",
    "    for channel_idx in channels_indexes:\n",
    "        layer.weight[channel_idx, ...] = 0\n",
    "        if layer.bias is not None:\n",
    "            layer.bias[channel_idx] = 0\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of diffrent models using different pruning techniques\n",
    "\n",
    "- Deletion of models is due to make sure that im not using the same model again and again (first draft, not sure if im correctlly restoring weights in each pruning technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYER to prune\n",
    "LAYER = 'features.28'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# Base model for importance score computation\n",
    "base_model = QualityPredictor()\n",
    "base_model.load_state_dict(torch.load('Models/VGG-16_finetuned_regression.pth'))\n",
    "base_model.eval()\n",
    "base_model.to(DEVICE)\n",
    "\n",
    "sorted_importance_scores = compute_feature_map_importance(base_model, train_dataloader, DEVICE, LAYER)\n",
    "del base_model\n",
    "\n",
    "# Model for noisy feature maps removal\n",
    "noisy_pruning_model = QualityPredictor()\n",
    "noisy_pruning_model.load_state_dict(torch.load('Models/VGG-16_finetuned_regression.pth'))\n",
    "noisy_pruning_model.eval()\n",
    "noisy_pruning_model.to(DEVICE)\n",
    "\n",
    "noisy_optimal_subset = remove_noisy_feature_maps(noisy_pruning_model, train_dataloader, DEVICE, LAYER, sorted_importance_scores, model_path='Models/noise_out_pruned_model.pth')\n",
    "\n",
    "del noisy_pruning_model\n",
    "\n",
    "# Model for optimal subset selection\n",
    "optimal_subset_model = QualityPredictor()\n",
    "optimal_subset_model.load_state_dict(torch.load('Models/VGG-16_finetuned_regression.pth'))\n",
    "optimal_subset_model.eval()\n",
    "optimal_subset_model.to(DEVICE)\n",
    "\n",
    "# Find the optimal subset of feature maps\n",
    "optimal_subset = find_optimal_feature_subset(optimal_subset_model, train_dataloader, DEVICE, LAYER, sorted_importance_scores, model_path='Models/optimal_set_pruned_model.pth')\n",
    "\n",
    "# Model for negative impact feature maps removal\n",
    "negative_impact_model = QualityPredictor()\n",
    "negative_impact_model.load_state_dict(torch.load('Models/VGG-16_finetuned_regression.pth'))\n",
    "negative_impact_model.eval()\n",
    "negative_impact_model.to(DEVICE)\n",
    "\n",
    "negative_impact_subset = remove_negative_impact_feature_maps(negative_impact_model, train_dataloader, DEVICE, LAYER, sorted_importance_scores, model_path='Models/negative_impact_pruned_model.pth')\n",
    "\n",
    "del negative_impact_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with already saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISY_PRUNED_MODEL_PATH = 'Models/noise_out_pruned_model.pth'\n",
    "BEST_SUBSET_PRUNED_MODEL_PATH = 'Models/optimal_set_pruned_model.pth'\n",
    "NEGATIVE_IMPACT_PRUNED_MODEL_PATH = 'Models/negative_impact_pruned_model.pth'\n",
    "\n",
    "noisy_pruned_model = QualityPredictor()\n",
    "noisy_pruned_model.load_state_dict(torch.load(NOISY_PRUNED_MODEL_PATH, weights_only=True))\n",
    "\n",
    "best_subset_pruned_model = QualityPredictor()\n",
    "best_subset_pruned_model.load_state_dict(torch.load(BEST_SUBSET_PRUNED_MODEL_PATH, weights_only=True))\n",
    "\n",
    "negative_impact_pruned_model = QualityPredictor()\n",
    "negative_impact_pruned_model.load_state_dict(torch.load(NEGATIVE_IMPACT_PRUNED_MODEL_PATH,weights_only=True))\n",
    "\n",
    "baseline_model = QualityPredictor()\n",
    "baseline_model.load_state_dict(torch.load('Models/VGG-16_finetuned_regression.pth',weights_only=True))\n",
    "\n",
    "# Testing\n",
    "\n",
    "# Test the baseline model\n",
    "print(\"Testing the baseline model\")\n",
    "test_model(baseline_model, test_dataloader, criterion, device)\n",
    "print(\"------------------\")\n",
    "# test the noisy pruned model\n",
    "print(\"Testing the noisy pruned model\")\n",
    "test_model(noisy_pruned_model, test_dataloader, criterion, device)\n",
    "print(\"------------------\")\n",
    "# test the best subset pruned model\n",
    "print(\"Testing the best subset pruned model\")\n",
    "test_model(best_subset_pruned_model, test_dataloader, criterion, device)\n",
    "print(\"------------------\")\n",
    "# test the negative impact pruned model\n",
    "print(\"Testing the negative impact pruned model\")\n",
    "test_model(negative_impact_pruned_model, test_dataloader, criterion, device)\n",
    "print(\"------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Aanalysis - Comparing the models zeroed out weights & Correlations between the models predicitons and ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-out weights analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that extract the indices of the zeroed out feature maps in a convolutional layer\n",
    "\n",
    "def get_zeroed_feature_maps(model, layer_name):\n",
    "    \"\"\"\n",
    "    Get the indices of the zeroed out feature maps in a convolutional layer.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        layer_name (str): The name of the convolutional layer.\n",
    "\n",
    "    Returns:\n",
    "        list: The indices of the zeroed out feature maps.\n",
    "    \"\"\"\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    zeroed_feature_maps = []\n",
    "\n",
    "    for i, weight in enumerate(layer.weight):\n",
    "        if torch.all(weight == 0):\n",
    "            zeroed_feature_maps.append(i)\n",
    "    zeroed_feature_maps.sort()\n",
    "\n",
    "    num_zeroed = len(zeroed_feature_maps)\n",
    "\n",
    "    return zeroed_feature_maps, num_zeroed\n",
    "\n",
    "# Get the zeroed out feature maps in the 'features.28' layer of the noisy pruned model\n",
    "_, noisy_num_zeroed = get_zeroed_feature_maps(noisy_pruned_model, 'features.28')\n",
    "\n",
    "# Get the zeroed out feature maps in the 'features.28' layer of the best subset pruned model\n",
    "_, best_subset_num_zeroed = get_zeroed_feature_maps(best_subset_pruned_model, 'features.28')\n",
    "\n",
    "# Get the zeroed out feature maps in the 'features.28' layer of the negative impact pruned model\n",
    "_, negative_impact_num_zeroed = get_zeroed_feature_maps(negative_impact_pruned_model, 'features.28')\n",
    "\n",
    "print(f\"Noisy pruned model: {noisy_num_zeroed} zeroed out feature maps\")\n",
    "print(f\"Best subset pruned model: {best_subset_num_zeroed} zeroed out feature maps\")\n",
    "print(f\"Negative impact pruned model: {negative_impact_num_zeroed} zeroed out feature maps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def plot_correlations(model, dataloader, device, save_path=None, combination_method='average', title=\"\"):\n",
    "    \"\"\"\n",
    "    Computes and plots correlation with multiple ways of combining quality and authenticity metrics.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained model to evaluate\n",
    "        dataloader (DataLoader): Test dataloader containing images and true scores\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu')\n",
    "        save_path (str, optional): Path to save the plot. If None, plot is displayed instead.\n",
    "        combination_method (str): Method to combine scores ('average', 'weighted', 'euclidean', or 'all')\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing correlation metrics\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Collect predictions and ground truth\n",
    "    pred_list = []\n",
    "    true_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            pred_list.append(outputs.cpu())\n",
    "            true_list.append(labels)\n",
    "    \n",
    "    # Concatenate batches\n",
    "    predictions = torch.cat(pred_list, dim=0).numpy()\n",
    "    ground_truth = torch.cat(true_list, dim=0).numpy()\n",
    "    \n",
    "    # Extract quality and authenticity scores\n",
    "    pred_quality = predictions[:, 0]\n",
    "    true_quality = ground_truth[:, 0]\n",
    "    \n",
    "    pred_authenticity = predictions[:, 1]\n",
    "    true_authenticity = ground_truth[:, 1]\n",
    "    \n",
    "    # Calculate correlation metrics for individual scores\n",
    "    quality_spearman, q_pvalue = spearmanr(pred_quality, true_quality)\n",
    "    auth_spearman, a_pvalue = spearmanr(pred_authenticity, true_authenticity)\n",
    "    \n",
    "    quality_r2 = r2_score(true_quality, pred_quality)\n",
    "    auth_r2 = r2_score(true_authenticity, pred_authenticity)\n",
    "    \n",
    "    # Different combination methods\n",
    "    combinations = {}\n",
    "    \n",
    "    # Method 1: Simple average (mean)\n",
    "    true_avg = (true_quality + true_authenticity) / 2\n",
    "    pred_avg = (pred_quality + pred_authenticity) / 2\n",
    "    avg_spearman, avg_pvalue = spearmanr(true_avg, pred_avg)\n",
    "    avg_r2 = r2_score(true_avg, pred_avg)\n",
    "    combinations['average'] = {\n",
    "        'true': true_avg,\n",
    "        'pred': pred_avg,\n",
    "        'spearman': avg_spearman,\n",
    "        'p_value': avg_pvalue,\n",
    "        'r2': avg_r2,\n",
    "        'name': 'Average Score',\n",
    "        'description': 'Simple average of quality and authenticity',\n",
    "        'color': 'purple'\n",
    "    }\n",
    "    \n",
    "    # Method 2: Weighted average (assume quality is twice as important)\n",
    "    true_weighted = (2*true_quality + true_authenticity) / 3\n",
    "    pred_weighted = (2*pred_quality + pred_authenticity) / 3\n",
    "    weighted_spearman, weighted_pvalue = spearmanr(true_weighted, pred_weighted)\n",
    "    weighted_r2 = r2_score(true_weighted, pred_weighted)\n",
    "    combinations['weighted'] = {\n",
    "        'true': true_weighted,\n",
    "        'pred': pred_weighted,\n",
    "        'spearman': weighted_spearman,\n",
    "        'p_value': weighted_pvalue,\n",
    "        'r2': weighted_r2,\n",
    "        'name': 'Weighted Score',\n",
    "        'description': 'Weighted average (2:1 quality:authenticity)',\n",
    "        'color': 'green'\n",
    "    }\n",
    "    \n",
    "    # Method 3: Euclidean distance in 2D space\n",
    "    # Normalize the values to [0,1] first to ensure equal weighting\n",
    "    true_quality_norm = (true_quality - true_quality.min()) / (true_quality.max() - true_quality.min())\n",
    "    true_auth_norm = (true_authenticity - true_authenticity.min()) / (true_authenticity.max() - true_authenticity.min())\n",
    "    pred_quality_norm = (pred_quality - pred_quality.min()) / (pred_quality.max() - pred_quality.min())\n",
    "    pred_auth_norm = (pred_authenticity - pred_authenticity.min()) / (pred_authenticity.max() - pred_authenticity.min())\n",
    "    \n",
    "    # Calculate 2D Euclidean distance\n",
    "    true_euclidean = np.sqrt(true_quality_norm**2 + true_auth_norm**2)\n",
    "    pred_euclidean = np.sqrt(pred_quality_norm**2 + pred_auth_norm**2)\n",
    "    euclidean_spearman, euclidean_pvalue = spearmanr(true_euclidean, pred_euclidean)\n",
    "    euclidean_r2 = r2_score(true_euclidean, pred_euclidean)\n",
    "    combinations['euclidean'] = {\n",
    "        'true': true_euclidean,\n",
    "        'pred': pred_euclidean,\n",
    "        'spearman': euclidean_spearman,\n",
    "        'p_value': euclidean_pvalue,\n",
    "        'r2': euclidean_r2,\n",
    "        'name': 'Euclidean Combined Score',\n",
    "        'description': 'Euclidean distance in normalized 2D space',\n",
    "        'color': 'orange'\n",
    "    }\n",
    "    \n",
    "    # Decide which combination to use for plotting\n",
    "    if combination_method == 'all':\n",
    "        # Create a 3x2 grid (3 rows, 2 columns)\n",
    "        fig = plt.figure(figsize=(14, 16))\n",
    "        fig.suptitle(title)\n",
    "        gs = gridspec.GridSpec(3, 2, height_ratios=[1, 1, 1])\n",
    "        \n",
    "        # Individual scores (top row)\n",
    "        ax1 = plt.subplot(gs[0, 0])  # Quality\n",
    "        ax2 = plt.subplot(gs[0, 1])  # Authenticity\n",
    "        \n",
    "        # Combined scores (middle and bottom rows)\n",
    "        ax3 = plt.subplot(gs[1, :])  # Average (full width)\n",
    "        ax4 = plt.subplot(gs[2, 0])  # Weighted\n",
    "        ax5 = plt.subplot(gs[2, 1])  # Euclidean\n",
    "        \n",
    "        axes = [ax1, ax2, ax3, ax4, ax5]\n",
    "    else:\n",
    "        # Use a 1x2 grid for individual scores and 1 row for the selected combined method\n",
    "        fig = plt.figure(figsize=(14, 12))\n",
    "        fig.suptitle(title)\n",
    "        gs = gridspec.GridSpec(2, 4, height_ratios=[1, 1])\n",
    "        \n",
    "        ax1 = plt.subplot(gs[0, 0:2])  # Quality\n",
    "        ax2 = plt.subplot(gs[0,2:4])  # Authenticity\n",
    "        ax3 = plt.subplot(gs[1, 1:3])  # Combined (full width)\n",
    "        \n",
    "        axes = [ax1, ax2, ax3]\n",
    "    \n",
    "    # Plot Quality score correlation\n",
    "    ax1.scatter(true_quality, pred_quality, alpha=0.7, color='blue')\n",
    "    \n",
    "    # Add identity line\n",
    "    min_val = min(min(true_quality), min(pred_quality))\n",
    "    max_val = max(max(true_quality), max(pred_quality))\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect prediction')\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(true_quality, pred_quality, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_sorted = np.sort(true_quality)\n",
    "    ax1.plot(x_sorted, p(x_sorted), 'g-', label=f'Best fit (y = {z[0]:.3f}x + {z[1]:.3f})')\n",
    "    \n",
    "    ax1.set_xlabel('True Quality Score')\n",
    "    ax1.set_ylabel('Predicted Quality Score')\n",
    "    ax1.set_title(f'Quality Score Correlation\\nSpearman ρ = {quality_spearman:.4f}, R² = {quality_r2:.4f}')\n",
    "    ax1.grid(alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot Authenticity score correlation\n",
    "    ax2.scatter(true_authenticity, pred_authenticity, alpha=0.7, color='blue')\n",
    "    \n",
    "    # Add identity line\n",
    "    min_val = min(min(true_authenticity), min(pred_authenticity))\n",
    "    max_val = max(max(true_authenticity), max(pred_authenticity))\n",
    "    ax2.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect prediction')\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(true_authenticity, pred_authenticity, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_sorted = np.sort(true_authenticity)\n",
    "    ax2.plot(x_sorted, p(x_sorted), 'g-', label=f'Best fit (y = {z[0]:.3f}x + {z[1]:.3f})')\n",
    "    \n",
    "    ax2.set_xlabel('True Authenticity Score')\n",
    "    ax2.set_ylabel('Predicted Authenticity Score')\n",
    "    ax2.set_title(f'Authenticity Score Correlation\\nSpearman ρ = {auth_spearman:.4f}, R² = {auth_r2:.4f}')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Plot Combined score(s)\n",
    "    if combination_method == 'all':\n",
    "        # Plot all three combination methods\n",
    "        comb_methods = ['average', 'weighted', 'euclidean']\n",
    "        comb_axes = [ax3, ax4, ax5]\n",
    "        \n",
    "        for method, ax in zip(comb_methods, comb_axes):\n",
    "            comb = combinations[method]\n",
    "            ax.scatter(comb['true'], comb['pred'], alpha=0.7, color=comb['color'])\n",
    "            \n",
    "            # Add identity line\n",
    "            min_val = min(min(comb['true']), min(comb['pred']))\n",
    "            max_val = max(max(comb['true']), max(comb['pred']))\n",
    "            ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect prediction')\n",
    "            \n",
    "            # Add regression line\n",
    "            z = np.polyfit(comb['true'], comb['pred'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_sorted = np.sort(comb['true'])\n",
    "            ax.plot(x_sorted, p(x_sorted), 'g-', label=f'Best fit (y = {z[0]:.3f}x + {z[1]:.3f})')\n",
    "            \n",
    "            ax.set_xlabel(f'True {comb[\"name\"]}')\n",
    "            ax.set_ylabel(f'Predicted {comb[\"name\"]}')\n",
    "            ax.set_title(f'{comb[\"name\"]} Correlation\\nSpearman ρ = {comb[\"spearman\"]:.4f}, R² = {comb[\"r2\"]:.4f}\\n{comb[\"description\"]}')\n",
    "            ax.grid(alpha=0.3)\n",
    "            ax.legend()\n",
    "    else:\n",
    "        # Plot just the selected method\n",
    "        comb = combinations[combination_method]\n",
    "        ax3.scatter(comb['true'], comb['pred'], alpha=0.7, color=comb['color'])\n",
    "        \n",
    "        # Add identity line\n",
    "        min_val = min(min(comb['true']), min(comb['pred']))\n",
    "        max_val = max(max(comb['true']), max(comb['pred']))\n",
    "        ax3.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect prediction')\n",
    "        \n",
    "        # Add regression line\n",
    "        z = np.polyfit(comb['true'], comb['pred'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_sorted = np.sort(comb['true'])\n",
    "        ax3.plot(x_sorted, p(x_sorted), 'g-', label=f'Best fit (y = {z[0]:.3f}x + {z[1]:.3f})')\n",
    "        \n",
    "        ax3.set_xlabel(f'True {comb[\"name\"]}')\n",
    "        ax3.set_ylabel(f'Predicted {comb[\"name\"]}')\n",
    "        ax3.set_title(f'{comb[\"name\"]} Correlation\\nSpearman ρ = {comb[\"spearman\"]:.4f}, R² = {comb[\"r2\"]:.4f}\\n{comb[\"description\"]}')\n",
    "        ax3.grid(alpha=0.3)\n",
    "        ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"Quality Score - Spearman ρ: {quality_spearman:.4f} (p-value: {q_pvalue:.4g}), R²: {quality_r2:.4f}\")\n",
    "    print(f\"Authenticity Score - Spearman ρ: {auth_spearman:.4f} (p-value: {a_pvalue:.4g}), R²: {auth_r2:.4f}\")\n",
    "    \n",
    "    # Print combined statistics\n",
    "    for method, comb in combinations.items():\n",
    "        if combination_method == 'all' or method == combination_method:\n",
    "            print(f\"{comb['name']} - Spearman ρ: {comb['spearman']:.4f} (p-value: {comb['p_value']:.4g}), R²: {comb['r2']:.4f}\")\n",
    "    \n",
    "    # Return all metrics\n",
    "    metrics = {\n",
    "        'quality': {\n",
    "            'spearman': quality_spearman,\n",
    "            'p_value': q_pvalue,\n",
    "            'r2': quality_r2\n",
    "        },\n",
    "        'authenticity': {\n",
    "            'spearman': auth_spearman,\n",
    "            'p_value': a_pvalue,\n",
    "            'r2': auth_r2\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add combined metrics\n",
    "    for method, comb in combinations.items():\n",
    "        metrics[method] = {\n",
    "            'spearman': comb['spearman'],\n",
    "            'p_value': comb['p_value'],\n",
    "            'r2': comb['r2']\n",
    "        }\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "\n",
    "BASELINE_PATH_NAME = 'Plots/baseline_correlation.png'\n",
    "NOISY_PATH_NAME = 'Plots/noisy_pruned_correlation.png'\n",
    "BEST_SUBSET_PATH_NAME = 'Plots/best_subset_correlation.png'\n",
    "NEGATIVE_PATH_NAME = 'Plots/negative_impact_correlation.png'\n",
    "\n",
    "# Example usage:\n",
    "plot_correlations(baseline_model, test_dataloader, device, save_path=BASELINE_PATH_NAME, title=\"BASELINE MODEL\")\n",
    "plot_correlations(noisy_pruned_model, test_dataloader, device, save_path=NOISY_PATH_NAME, title=\"NOISY PRUNED MODEL\")\n",
    "plot_correlations(best_subset_pruned_model, test_dataloader, device, save_path=BEST_SUBSET_PATH_NAME, title=\"BEST SUBSET PRUNED MODEL\")\n",
    "plot_correlations(negative_impact_pruned_model, test_dataloader, device, save_path=NEGATIVE_PATH_NAME, title=\"NEGATIVE IMPACT PRUNED MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
