{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database creations using pytorch Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageQualityDataset(Dataset):\n",
    "    \"\"\"Dataset for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its labels by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (image, labels) where:\n",
    "                image (PIL.Image): The image.\n",
    "                labels (torch.Tensor): Tensor containing quality and authenticity scores.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(os.getcwd(), self.data.iloc[idx, 3])  # image_path column\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        quality = self.data.iloc[idx, 0]  # Quality column\n",
    "        authenticity = self.data.iloc[idx, 1]  # Authenticity column\n",
    "        labels = torch.tensor([quality, authenticity], dtype=torch.float)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in vgg.features.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Extract features up to fc2\n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        self.fc1 = vgg.classifier[:-1]  # Up to fc2 (4096 -> 128)\n",
    "        \n",
    "        # New regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2)  # Predict quality and realness\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        features = self.fc1(x)\n",
    "        predictions = self.regression_head(features)\n",
    "        return predictions, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations for the ImageNet dataset\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "annotations_file = 'Dataset/AIGCIQA2023/mos_data.csv'\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageQualityDataset(csv_file=annotations_file, transform=data_transforms)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Create a dictionary containing the data loaders\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader,\n",
    "    'test': test_dataloader\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_MODEL_PATH = 'Weights/VGG-16_finetuned_regression.pth'\n",
    "NOISY_PRUNED_MODEL_PATH = 'Weights/noise_out_pruned_model.pth'\n",
    "NEGATIVE_IMPACT_PRUNED_MODEL_PATH = 'Weights/negative_impact_pruned_model.pth'\n",
    "\n",
    "noisy_pruned_model = QualityPredictor()\n",
    "noisy_pruned_model.load_state_dict(torch.load(NOISY_PRUNED_MODEL_PATH, weights_only=True))\n",
    "\n",
    "negative_impact_pruned_model = QualityPredictor()\n",
    "negative_impact_pruned_model.load_state_dict(torch.load(NEGATIVE_IMPACT_PRUNED_MODEL_PATH,weights_only=True))\n",
    "\n",
    "baseline_model = QualityPredictor()\n",
    "baseline_model.load_state_dict(torch.load(BASELINE_MODEL_PATH,weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Sort the importance scores by channel ID and normalize them to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set numpy print options\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "importance_scores = np.load('Ranking_arrays/importance_scores.npy')\n",
    "# Sort the importance scores by channel index to replace the original position of the channel\n",
    "sorted_by_index_scores = importance_scores[importance_scores[:, 0].argsort()][:,1]\n",
    "\n",
    "def z_scores(scores):\n",
    "    return (scores - np.mean(scores)) / np.std(scores)\n",
    "\n",
    "\n",
    "small_bias = 0.01\n",
    "z_scored_importance_scores = z_scores(sorted_by_index_scores) + small_bias\n",
    "\n",
    "#plot the z-scored importance scores distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(z_scored_importance_scores, bins=100)\n",
    "plt.title('Z-scored importance scores distribution')\n",
    "plt.xlabel('Z-scored importance score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Extracting the activations of the last convolutional layer for the images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureMapHook:\n",
    "    \"\"\"Hook to extract feature maps from neural network layers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_maps = []\n",
    "    \n",
    "    def __call__(self, module, input, output):\n",
    "        # Detach from computation graph and move to CPU\n",
    "        self.feature_maps.append(output.detach().cpu())\n",
    "\n",
    "def get_feature_maps(model, dataloader, layer_name, device):\n",
    "    \"\"\"\n",
    "    Extracts the feature maps of a specific layer from a model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        dataloader (DataLoader): DataLoader for evaluation.\n",
    "        layer_name (str): The name of the layer to extract feature maps from.\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The feature maps as a numpy array with shape (240, num_features).\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Register a hook to extract feature maps\n",
    "    hook = FeatureMapHook()\n",
    "    target_layer = dict(model.named_modules())[layer_name]\n",
    "    hook_handle = target_layer.register_forward_hook(hook)\n",
    "    \n",
    "    # Forward pass to extract feature maps from the dataloader\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            model(inputs)\n",
    "\n",
    "    # Remove the hook\n",
    "    hook_handle.remove()\n",
    "    \n",
    "    # Process the feature maps to get the desired shape\n",
    "    all_features = []\n",
    "    \n",
    "    for batch_features in hook.feature_maps:\n",
    "        # Add batch features to our collection\n",
    "        all_features.append(batch_features)\n",
    "    \n",
    "    # Concatenate all batches and convert to numpy\n",
    "    features_tensor = torch.cat(all_features, dim=0)\n",
    "    \n",
    "    # Ensure we have exactly the number of samples we expect in the dataloader \n",
    "    assert features_tensor.shape[0] == len(dataloader.dataset) \n",
    "    \n",
    "    # Convert to numpy array\n",
    "    features_array = features_tensor.numpy()\n",
    "    \n",
    "    return features_array\n",
    "\n",
    "test_functions_dataloader = DataLoader(torch.utils.data.Subset(val_dataset, range(20)), \n",
    "                                     batch_size=20, \n",
    "                                     shuffle=False, \n",
    "                                     num_workers=10)\n",
    "\n",
    "layer_28_feature_maps_base = get_feature_maps(baseline_model, test_functions_dataloader, 'features.28', device)\n",
    "layer_28_feature_maps_noisy = get_feature_maps(noisy_pruned_model, test_functions_dataloader, 'features.28', device)\n",
    "layer_28_feature_maps_neg_impact = get_feature_maps(negative_impact_pruned_model, test_functions_dataloader, 'features.28', device)\n",
    "\n",
    "shape_base = layer_28_feature_maps_base.shape\n",
    "shape_noisy = layer_28_feature_maps_noisy.shape\n",
    "shape_neg_impact = layer_28_feature_maps_neg_impact.shape\n",
    "\n",
    "print(f'Feature maps shape for the baseline model: {shape_base}')\n",
    "print(f'Feature maps shape for the noisy pruned model: {shape_noisy}')\n",
    "print(f'Feature maps shape for the negative impact pruned model: {shape_neg_impact}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Analyze the activations of the last convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zeroed_feature_maps(model, layer_name):\n",
    "    \"\"\"\n",
    "    Get the indices of the zeroed out feature maps in a convolutional layer.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        layer_name (str): The name of the convolutional layer.\n",
    "\n",
    "    Returns:\n",
    "        list: The indices of the zeroed out feature maps.\n",
    "        int: The number of zeroed out feature maps.\n",
    "    \"\"\"\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    zeroed_feature_maps = []\n",
    "\n",
    "    for i, weight in enumerate(layer.weight):\n",
    "        if torch.all(weight == 0):\n",
    "            zeroed_feature_maps.append(i)\n",
    "    zeroed_feature_maps.sort()\n",
    "    #coverto to 0-based indexing\n",
    "    zeroed_feature_maps = [i-1 for i in zeroed_feature_maps]\n",
    "\n",
    "    num_zeroed = len(zeroed_feature_maps)\n",
    "\n",
    "    return zeroed_feature_maps, num_zeroed\n",
    "\n",
    "def analyze_feature_map_activations_zeros(feature_maps, model_name):\n",
    "    \"\"\"\n",
    "    Analyzes zero-valued activations in feature maps.\n",
    "    \n",
    "    Args:\n",
    "        feature_maps (numpy.ndarray): Feature maps with shape (n_samples, n_channels, height, width)\n",
    "        model_name (str): Name of the model for reporting\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints statistics about zero activations\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    n_samples, n_channels, height, width = feature_maps.shape\n",
    "    total_elements = n_samples * height * width  # Total elements per channel\n",
    "    \n",
    "    # Initialize counters\n",
    "    fully_zeroed_channels = 0\n",
    "    channel_zero_percentages = []\n",
    "    \n",
    "    # Analyze each channel\n",
    "    for channel_idx in range(n_channels):\n",
    "        # Get activations for this channel across all samples and spatial locations\n",
    "        channel_activations = feature_maps[:, channel_idx, :, :]\n",
    "        \n",
    "        # Count zeros in this channel\n",
    "        zero_count = np.sum(channel_activations == 0)\n",
    "        zero_percentage = (zero_count / total_elements) * 100\n",
    "        channel_zero_percentages.append(zero_percentage)\n",
    "        \n",
    "        # Check if channel is completely zeroed\n",
    "        if zero_percentage == 100:\n",
    "            fully_zeroed_channels += 1\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    avg_zero_percentage = np.mean(channel_zero_percentages)\n",
    "    median_zero_percentage = np.median(channel_zero_percentages)\n",
    "    \n",
    "    # Count channels with different zero percentages\n",
    "    high_zero_channels = sum(p >= 90 for p in channel_zero_percentages)  # >90% zeros\n",
    "    medium_zero_channels = sum(50 <= p < 90 for p in channel_zero_percentages)  # 50-90% zeros\n",
    "    low_zero_channels = sum(10 <= p < 50 for p in channel_zero_percentages)  # 10-50% zeros\n",
    "    sparse_zero_channels = sum(p < 10 for p in channel_zero_percentages)  # <10% zeros\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n===== {model_name} Zero Activation Analysis =====\")\n",
    "    print(f\"Completely zeroed channels: {fully_zeroed_channels} out of {n_channels} ({fully_zeroed_channels/n_channels*100:.2f}%)\")\n",
    "    print(f\"Average percentage of zeros per channel: {avg_zero_percentage:.2f}%\")\n",
    "    print(f\"Median percentage of zeros per channel: {median_zero_percentage:.2f}%\")\n",
    "    print(\"\\nChannel distribution by zero percentage:\")\n",
    "    print(f\"  - Fully zeroed (100%): {fully_zeroed_channels} channels\")\n",
    "    print(f\"  - Mostly zeroed (90-99.9%): {high_zero_channels - fully_zeroed_channels} channels\")\n",
    "    print(f\"  - Medium zeros (50-89.9%): {medium_zero_channels} channels\")\n",
    "    print(f\"  - Low zeros (10-49.9%): {low_zero_channels} channels\")\n",
    "    print(f\"  - Sparse zeros (<10%): {sparse_zero_channels} channels\")\n",
    "    \n",
    "    return channel_zero_percentages  # Return for further analysis if needed\n",
    "\n",
    "_, baseline_model_num_zeroed = get_zeroed_feature_maps(baseline_model, 'features.28')\n",
    "\n",
    "_, noisy_num_zeroed = get_zeroed_feature_maps(noisy_pruned_model, 'features.28')\n",
    "\n",
    "_, negative_impact_num_zeroed = get_zeroed_feature_maps(negative_impact_pruned_model, 'features.28')\n",
    "\n",
    "# Analyze each model's feature maps\n",
    "baseline_zero_percentages = analyze_feature_map_activations_zeros(layer_28_feature_maps_base, \"Baseline Model\")\n",
    "noisy_zero_percentages = analyze_feature_map_activations_zeros(layer_28_feature_maps_noisy, \"Noisy Pruned Model\")\n",
    "negative_impact_zero_percentages = analyze_feature_map_activations_zeros(layer_28_feature_maps_neg_impact, \"Negative Impact Pruned Model\")\n",
    "\n",
    "\n",
    "print(\"\\n===== Comparison: Zeroed Weights vs. Zeroed Activations =====\")\n",
    "print(f\"Baseline model: {baseline_model_num_zeroed} zeroed weight channels, {sum(np.array(baseline_zero_percentages) == 100)} zeroed activation channels\")\n",
    "print(f\"Noisy pruned model: {noisy_num_zeroed} zeroed weight channels, {sum(np.array(noisy_zero_percentages) == 100)} zeroed activation channels\")\n",
    "print(f\"Negative impact model: {negative_impact_num_zeroed} zeroed weight channels, {sum(np.array(negative_impact_zero_percentages) == 100)} zeroed activation channels\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pathlib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def create_heatmaps_from_dataloader(dataloader, batch_feature_maps,channel_importance_scores, output_dir=None):\n",
    "    \"\"\"\n",
    "    Create and save heatmaps for images in a dataloader using channel importance scores.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to extract feature maps\n",
    "        dataloader: PyTorch DataLoader containing images\n",
    "        channel_importance_scores: Array of importance scores for each channel (512,)\n",
    "        output_dir: Directory to save the output heatmaps\n",
    "        device: Device to run model on ('cuda' or 'cpu')\n",
    "        feature_layer_name: Name of the layer to extract features from\n",
    "        batch_limit: Maximum number of batches to process (None = all)\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = pathlib.Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    ORIGINAL_IMAGE_DIR = 'Heatmap_images/original_images'\n",
    "\n",
    "    # Define inverse normalization to recover original images\n",
    "    inv_normalize = transforms.Compose([\n",
    "        transforms.Normalize(\n",
    "            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "            std=[1/0.229, 1/0.224, 1/0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Process each batch\n",
    "    heatmap_list = []\n",
    "    img_idx = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        # Process each image in the batch\n",
    "        for i in range(inputs.size(0)):\n",
    "            # Get feature maps for this image\n",
    "            feature_maps = batch_feature_maps[i]  # Shape: (512, H, W)\n",
    "            \n",
    "            # Compute heatmap\n",
    "            heatmap = compute_heatmap(feature_maps, channel_importance_scores)\n",
    "            heatmap_list.append(heatmap)\n",
    "            \n",
    "            # Convert tensor back to image\n",
    "            img_tensor = inputs[i].cpu()\n",
    "            # Denormalize the image\n",
    "            img_tensor = inv_normalize(img_tensor)\n",
    "            img_tensor = torch.clamp(img_tensor, 0, 1)\n",
    "            img_np = img_tensor.permute(1, 2, 0).numpy() * 255\n",
    "            original_img = Image.fromarray(img_np.astype(np.uint8))\n",
    "            \n",
    "            # Save the original image\n",
    "            original_img_path = ORIGINAL_IMAGE_DIR + f\"/original_img_idx{img_idx}.png\"\n",
    "            original_img.save(original_img_path)\n",
    "            \n",
    "            # Overlay heatmap on original image\n",
    "            overlaid_img = overlay_mask(original_img, heatmap, enhance_contrast=True)\n",
    "            \n",
    "            # Save the overlaid image\n",
    "            output_path = output_dir / f\"heatmap_img_idx{img_idx}.png\"\n",
    "            overlaid_img.save(output_path)\n",
    "            \n",
    "            print(f\"Processed image {img_idx}\")\n",
    "            img_idx += 1\n",
    "        \n",
    "        print(f\"Completed batch {batch_idx+1}/{len(dataloader)}\")\n",
    "\n",
    "    return heatmap_list\n",
    "\n",
    "# def compute_heatmap(feature_maps, importance_scores=None):\n",
    "#     \"\"\"\n",
    "#     Compute weighted heatmap that emphasizes channel importance scores.\n",
    "    \n",
    "#     Args:\n",
    "#         feature_maps: Feature maps with shape (n_channels, height, width)\n",
    "#         importance_scores: Z-score Importance scores for each channel (n_channels,)\n",
    "        \n",
    "#     Returns:\n",
    "#         Weighted heatmap with shape (height, width)\n",
    "#     \"\"\"\n",
    "#     # If no scores provided, use uniform weights\n",
    "#     if importance_scores is None:\n",
    "#         importance_scores = np.ones(feature_maps.shape[0])\n",
    "    \n",
    "#     # Z-score normalize each feature map before weighting\n",
    "#     normalized_maps = np.zeros_like(feature_maps)\n",
    "#     for channel_idx in range(feature_maps.shape[0]):\n",
    "#         channel = feature_maps[channel_idx]\n",
    "#         mean_val = np.mean(channel)\n",
    "#         std_val = np.std(channel)\n",
    "        \n",
    "#         # Avoid division by zero and standardize\n",
    "#         if std_val > 0:\n",
    "#             normalized_maps[channel_idx] = (channel - mean_val) / std_val\n",
    "#         # If std is zero, leave as zeros (flat channel doesn't contribute)\n",
    "    \n",
    "#     # Apply importance weights\n",
    "#     weighted_maps = normalized_maps * importance_scores[:, np.newaxis, np.newaxis]\n",
    "    \n",
    "#     # Sum across channels to get final heatmap\n",
    "#     weighted_sum = np.sum(weighted_maps, axis=0)\n",
    "    \n",
    "#     # Final normalization for visualization (min-max scaling to [0,1])\n",
    "#     min_val = np.min(weighted_sum)\n",
    "#     max_val = np.max(weighted_sum)\n",
    "    \n",
    "#     if max_val > min_val:\n",
    "#         final_heatmap = (weighted_sum - min_val) / (max_val - min_val)\n",
    "#     else:\n",
    "#         final_heatmap = np.zeros_like(weighted_sum)\n",
    "    \n",
    "#     return final_heatmap\n",
    "def compute_heatmap(feature_maps, importance_scores=None):\n",
    "    \"\"\"\n",
    "    Compute weighted heatmap that emphasizes channel importance scores.\n",
    "    \n",
    "    Args:\n",
    "        feature_maps: Feature maps with shape (n_channels, height, width)\n",
    "        importance_scores: Z-score Importance scores for each channel (n_channels,)\n",
    "        \n",
    "    Returns:\n",
    "        Weighted heatmap with shape (height, width)\n",
    "    \"\"\"\n",
    "    # If no scores provided, use uniform weights\n",
    "    if importance_scores is None:\n",
    "        importance_scores = np.ones(feature_maps.shape[0])\n",
    "    \n",
    "    # Z-score normalize each feature map before weighting\n",
    "    normalized_maps = np.zeros_like(feature_maps)\n",
    "    for channel_idx in range(feature_maps.shape[0]):\n",
    "        channel = feature_maps[channel_idx]\n",
    "        mean_val = np.mean(channel)\n",
    "        std_val = np.std(channel)\n",
    "        \n",
    "        # Avoid division by zero and standardize\n",
    "        if std_val > 0:\n",
    "            normalized_maps[channel_idx] = (channel - mean_val) / std_val\n",
    "        # If std is zero, leave as zeros (flat channel doesn't contribute)\n",
    "    \n",
    "    # Apply importance weights\n",
    "    \n",
    "    weighted_maps = normalized_maps * importance_scores[:, np.newaxis, np.newaxis]\n",
    "    \n",
    "    # Sum across channels to get final heatmap\n",
    "    weighted_sum = np.sum(weighted_maps, axis=0)\n",
    "    \n",
    "    # Final normalization for visualization (min-max scaling to [0,1])\n",
    "    min_val = np.min(weighted_sum)\n",
    "    max_val = np.max(weighted_sum)\n",
    "    \n",
    "    if max_val > min_val:\n",
    "        final_heatmap = (weighted_sum - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        final_heatmap = np.zeros_like(weighted_sum)\n",
    "    \n",
    "    return final_heatmap\n",
    "\n",
    "def overlay_mask(img, mask, colormap=\"jet_r\", alpha=0.5, enhance_contrast=True):\n",
    "    \"\"\"\n",
    "    Overlay a colormapped mask on a background image with improved contrast.\n",
    "    \n",
    "    Args:\n",
    "        img: Background image (PIL Image)\n",
    "        mask: Mask to be overlayed (normalized heatmap)\n",
    "        colormap: Matplotlib colormap name\n",
    "        alpha: Transparency of the overlay (0-1)\n",
    "        enhance_contrast: Whether to apply contrast enhancement\n",
    "        \n",
    "    Returns:\n",
    "        Overlayed image (PIL Image)\n",
    "    \"\"\"\n",
    "    # Get the colormap\n",
    "    cmap = cm.get_cmap(colormap)\n",
    "    \n",
    "    # Resize mask to match image dimensions\n",
    "    overlay = cv2.resize(mask, img.size)\n",
    "    \n",
    "    # Apply contrast enhancement if requested\n",
    "    if enhance_contrast:\n",
    "        # Gamma correction can enhance visibility of mid-range values\n",
    "        # Values < 1 brighten the image, values > 1 darken it\n",
    "        gamma = 0.5\n",
    "        overlay = np.power(overlay, gamma)\n",
    "    \n",
    "    overlay_colored = (255 * cmap(overlay)[:, :, :3]).astype(np.uint8)\n",
    "    \n",
    "    # Blend the original image with the colored mask\n",
    "    overlayed_img = Image.fromarray(\n",
    "        (alpha * np.asarray(img) + (1 - alpha) * overlay_colored).astype(np.uint8)\n",
    "    )\n",
    "    \n",
    "    return overlayed_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup variables for main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the first 5 elements of the validation dataset to test the functions above\n",
    "\n",
    "OUTPUT_DIR_BASE = 'Heatmap_images/base_model'\n",
    "OUTPUT_DIR_NOISY = 'Heatmap_images/noisy_pruned_model'\n",
    "OUTPUT_DIR_NEG_IMPACT = 'Heatmap_images/negative_impact_pruned_model'\n",
    "\n",
    "channel_importance_scores = -z_scored_importance_scores\n",
    "# Extracted features (Batch size 20, 512 channels, 14x14 spatial dimensions)\n",
    "batch_features_base = layer_28_feature_maps_base\n",
    "batch_features_noisy = layer_28_feature_maps_noisy\n",
    "batch_features_neg_impact = layer_28_feature_maps_neg_impact\n",
    "\n",
    "# Extract removed channels indices from the pruned models\n",
    "noisy_removed_channels, _ = get_zeroed_feature_maps(noisy_pruned_model, 'features.28')\n",
    "negative_impact_removed_channels, _ = get_zeroed_feature_maps(negative_impact_pruned_model, 'features.28')\n",
    "\n",
    "print(noisy_removed_channels)\n",
    "print(negative_impact_removed_channels)\n",
    "\n",
    "def selective_importance_with_ignored_indices(original_scores, indices_to_ignore):\n",
    "    \"\"\"\n",
    "    Keep the original values of an array except for specific indices which are set to 0.\n",
    "    \n",
    "    Args:\n",
    "        original_scores: 1D array of scores with length 512\n",
    "        indices_to_ignore: Indices where values should be set to 0 (ignored)\n",
    "        \n",
    "    Returns:\n",
    "        Modified array where indices in indices_to_ignore are set to 0\n",
    "        and all other values the original values\n",
    "    \"\"\"\n",
    "    modified_scores = original_scores.copy()\n",
    "    modified_scores[indices_to_ignore] = 0\n",
    "    return modified_scores\n",
    "\n",
    "# Apply selective importance to the noisy pruned model\n",
    "baseline_scores = np.ones(512)\n",
    "noisy_scores = selective_importance_with_ignored_indices(channel_importance_scores, noisy_removed_channels)\n",
    "negative_impact_scores = selective_importance_with_ignored_indices(channel_importance_scores, negative_impact_removed_channels)\n",
    "\n",
    "# print the distribution of the noisy scores\n",
    "plt.hist(channel_importance_scores, bins=100)\n",
    "plt.title('Noisy pruned model scores distribution')\n",
    "plt.xlabel('Importance score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# print the distribution of the noisy scores\n",
    "plt.hist(noisy_scores, bins=100)\n",
    "plt.title('Noisy pruned model scores distribution')\n",
    "plt.xlabel('Importance score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# print the distribution of the negative impact scores\n",
    "plt.hist(negative_impact_scores, bins=100)\n",
    "plt.title('Negative impact pruned model scores distribution')\n",
    "plt.xlabel('Importance score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_list_base = create_heatmaps_from_dataloader(test_functions_dataloader, batch_features_base, np.ones(512),OUTPUT_DIR_BASE)\n",
    "\n",
    "heatmap_list_noisy = create_heatmaps_from_dataloader(test_functions_dataloader, batch_features_noisy, noisy_scores, OUTPUT_DIR_NOISY)\n",
    "\n",
    "heatmap_list_neg_impact = create_heatmaps_from_dataloader(test_functions_dataloader, batch_features_neg_impact,negative_impact_scores, OUTPUT_DIR_NEG_IMPACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def display_image_comparison(num_images=5, figsize=(20, 15)):\n",
    "    \"\"\"\n",
    "    Display a comparison grid of images showing original images and their corresponding heatmaps\n",
    "    from different models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    num_images : int, default=5\n",
    "        Number of different images to display (rows in the grid)\n",
    "    figsize : tuple, default=(20, 15)\n",
    "        Figure size for the plot\n",
    "    \"\"\"\n",
    "    # Create figure and axes\n",
    "    fig, axs = plt.subplots(num_images, 4, figsize=figsize)\n",
    "    \n",
    "    # Define column titles\n",
    "    col_titles = ['Original Image', 'Base Model Heatmap', \n",
    "                 'Noisy Pruned Model Heatmap', 'Negative Impact Pruned Model Heatmap']\n",
    "    \n",
    "    # Set column titles at the top of the grid\n",
    "    for col, title in enumerate(col_titles):\n",
    "        axs[0, col].set_title(title, fontsize=14, pad=10)\n",
    "    \n",
    "    # Define folder paths\n",
    "    folders = [\n",
    "        'Heatmap_images/original_images/',\n",
    "        'Heatmap_images/base_model/',\n",
    "        'Heatmap_images/noisy_pruned_model/',\n",
    "        'Heatmap_images/negative_impact_pruned_model/'\n",
    "    ]\n",
    "    \n",
    "    # Image file patterns\n",
    "    patterns = [\n",
    "        'original_img_idx{}.png',\n",
    "        'heatmap_img_idx{}.png',\n",
    "        'heatmap_img_idx{}.png',\n",
    "        'heatmap_img_idx{}.png'\n",
    "    ]\n",
    "    \n",
    "    # Load and display each image\n",
    "    for row in range(num_images):\n",
    "        # Add row label (image index)\n",
    "        axs[row, 0].set_ylabel(f'Image {row}', fontsize=12, rotation=0, labelpad=40, va='center')\n",
    "        \n",
    "        for col in range(4):\n",
    "            # Construct image path\n",
    "            img_path = os.path.join(folders[col], patterns[col].format(row))\n",
    "            \n",
    "            # Check if file exists\n",
    "            if os.path.exists(img_path):\n",
    "                try:\n",
    "                    img = mpimg.imread(img_path)\n",
    "                    axs[row, col].imshow(img)\n",
    "                except Exception as e:\n",
    "                    axs[row, col].text(0.5, 0.5, f\"Error loading image:\\n{str(e)}\", \n",
    "                                      ha='center', va='center', color='red')\n",
    "            else:\n",
    "                axs[row, col].text(0.5, 0.5, f\"File not found:\\n{img_path}\", \n",
    "                                  ha='center', va='center', color='red')\n",
    "            \n",
    "            # Turn off axis ticks\n",
    "            axs[row, col].set_xticks([])\n",
    "            axs[row, col].set_yticks([])\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95, wspace=0.1, hspace=0.2)\n",
    "    \n",
    "    # Save the figure (optional)\n",
    "    plt.savefig('heatmap_comparison.png', dpi=300, bbox_inches='tight')\n",
    "# Call the function to display the images\n",
    "display_image_comparison(num_images=20, figsize=(30, 30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distributions of the 14x14 heatmaps created for the first 5 images in the validation set\n",
    "\n",
    "def plot_heatmap_distributions(heatmaps, model_name):\n",
    "    \"\"\"\n",
    "    Plot the distribution of values in the heatmaps.\n",
    "    \n",
    "    Args:\n",
    "        heatmaps (list): List of heatmaps (numpy arrays).\n",
    "        model_name (str): Name of the model for the plot title.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for i, heatmap in enumerate(heatmaps):\n",
    "        # Flatten the heatmap\n",
    "        heatmap_flat = heatmap.flatten()\n",
    "        \n",
    "        # Plot histogram\n",
    "        plt.hist(heatmap_flat, bins=50, alpha=0.5, label=f'Image {i}')\n",
    "    \n",
    "    plt.title(f'Heatmap Value Distribution ({model_name})')\n",
    "    plt.xlabel('Heatmap Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "# Plot the heatmap distributions for the three models\n",
    "plot_heatmap_distributions(heatmap_list_base, 'Base Model')\n",
    "plot_heatmap_distributions(heatmap_list_noisy, 'Noisy Pruned Model')\n",
    "plot_heatmap_distributions(heatmap_list_neg_impact, 'Negative Impact Pruned Model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
