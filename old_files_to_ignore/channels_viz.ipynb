{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16, VGG16_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set numpy print options\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "importance_scores = np.load('Ranking_arrays/importance_scores.npy')\n",
    "#sort the importance scores\n",
    "sorted_by_index_scores = importance_scores[importance_scores[:, 0].argsort()][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in vgg.features.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Extract features up to fc2\n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        self.fc1 = vgg.classifier[:-1]  # Up to fc2 (4096 -> 128)\n",
    "        \n",
    "        # New regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2)  # Predict quality and realness\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        features = self.fc1(x)\n",
    "        predictions = self.regression_head(features)\n",
    "        return predictions, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pruned models \n",
    "BASELINE_MODEL_PATH = 'Weights/VGG-16_finetuned_regression.pth'\n",
    "NOISY_PRUNED_MODEL_PATH = 'Weights/noise_out_pruned_model.pth'\n",
    "NEGATIVE_IMPACT_PRUNED_MODEL_PATH = 'Weights/negative_impact_pruned_model.pth'\n",
    "\n",
    "noisy_pruned_model = QualityPredictor()\n",
    "noisy_pruned_model.load_state_dict(torch.load(NOISY_PRUNED_MODEL_PATH, weights_only=True))\n",
    "\n",
    "negative_impact_pruned_model = QualityPredictor()\n",
    "negative_impact_pruned_model.load_state_dict(torch.load(NEGATIVE_IMPACT_PRUNED_MODEL_PATH,weights_only=True))\n",
    "\n",
    "\n",
    "def get_zeroed_feature_maps_indices(model, layer_name):\n",
    "    \"\"\"\n",
    "    Get the indices of the zeroed out feature maps in a convolutional layer.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        layer_name (str): The name of the convolutional layer.\n",
    "\n",
    "    Returns:\n",
    "        list: The indices of the zeroed out feature maps.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    zeroed_feature_maps = []\n",
    "\n",
    "    for i, weight in enumerate(layer.weight):\n",
    "        if torch.all(weight == 0):\n",
    "            zeroed_feature_maps.append(i)\n",
    "    zeroed_feature_maps.sort()\n",
    "\n",
    "    return zeroed_feature_maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy Pruned Model:\n",
      "[1, 2, 3, 13, 14, 16, 24, 28, 32, 34, 40, 47, 49, 51, 60, 62, 67, 73, 76, 83, 85, 86, 87, 92, 93, 96, 99, 111, 113, 121, 123, 124, 125, 130, 136, 137, 138, 142, 147, 150, 153, 158, 162, 163, 165, 175, 182, 183, 184, 191, 193, 196, 200, 204, 212, 218, 220, 224, 233, 234, 236, 237, 239, 245, 250, 251, 254, 261, 262, 263, 264, 265, 276, 278, 281, 283, 288, 289, 290, 292, 293, 294, 299, 300, 302, 303, 320, 323, 325, 326, 330, 353, 356, 364, 375, 377, 381, 384, 387, 390, 391, 393, 401, 404, 407, 417, 422, 423, 425, 430, 437, 438, 441, 442, 447, 448, 451, 453, 454, 458, 461, 467, 472, 473, 476, 481, 482, 485, 493, 497, 499, 506, 510]\n",
      "Negative Impact Pruned Model:\n",
      "[1, 3, 4, 5, 6, 9, 13, 14, 19, 24, 26, 29, 30, 32, 34, 39, 40, 43, 45, 47, 49, 51, 55, 59, 62, 69, 73, 74, 76, 79, 80, 83, 86, 92, 93, 94, 95, 96, 99, 110, 112, 113, 115, 121, 129, 136, 137, 142, 144, 146, 147, 150, 152, 163, 164, 165, 168, 174, 175, 176, 177, 180, 183, 184, 190, 196, 198, 202, 205, 207, 209, 211, 212, 213, 214, 215, 220, 221, 224, 226, 227, 228, 234, 236, 237, 238, 239, 242, 249, 250, 251, 255, 256, 257, 260, 261, 262, 263, 265, 269, 278, 281, 284, 285, 287, 289, 290, 293, 300, 302, 303, 304, 305, 307, 310, 311, 320, 321, 322, 323, 324, 326, 327, 331, 338, 340, 341, 343, 344, 345, 352, 360, 364, 368, 371, 375, 384, 385, 388, 390, 396, 401, 402, 403, 404, 407, 413, 416, 417, 421, 423, 428, 430, 432, 434, 435, 437, 438, 442, 447, 448, 453, 457, 458, 464, 467, 470, 472, 474, 476, 478, 480, 481, 482, 483, 484, 485, 486, 489, 490, 494, 496, 497, 501, 510]\n",
      "Baseline Model: 36.13% of the scores are below 0\n",
      "Noisy Pruned Model: 61.65% of the scores are below 0\n",
      "Negative Impact Pruned Model: 100.00% of the scores are below 0\n",
      "Noisy Pruned Model Z-scores:\n",
      "[np.float64(0.3864638311815031), np.float64(0.16257130848009152), np.float64(0.6701633647328268), np.float64(0.7945497883149565), np.float64(1.256963613372936), np.float64(0.3134947183249776), np.float64(0.8023923340833312), np.float64(0.19979300341565306), np.float64(0.7646234847918366), np.float64(0.9136774505872844), np.float64(0.6321969319239118), np.float64(1.0782037256699524), np.float64(0.6061235069904877), np.float64(1.1728462305142733), np.float64(-0.049291417756846034), np.float64(0.9520238516989308), np.float64(-0.027245656832374), np.float64(0.9152049231642645), np.float64(0.8700874868979458), np.float64(0.4795332672525162), np.float64(0.26765534228346943), np.float64(1.3109267117268402), np.float64(0.16832402858344392), np.float64(0.8925132161251491), np.float64(0.6266493947040344), np.float64(0.39521830087643306), np.float64(0.8845870773301735), np.float64(0.08627367329163953), np.float64(1.3526928275630685), np.float64(0.6033953345768767), np.float64(-0.31760223504755064), np.float64(0.2882952204878355), np.float64(0.1822384678294654), np.float64(0.3495309121560174), np.float64(0.8640003946881898), np.float64(1.012864376332272), np.float64(0.3216716361997559), np.float64(0.3828465329395008), np.float64(0.9261252121847631), np.float64(0.6523200532365633), np.float64(0.31583532306980266), np.float64(-1.2912938088947594), np.float64(0.04587544334524403), np.float64(0.6034409307732044), np.float64(1.205257526737256), np.float64(1.3816464122311962), np.float64(0.1395908255309004), np.float64(0.9880752442620798), np.float64(0.7271738088746362), np.float64(0.22367021155929012), np.float64(0.10238432932744812), np.float64(0.4124536630883264), np.float64(-0.3551431033574065), np.float64(0.3007657801834779), np.float64(0.3942151845572223), np.float64(-0.10386246539512031), np.float64(0.6239136229243687), np.float64(1.4617437304469616), np.float64(0.2342257310091666), np.float64(1.2832954167522175), np.float64(1.4169378681888825), np.float64(0.8558614736436846), np.float64(0.4637797814212751), np.float64(0.24616433508098512), np.float64(0.8027115074576255), np.float64(0.8066783765381407), np.float64(0.1432689187013397), np.float64(1.266166445665089), np.float64(1.4288840716267557), np.float64(0.9624501819258786), np.float64(0.17135617563923997), np.float64(0.921892365292336), np.float64(0.15643102070795314), np.float64(0.5369084809649477), np.float64(0.44640763062039845), np.float64(0.2192245824173335), np.float64(-0.05053771378980481), np.float64(0.3909018609574051), np.float64(0.72709781521409), np.float64(0.3076812032931882), np.float64(0.5960163501378342), np.float64(0.222613899677697), np.float64(0.15375604385672456), np.float64(0.8341424854595615), np.float64(1.0095966489287826), np.float64(1.0141258710973402), np.float64(0.46012448634899966), np.float64(0.716937462799054), np.float64(0.11021167636371361), np.float64(1.2058958734858447), np.float64(0.21388222808093094), np.float64(0.16460793858273146), np.float64(0.25218303299624945), np.float64(0.4047707040070988), np.float64(0.8574345424169924), np.float64(0.3536269704594612), np.float64(0.29727767116440423), np.float64(0.8642511737679925), np.float64(0.3313912253836235), np.float64(0.5127500962772895), np.float64(0.28537706392285883), np.float64(0.28486030703114423), np.float64(0.8296740582194411), np.float64(1.1925285885957564), np.float64(0.7665537237697118), np.float64(0.6459821819470046), np.float64(0.3287846428268865), np.float64(1.5530881104235743), np.float64(0.281607778359764), np.float64(0.6460885730717694), np.float64(0.6705129355713396), np.float64(0.6505873977761084), np.float64(0.36108194855904996), np.float64(0.48142550940011825), np.float64(0.955093995585), np.float64(0.7716908952226393), np.float64(0.35484286902820145), np.float64(0.3920265671334899), np.float64(0.022043831397934727), np.float64(1.2521076184640296), np.float64(0.1969356417791134), np.float64(0.6673212018283964), np.float64(0.4313304883680191), np.float64(0.31858629358157753), np.float64(0.4243770684280357), np.float64(0.7561425922748731), np.float64(0.7472209365267415), np.float64(0.464676506615721), np.float64(-0.37385274258389745), np.float64(3.476791633491775), np.float64(0.23664992878059252), np.float64(0.1397048160217198), np.float64(1.1496833627797711)]\n",
      "Negative Impact Pruned Model Z-scores:\n",
      "[np.float64(0.3864638311815031), np.float64(0.6701633647328268), np.float64(0.5335875579990759), np.float64(0.5859775875796723), np.float64(0.47985244062681054), np.float64(0.7216338710208133), np.float64(0.7945497883149565), np.float64(1.256963613372936), np.float64(0.4510660420118846), np.float64(0.8023923340833312), np.float64(0.6576700069390204), np.float64(0.44201519704082426), np.float64(0.49975518032387783), np.float64(0.7646234847918366), np.float64(0.9136774505872844), np.float64(0.6908108423032475), np.float64(0.6321969319239118), np.float64(0.5888349492162119), np.float64(0.434203048736668), np.float64(1.0782037256699524), np.float64(0.6061235069904877), np.float64(1.1728462305142733), np.float64(0.6637343010506126), np.float64(0.9886527960822314), np.float64(0.9520238516989308), np.float64(0.599337273103706), np.float64(0.9152049231642645), np.float64(0.40548504441623373), np.float64(0.8700874868979458), np.float64(0.6038816940043727), np.float64(0.5210182065447233), np.float64(0.4795332672525162), np.float64(1.3109267117268402), np.float64(0.8925132161251491), np.float64(0.6266493947040344), np.float64(0.5716907793969741), np.float64(0.7390212205537993), np.float64(0.39521830087643306), np.float64(0.8845870773301735), np.float64(0.5802932617708115), np.float64(0.571508394611663), np.float64(1.3526928275630685), np.float64(0.5121801438231924), np.float64(0.6033953345768767), np.float64(0.5148703194065303), np.float64(0.8640003946881898), np.float64(1.012864376332272), np.float64(0.3828465329395008), np.float64(0.54724361879924), np.float64(0.5194679358695795), np.float64(0.9261252121847631), np.float64(0.6523200532365633), np.float64(0.6250991240288905), np.float64(0.6034409307732044), np.float64(0.5674655318706017), np.float64(1.205257526737256), np.float64(0.47955606535068007), np.float64(0.9011308972310959), np.float64(1.3816464122311962), np.float64(0.5213981748474547), np.float64(0.55345230086587), np.float64(0.7125450292194798), np.float64(0.9880752442620798), np.float64(0.7271738088746362), np.float64(0.7056296061097695), np.float64(0.4124536630883264), np.float64(0.6126513624314119), np.float64(0.9015184648998817), np.float64(0.6021642372760272), np.float64(0.5441810742792255), np.float64(0.43325312797983967), np.float64(0.41278803519472995), np.float64(0.3942151845572223), np.float64(0.5903168255968642), np.float64(0.5023617628806148), np.float64(0.6900357069656756), np.float64(0.6239136229243687), np.float64(0.4455641009883349), np.float64(1.4617437304469616), np.float64(0.4655884305422762), np.float64(0.5711740225052595), np.float64(0.4371060065695354), np.float64(1.2832954167522175), np.float64(1.4169378681888825), np.float64(0.8558614736436846), np.float64(0.6123017915928991), np.float64(0.4637797814212751), np.float64(0.49603149095711074), np.float64(0.6765544315847678), np.float64(0.8027115074576255), np.float64(0.8066783765381407), np.float64(0.4621687158176942), np.float64(0.43666524333836704), np.float64(0.7891010428537891), np.float64(0.5476387858340807), np.float64(1.266166445665089), np.float64(1.4288840716267557), np.float64(0.9624501819258786), np.float64(0.921892365292336), np.float64(0.43940101511803265), np.float64(0.5369084809649477), np.float64(0.44640763062039845), np.float64(0.4655276356138392), np.float64(0.6683699143439349), np.float64(0.9502759975063666), np.float64(0.3909018609574051), np.float64(0.72709781521409), np.float64(0.5960163501378342), np.float64(0.8341424854595615), np.float64(1.0095966489287826), np.float64(1.0141258710973402), np.float64(0.6955908435516077), np.float64(0.4098850773618626), np.float64(0.8866389061649227), np.float64(0.6265278048471603), np.float64(0.5943216915076523), np.float64(0.46012448634899966), np.float64(0.6663636817055134), np.float64(0.5029621127989303), np.float64(0.716937462799054), np.float64(0.5588250526664911), np.float64(1.2058958734858447), np.float64(0.5976046176432511), np.float64(0.5174769019632672), np.float64(0.6668348424009003), np.float64(0.8404195618206832), np.float64(0.4962138757424218), np.float64(0.7832571303577812), np.float64(0.8112303968048621), np.float64(0.3822765804854038), np.float64(0.6750117602756787), np.float64(0.5609224776975681), np.float64(0.4047707040070988), np.float64(0.6650109945477899), np.float64(0.43692362178422434), np.float64(0.8574345424169924), np.float64(0.8642511737679925), np.float64(0.5173781102045572), np.float64(0.4201670196337725), np.float64(0.5127500962772895), np.float64(0.7953173242864737), np.float64(0.8296740582194411), np.float64(0.46305784164608554), np.float64(0.5218161399804592), np.float64(1.1925285885957564), np.float64(0.7665537237697118), np.float64(0.49147187132433473), np.float64(0.48879689447310615), np.float64(0.6459821819470046), np.float64(0.7638179519900461), np.float64(1.5530881104235743), np.float64(0.7314598513294457), np.float64(0.6460885730717694), np.float64(0.8930603704810823), np.float64(0.3978400821652793), np.float64(0.5567808231977965), np.float64(0.6705129355713396), np.float64(0.6505873977761084), np.float64(0.48142550940011825), np.float64(0.955093995585), np.float64(0.7716908952226393), np.float64(0.3920265671334899), np.float64(0.8871176662263642), np.float64(1.2521076184640296), np.float64(0.6028861770512167), np.float64(0.6673212018283964), np.float64(0.6777931282516719), np.float64(0.4313304883680191), np.float64(0.5280172226810345), np.float64(0.4243770684280357), np.float64(0.8258135802636907), np.float64(0.48978481206020763), np.float64(0.7561425922748731), np.float64(0.7472209365267415), np.float64(0.41876113691366657), np.float64(0.745275498816757), np.float64(0.464676506615721), np.float64(0.47736744792694763), np.float64(0.64912831949362), np.float64(0.418426764807263), np.float64(0.555428136040073), np.float64(0.5274624689590468), np.float64(3.476791633491775), np.float64(0.7347579761971537), np.float64(1.1496833627797711)]\n"
     ]
    }
   ],
   "source": [
    "noisy_indices = get_zeroed_feature_maps_indices(noisy_pruned_model, 'features.28')\n",
    "negative_indices = get_zeroed_feature_maps_indices(negative_impact_pruned_model, 'features.28')\n",
    "\n",
    "print('Noisy Pruned Model:')\n",
    "print(noisy_indices)\n",
    "\n",
    "print('Negative Impact Pruned Model:')\n",
    "print(negative_indices)\n",
    "\n",
    "\n",
    "\n",
    "def z_score(score, mean, std):\n",
    "    \"\"\"\n",
    "    Compute the Z-scores of a single score with respect to a set of scores.\"\n",
    "    \"\"\"\n",
    "    z_score = (score - mean) / std\n",
    "    return z_score\n",
    "\n",
    "scores_mean = np.mean(sorted_by_index_scores)\n",
    "scores_std = np.std(sorted_by_index_scores)\n",
    "\n",
    "# Now i want to create an array of the same length as the importance scores array, where each element in the noisy_indices array is replaced with the z-score of the corresponding element in the importance scores array and the other elements are substituted with 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
