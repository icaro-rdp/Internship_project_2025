{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database creations using pytorch Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAuthenticityDataset(Dataset):\n",
    "    \"\"\"Dataset for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its labels by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (image, labels) where:\n",
    "                image (PIL.Image): The image.\n",
    "                labels (torch.Tensor): Tensor containing quality and authenticity scores.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(os.getcwd(), self.data.iloc[idx, 3])  # image_path column\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        authenticity = self.data.iloc[idx, 1]  # Authenticity column\n",
    "        labels = torch.tensor([authenticity], dtype=torch.float)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthenticityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in vgg.features.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Extract features up to fc2\n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        self.fc1 = vgg.classifier[:-1]  # Up to fc2 (4096 -> 128)\n",
    "        \n",
    "        # New regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1)  # Predict authenticity\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        features = self.fc1(x)\n",
    "        predictions = self.regression_head(features)\n",
    "        return predictions, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Trains the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloaders (dict): A dictionary containing the training and validation data loaders.\n",
    "        criterion (nn.Module): The loss function.\n",
    "        optimizer (optim.Optimizer): The optimizer.\n",
    "        num_epochs (int): Number of epochs to train for. Defaults to 10.\n",
    "        device (str): Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The trained model.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:  # Iterate over training and validation phases\n",
    "            print(f'{phase} phase')\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:  # Iterate over data in the current phase\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):  # Enable gradients only during training\n",
    "                    outputs, _ = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}') # Print loss for the current phase\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return model\n",
    "\n",
    "def test_model(model, dataloader, criterion, device='cuda'):\n",
    "\n",
    "    \"\"\"\n",
    "    Tests the model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        dataloader (DataLoader): The test data loader.\n",
    "        criterion (nn.Module): The loss function.\n",
    "        device (str): Device to use for testing ('cuda' or 'cpu'). Defaults to 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss on the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs, _ = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    test_loss = running_loss / len(dataloader.dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    return test_loss\n",
    "\n",
    "def get_predictions(model, dataloader, device)-> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Get predictions from the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        dataloader (DataLoader): The data loader.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (predictions, labels) where:\n",
    "            predictions (torch.Tensor): Predictions from the model.\n",
    "            labels (torch.Tensor): Ground truth labels.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, target in dataloader:\n",
    "            outputs, _ = model(inputs.to(device))\n",
    "            predictions.append(outputs)\n",
    "            labels.append(target)\n",
    "\n",
    "    #move to cpu and concatenate\n",
    "    predictions = torch.cat(predictions).cpu()\n",
    "    labels = torch.cat(labels).cpu()\n",
    "\n",
    "    return predictions, labels\n",
    "\n",
    "def get_regression_errors(tuple: tuple[torch.Tensor, torch.Tensor]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Get regression errors.\n",
    "\n",
    "    Args:\n",
    "        tuple: A tuple (predictions, labels) where:\n",
    "            predictions (torch.Tensor): Predictions from the model.\n",
    "            labels (torch.Tensor): Ground truth labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (quality_errors, authenticity_errors) where:\n",
    "            quality_errors (torch.Tensor): Quality errors.\n",
    "            authenticity_errors (torch.Tensor): Authenticity errors.\n",
    "    \"\"\"\n",
    "    predictions, labels = tuple\n",
    "    authenticity_errors = predictions[:, 0] - labels[:, 0]\n",
    "    return authenticity_errors\n",
    "\n",
    "def get_rmse(errors: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get the root mean squared error.\n",
    "\n",
    "    Args:\n",
    "        errors (torch.Tensor): Errors.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Root mean squared error.\n",
    "    \"\"\"\n",
    "    return torch.sqrt(torch.mean(errors ** 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations for the ImageNet dataset\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "annotations_file = 'Dataset/AIGCIQA2023/real_images_annotations.csv'\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageAuthenticityDataset(csv_file=annotations_file, transform=data_transforms)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Create a dictionary containing the data loaders\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader,\n",
    "    'test': test_dataloader\n",
    "}\n",
    "\n",
    "model = AuthenticityPredictor()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss (regression)\n",
    "optimizer = optim.Adam(model.regression_head.parameters(), lr=0.001)\n",
    "\n",
    "model_path = 'Models/VGG-16_authenticity_finetuned.pth'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticity_predictor_trained= train_model(model, dataloaders, criterion, optimizer, EPOCHS, device)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(authenticity_predictor_trained.state_dict(), \"Models/VGG-16_real_authenticity_finetuned.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities for pruning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_map_importance(model, dataloader, device, layer_name) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Computes the importance of each feature map in a convolution\n",
    "    layer by measuring the change in predictions when the feature map is zero\n",
    "    out.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (indices, importance_scores) where both are numpy arrays\n",
    "    \"\"\"\n",
    "    #if importance_scores.npy exists, load it\n",
    "    if os.path.exists('Ranking_arrays/real_authenticity_importance_scores.npy'):\n",
    "        print(\"Importance scores already computed, loading from file\")\n",
    "        return np.load('Ranking_arrays/real_authenticity_importance_scores.npy')\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    importance_scores = []\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    baseline_predictions = get_predictions(model, dataloader, device)\n",
    "    regression_errors = get_regression_errors(baseline_predictions)\n",
    "    baseline_authenticity_rmse = get_rmse(regression_errors)\n",
    "\n",
    "    \n",
    "    print(f'Authenticity baseline RMSE: {baseline_authenticity_rmse:.4f}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(layer.out_channels):\n",
    "            # Create a backup of the weights and bias\n",
    "            backup_weights = layer.weight[i, ...].clone()\n",
    "            backup_bias = layer.bias[i].clone() if layer.bias is not None else None\n",
    "\n",
    "            # Zero out the i-th output channel\n",
    "            layer.weight[i, ...] = 0\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[i] = 0\n",
    "\n",
    "            # Get predictions with the pruned feature map\n",
    "            pruned_predictions = get_predictions(model, dataloader, device)\n",
    "            pruned_regression_errors = get_regression_errors(pruned_predictions)\n",
    "            pruned_authenticity_rmse = get_rmse(pruned_regression_errors)\n",
    "            \n",
    "    \n",
    "            # Compute importance score\n",
    "            importance_score = baseline_authenticity_rmse - pruned_authenticity_rmse\n",
    "            importance_scores.append([i, importance_score])\n",
    "            \n",
    "\n",
    "            print(f'Feature map {i}: Importance score: {importance_score:.4f}')\n",
    "            \n",
    "            # After computing importance, restore weights and bias\n",
    "            layer.weight[i, ...] = backup_weights\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[i] = backup_bias \n",
    "\n",
    "    sorted_importance_scores = sorted(importance_scores, key=lambda x: x[1], reverse=True)\n",
    "    # save np array \n",
    "    np.save('authenticity_importance_scores.np', sorted_importance_scores)\n",
    "    return np.array(sorted_importance_scores)\n",
    "\n",
    "def remove_noisy_feature_maps(model, dataloader, device, layer_name, sorted_importance_scores, model_path='Models/pruned_model.pth'):\n",
    "    \"\"\"\n",
    "    Remove noisy feature maps from a convolutional layer based on importance scores.\n",
    "    Feature maps are zeroed out one by one and kept zeroed only if model performance improves.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        dataloader: DataLoader for evaluation\n",
    "        device: Device to run the model on (cuda/cpu)\n",
    "        layer_name: Name of the layer to optimize\n",
    "        sorted_importance_scores: List of tuples (channel_index, importance_score) sorted by importance\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with pruning results and performance metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Get the target layer\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    \n",
    "    # Create a backup of the original weights and bias\n",
    "    original_weights = layer.weight.clone()\n",
    "    original_bias = layer.bias.clone() if layer.bias is not None else None\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    removed_features = []\n",
    "    rmse_history = []\n",
    "    \n",
    "    # Get baseline performance\n",
    "    baseline_predictions = get_predictions(model, dataloader, device)\n",
    "    baseline_regression_errors = get_regression_errors(baseline_predictions)\n",
    "    baseline_authenticity_rmse = get_rmse(baseline_regression_errors)\n",
    "    \n",
    "    \n",
    "    print(f\"Baseline authenticity RMSE: {baseline_authenticity_rmse:.4f}\")\n",
    "    print(\"------------------\")\n",
    "    \n",
    "    # Track initial performance\n",
    "    rmse_history.append(([], baseline_authenticity_rmse))\n",
    "    baseline_rmse = baseline_authenticity_rmse\n",
    "    \n",
    "    # Iterate over the sorted indices and if removing a feature map improves performance, keep it removed\n",
    "    for idx, (channel_idx, importance_score) in enumerate(sorted_importance_scores):\n",
    "        channel_idx = int(channel_idx)\n",
    "        \n",
    "        # Temporarily zero out this feature map\n",
    "        layer.weight[channel_idx, ...] = 0\n",
    "        if layer.bias is not None:\n",
    "            layer.bias[channel_idx] = 0\n",
    "        \n",
    "        # Evaluate model with feature map removed\n",
    "        predictions = get_predictions(model, dataloader, device)\n",
    "        regression_errors = get_regression_errors(predictions)\n",
    "        authenticity_pruned_rmse = get_rmse(regression_errors)\n",
    "        \n",
    "        print(f\"Iteration {idx+1}/{len(sorted_importance_scores)}: \" +\n",
    "              f\"Testing removal of channel {channel_idx}, \" +\n",
    "              f\"Importance: {importance_score:.4f}, \" +\n",
    "              f\"RMSE: {authenticity_pruned_rmse:.4f}\")\n",
    "        \n",
    "        # Decide whether to keep this feature map removed\n",
    "        if authenticity_pruned_rmse < baseline_rmse:\n",
    "            baseline_rmse = authenticity_pruned_rmse # Update baseline RMSE\n",
    "            removed_features.append(channel_idx)\n",
    "            rmse_history.append((removed_features.copy(), baseline_rmse))\n",
    "            print(f\"  ✓ IMPROVING: Zeroing out feature map {channel_idx}\")\n",
    "        else:\n",
    "            # Restore the feature map\n",
    "            layer.weight[channel_idx, ...] = original_weights[channel_idx, ...]\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[channel_idx] = original_bias[channel_idx]\n",
    "            print(f\"  ✗ NOT IMPROVING: Keeping feature map {channel_idx}\")\n",
    "        \n",
    "        print(f\"  Current best RMSE: {baseline_rmse:.4f}\")\n",
    "        print(\"------------------\")\n",
    "    \n",
    "    # Final statistics\n",
    "    print(\"\\n------------------\")\n",
    "    print(f\"Final RMSE: {baseline_rmse:.4f} after removing {len(removed_features)} feature maps\")\n",
    "    print(f\"Improvement over baseline: {baseline_authenticity_rmse - baseline_rmse:.4f}\")\n",
    "    print(f\"Feature reduction: {(len(removed_features)/len(sorted_importance_scores))*100:.1f}%\")\n",
    "    \n",
    "    # Save the pruned model\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    return {\n",
    "        'removed_features': removed_features,\n",
    "        'baseline_rmse': baseline_authenticity_rmse,\n",
    "        'final_rmse': baseline_rmse,\n",
    "        'improvement': baseline_authenticity_rmse - baseline_rmse,\n",
    "        'reduction_percentage': (len(removed_features)/len(sorted_importance_scores))*100,\n",
    "        'rmse_history': rmse_history\n",
    "    }\n",
    "\n",
    "def remove_negative_impact_feature_maps(model, dataloader, device, layer_name, sorted_importance_scores, model_path='Models/negative_impact_pruned_model.pth'):\n",
    "    \"\"\"\n",
    "    Remove feature maps that have a negative impact on model performance based on importance scores (impotance score > 0).\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        dataloader: DataLoader for evaluation\n",
    "        device: Device to run the model on (cuda/cpu)\n",
    "        layer_name: Name of the layer to optimize\n",
    "        sorted_importance_scores: List of tuples (channel_index, importance_score) sorted by importance\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with pruning results and performance metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Get the target layer\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    \n",
    "    # Create a backup of the original weights and bias\n",
    "    original_weights = layer.weight.clone()\n",
    "    original_bias = layer.bias.clone() if layer.bias is not None else None\n",
    "    \n",
    "    # Get baseline performance\n",
    "    predictions = get_predictions(model, dataloader, device)\n",
    "    regression_errors = get_regression_errors(predictions)\n",
    "    authenticity_rmse = get_rmse(regression_errors)\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    removed_features = []\n",
    "    \n",
    "    # Iterate over the sorted indices and zero out all the feature maps that have a negative impact (importance < 0)\n",
    "\n",
    "    for idx, (channel_idx, importance_score) in enumerate(sorted_importance_scores):\n",
    "        print(f\"Iteration {idx} - Channel {channel_idx}: Importance score: {importance_score:.4f}\")\n",
    "        if importance_score > 0:\n",
    "            channel_idx = int(channel_idx)\n",
    "            layer.weight[channel_idx, ...] = 0\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[channel_idx] = 0\n",
    "            removed_features.append(channel_idx)\n",
    "\n",
    "    # Evaluate model with feature maps removed\n",
    "    new_predictions = get_predictions(model, dataloader, device)\n",
    "    new_regression_errors = get_regression_errors(new_predictions)\n",
    "    new_authenticity_rmse = get_rmse(new_regression_errors)\n",
    "\n",
    "    # Save the pruned model\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # Restore original weights for future use\n",
    "    layer.weight.data.copy_(original_weights)\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.copy_(original_bias)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'removed_features': removed_features,\n",
    "        'baseline_rmse': authenticity_rmse,\n",
    "        'final_rmse': new_authenticity_rmse,\n",
    "        'improvement': authenticity_rmse - new_authenticity_rmse,\n",
    "        'reduction_percentage': (len(removed_features)/len(sorted_importance_scores))*100\n",
    "    }\n",
    "\n",
    "def remove_channels(model,device,layer_name,channels_indexes)->AuthenticityPredictor:\n",
    "    \"\"\"\n",
    "    Remove channels, using an index list, from a convolutional layer in a model.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        device: Device to run the model on (cuda/cpu)\n",
    "        layer_name: Name of the layer to optimize\n",
    "        channels_indexes: List of channel indexes to remove\n",
    "        \n",
    "    Returns:\n",
    "        The pruned model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Get the target layer\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "        \n",
    "    # Zero out the specified channels\n",
    "    for channel_idx in channels_indexes:\n",
    "        layer.weight[channel_idx, ...] = 0\n",
    "        if layer.bias is not None:\n",
    "            layer.bias[channel_idx] = 0\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of diffrent models using different pruning techniques\n",
    "\n",
    "- Deletion of models is due to make sure that im not using the same model again and again (first draft, not sure if im correctlly restoring weights in each pruning technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1181092/337404404.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  base_model.load_state_dict(torch.load('Models/VGG-16_real_authenticity_finetuned.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance scores already computed, loading from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1181092/337404404.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  negative_impact_model.load_state_dict(torch.load('Models/VGG-16_real_authenticity_finetuned.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Channel 497.0: Importance score: 0.5250\n",
      "Iteration 1 - Channel 458.0: Importance score: 0.1914\n",
      "Iteration 2 - Channel 404.0: Importance score: 0.1733\n",
      "Iteration 3 - Channel 47.0: Importance score: 0.1677\n",
      "Iteration 4 - Channel 423.0: Importance score: 0.1609\n",
      "Iteration 5 - Channel 86.0: Importance score: 0.1514\n",
      "Iteration 6 - Channel 224.0: Importance score: 0.1420\n",
      "Iteration 7 - Channel 165.0: Importance score: 0.1310\n",
      "Iteration 8 - Channel 262.0: Importance score: 0.1286\n",
      "Iteration 9 - Channel 236.0: Importance score: 0.1250\n",
      "Iteration 10 - Channel 261.0: Importance score: 0.1216\n",
      "Iteration 11 - Channel 202.0: Importance score: 0.1190\n",
      "Iteration 12 - Channel 76.0: Importance score: 0.1121\n",
      "Iteration 13 - Channel 307.0: Importance score: 0.1117\n",
      "Iteration 14 - Channel 175.0: Importance score: 0.1090\n",
      "Iteration 15 - Channel 234.0: Importance score: 0.1058\n",
      "Iteration 16 - Channel 113.0: Importance score: 0.1050\n",
      "Iteration 17 - Channel 137.0: Importance score: 0.1041\n",
      "Iteration 18 - Channel 263.0: Importance score: 0.1010\n",
      "Iteration 19 - Channel 92.0: Importance score: 0.1009\n",
      "Iteration 20 - Channel 213.0: Importance score: 0.1000\n",
      "Iteration 21 - Channel 344.0: Importance score: 0.0920\n",
      "Iteration 22 - Channel 183.0: Importance score: 0.0909\n",
      "Iteration 23 - Channel 447.0: Importance score: 0.0901\n",
      "Iteration 24 - Channel 323.0: Importance score: 0.0858\n",
      "Iteration 25 - Channel 59.0: Importance score: 0.0850\n",
      "Iteration 26 - Channel 147.0: Importance score: 0.0834\n",
      "Iteration 27 - Channel 287.0: Importance score: 0.0823\n",
      "Iteration 28 - Channel 62.0: Importance score: 0.0821\n",
      "Iteration 29 - Channel 302.0: Importance score: 0.0812\n",
      "Iteration 30 - Channel 237.0: Importance score: 0.0793\n",
      "Iteration 31 - Channel 174.0: Importance score: 0.0787\n",
      "Iteration 32 - Channel 80.0: Importance score: 0.0748\n",
      "Iteration 33 - Channel 157.0: Importance score: 0.0705\n",
      "Iteration 34 - Channel 361.0: Importance score: 0.0698\n",
      "Iteration 35 - Channel 326.0: Importance score: 0.0684\n",
      "Iteration 36 - Channel 73.0: Importance score: 0.0670\n",
      "Iteration 37 - Channel 51.0: Importance score: 0.0658\n",
      "Iteration 38 - Channel 343.0: Importance score: 0.0657\n",
      "Iteration 39 - Channel 14.0: Importance score: 0.0652\n",
      "Iteration 40 - Channel 250.0: Importance score: 0.0634\n",
      "Iteration 41 - Channel 457.0: Importance score: 0.0622\n",
      "Iteration 42 - Channel 396.0: Importance score: 0.0605\n",
      "Iteration 43 - Channel 324.0: Importance score: 0.0603\n",
      "Iteration 44 - Channel 265.0: Importance score: 0.0594\n",
      "Iteration 45 - Channel 164.0: Importance score: 0.0594\n",
      "Iteration 46 - Channel 300.0: Importance score: 0.0591\n",
      "Iteration 47 - Channel 481.0: Importance score: 0.0586\n",
      "Iteration 48 - Channel 352.0: Importance score: 0.0575\n",
      "Iteration 49 - Channel 290.0: Importance score: 0.0572\n",
      "Iteration 50 - Channel 99.0: Importance score: 0.0555\n",
      "Iteration 51 - Channel 55.0: Importance score: 0.0551\n",
      "Iteration 52 - Channel 401.0: Importance score: 0.0551\n",
      "Iteration 53 - Channel 3.0: Importance score: 0.0549\n",
      "Iteration 54 - Channel 510.0: Importance score: 0.0521\n",
      "Iteration 55 - Channel 303.0: Importance score: 0.0515\n",
      "Iteration 56 - Channel 341.0: Importance score: 0.0513\n",
      "Iteration 57 - Channel 340.0: Importance score: 0.0511\n",
      "Iteration 58 - Channel 360.0: Importance score: 0.0504\n",
      "Iteration 59 - Channel 144.0: Importance score: 0.0496\n",
      "Iteration 60 - Channel 180.0: Importance score: 0.0485\n",
      "Iteration 61 - Channel 4.0: Importance score: 0.0480\n",
      "Iteration 62 - Channel 79.0: Importance score: 0.0478\n",
      "Iteration 63 - Channel 223.0: Importance score: 0.0477\n",
      "Iteration 64 - Channel 209.0: Importance score: 0.0450\n",
      "Iteration 65 - Channel 421.0: Importance score: 0.0448\n",
      "Iteration 66 - Channel 34.0: Importance score: 0.0439\n",
      "Iteration 67 - Channel 478.0: Importance score: 0.0436\n",
      "Iteration 68 - Channel 428.0: Importance score: 0.0435\n",
      "Iteration 69 - Channel 368.0: Importance score: 0.0432\n",
      "Iteration 70 - Channel 220.0: Importance score: 0.0430\n",
      "Iteration 71 - Channel 304.0: Importance score: 0.0419\n",
      "Iteration 72 - Channel 501.0: Importance score: 0.0419\n",
      "Iteration 73 - Channel 255.0: Importance score: 0.0416\n",
      "Iteration 74 - Channel 257.0: Importance score: 0.0414\n",
      "Iteration 75 - Channel 9.0: Importance score: 0.0410\n",
      "Iteration 76 - Channel 184.0: Importance score: 0.0407\n",
      "Iteration 77 - Channel 432.0: Importance score: 0.0401\n",
      "Iteration 78 - Channel 312.0: Importance score: 0.0388\n",
      "Iteration 79 - Channel 424.0: Importance score: 0.0388\n",
      "Iteration 80 - Channel 489.0: Importance score: 0.0384\n",
      "Iteration 81 - Channel 453.0: Importance score: 0.0368\n",
      "Iteration 82 - Channel 296.0: Importance score: 0.0365\n",
      "Iteration 83 - Channel 249.0: Importance score: 0.0364\n",
      "Iteration 84 - Channel 190.0: Importance score: 0.0361\n",
      "Iteration 85 - Channel 417.0: Importance score: 0.0352\n",
      "Iteration 86 - Channel 321.0: Importance score: 0.0351\n",
      "Iteration 87 - Channel 43.0: Importance score: 0.0338\n",
      "Iteration 88 - Channel 467.0: Importance score: 0.0336\n",
      "Iteration 89 - Channel 260.0: Importance score: 0.0333\n",
      "Iteration 90 - Channel 168.0: Importance score: 0.0330\n",
      "Iteration 91 - Channel 486.0: Importance score: 0.0323\n",
      "Iteration 92 - Channel 26.0: Importance score: 0.0319\n",
      "Iteration 93 - Channel 124.0: Importance score: 0.0316\n",
      "Iteration 94 - Channel 281.0: Importance score: 0.0315\n",
      "Iteration 95 - Channel 251.0: Importance score: 0.0313\n",
      "Iteration 96 - Channel 152.0: Importance score: 0.0312\n",
      "Iteration 97 - Channel 426.0: Importance score: 0.0311\n",
      "Iteration 98 - Channel 126.0: Importance score: 0.0311\n",
      "Iteration 99 - Channel 333.0: Importance score: 0.0307\n",
      "Iteration 100 - Channel 413.0: Importance score: 0.0303\n",
      "Iteration 101 - Channel 24.0: Importance score: 0.0300\n",
      "Iteration 102 - Channel 285.0: Importance score: 0.0299\n",
      "Iteration 103 - Channel 482.0: Importance score: 0.0295\n",
      "Iteration 104 - Channel 95.0: Importance score: 0.0294\n",
      "Iteration 105 - Channel 130.0: Importance score: 0.0290\n",
      "Iteration 106 - Channel 438.0: Importance score: 0.0284\n",
      "Iteration 107 - Channel 163.0: Importance score: 0.0271\n",
      "Iteration 108 - Channel 198.0: Importance score: 0.0267\n",
      "Iteration 109 - Channel 93.0: Importance score: 0.0267\n",
      "Iteration 110 - Channel 205.0: Importance score: 0.0257\n",
      "Iteration 111 - Channel 176.0: Importance score: 0.0255\n",
      "Iteration 112 - Channel 311.0: Importance score: 0.0244\n",
      "Iteration 113 - Channel 331.0: Importance score: 0.0241\n",
      "Iteration 114 - Channel 136.0: Importance score: 0.0239\n",
      "Iteration 115 - Channel 375.0: Importance score: 0.0236\n",
      "Iteration 116 - Channel 121.0: Importance score: 0.0228\n",
      "Iteration 117 - Channel 39.0: Importance score: 0.0220\n",
      "Iteration 118 - Channel 278.0: Importance score: 0.0218\n",
      "Iteration 119 - Channel 377.0: Importance score: 0.0215\n",
      "Iteration 120 - Channel 498.0: Importance score: 0.0211\n",
      "Iteration 121 - Channel 384.0: Importance score: 0.0203\n",
      "Iteration 122 - Channel 110.0: Importance score: 0.0203\n",
      "Iteration 123 - Channel 308.0: Importance score: 0.0200\n",
      "Iteration 124 - Channel 480.0: Importance score: 0.0196\n",
      "Iteration 125 - Channel 320.0: Importance score: 0.0195\n",
      "Iteration 126 - Channel 305.0: Importance score: 0.0192\n",
      "Iteration 127 - Channel 496.0: Importance score: 0.0192\n",
      "Iteration 128 - Channel 422.0: Importance score: 0.0191\n",
      "Iteration 129 - Channel 29.0: Importance score: 0.0186\n",
      "Iteration 130 - Channel 61.0: Importance score: 0.0184\n",
      "Iteration 131 - Channel 221.0: Importance score: 0.0174\n",
      "Iteration 132 - Channel 112.0: Importance score: 0.0174\n",
      "Iteration 133 - Channel 437.0: Importance score: 0.0171\n",
      "Iteration 134 - Channel 385.0: Importance score: 0.0170\n",
      "Iteration 135 - Channel 19.0: Importance score: 0.0170\n",
      "Iteration 136 - Channel 129.0: Importance score: 0.0169\n",
      "Iteration 137 - Channel 464.0: Importance score: 0.0164\n",
      "Iteration 138 - Channel 239.0: Importance score: 0.0163\n",
      "Iteration 139 - Channel 325.0: Importance score: 0.0162\n",
      "Iteration 140 - Channel 499.0: Importance score: 0.0149\n",
      "Iteration 141 - Channel 242.0: Importance score: 0.0143\n",
      "Iteration 142 - Channel 146.0: Importance score: 0.0140\n",
      "Iteration 143 - Channel 430.0: Importance score: 0.0136\n",
      "Iteration 144 - Channel 490.0: Importance score: 0.0132\n",
      "Iteration 145 - Channel 40.0: Importance score: 0.0125\n",
      "Iteration 146 - Channel 284.0: Importance score: 0.0124\n",
      "Iteration 147 - Channel 435.0: Importance score: 0.0119\n",
      "Iteration 148 - Channel 310.0: Importance score: 0.0119\n",
      "Iteration 149 - Channel 484.0: Importance score: 0.0115\n",
      "Iteration 150 - Channel 322.0: Importance score: 0.0112\n",
      "Iteration 151 - Channel 494.0: Importance score: 0.0112\n",
      "Iteration 152 - Channel 69.0: Importance score: 0.0108\n",
      "Iteration 153 - Channel 177.0: Importance score: 0.0105\n",
      "Iteration 154 - Channel 402.0: Importance score: 0.0103\n",
      "Iteration 155 - Channel 5.0: Importance score: 0.0100\n",
      "Iteration 156 - Channel 443.0: Importance score: 0.0097\n",
      "Iteration 157 - Channel 505.0: Importance score: 0.0097\n",
      "Iteration 158 - Channel 472.0: Importance score: 0.0096\n",
      "Iteration 159 - Channel 362.0: Importance score: 0.0094\n",
      "Iteration 160 - Channel 100.0: Importance score: 0.0094\n",
      "Iteration 161 - Channel 448.0: Importance score: 0.0085\n",
      "Iteration 162 - Channel 390.0: Importance score: 0.0085\n",
      "Iteration 163 - Channel 338.0: Importance score: 0.0084\n",
      "Iteration 164 - Channel 293.0: Importance score: 0.0083\n",
      "Iteration 165 - Channel 115.0: Importance score: 0.0082\n",
      "Iteration 166 - Channel 238.0: Importance score: 0.0081\n",
      "Iteration 167 - Channel 49.0: Importance score: 0.0078\n",
      "Iteration 168 - Channel 134.0: Importance score: 0.0072\n",
      "Iteration 169 - Channel 347.0: Importance score: 0.0068\n",
      "Iteration 170 - Channel 27.0: Importance score: 0.0067\n",
      "Iteration 171 - Channel 13.0: Importance score: 0.0063\n",
      "Iteration 172 - Channel 434.0: Importance score: 0.0060\n",
      "Iteration 173 - Channel 227.0: Importance score: 0.0059\n",
      "Iteration 174 - Channel 332.0: Importance score: 0.0055\n",
      "Iteration 175 - Channel 1.0: Importance score: 0.0052\n",
      "Iteration 176 - Channel 452.0: Importance score: 0.0051\n",
      "Iteration 177 - Channel 483.0: Importance score: 0.0050\n",
      "Iteration 178 - Channel 153.0: Importance score: 0.0040\n",
      "Iteration 179 - Channel 455.0: Importance score: 0.0025\n",
      "Iteration 180 - Channel 54.0: Importance score: 0.0024\n",
      "Iteration 181 - Channel 418.0: Importance score: 0.0024\n",
      "Iteration 182 - Channel 364.0: Importance score: 0.0019\n",
      "Iteration 183 - Channel 64.0: Importance score: 0.0018\n",
      "Iteration 184 - Channel 474.0: Importance score: 0.0013\n",
      "Iteration 185 - Channel 214.0: Importance score: 0.0012\n",
      "Iteration 186 - Channel 197.0: Importance score: 0.0006\n",
      "Iteration 187 - Channel 217.0: Importance score: 0.0005\n",
      "Iteration 188 - Channel 30.0: Importance score: 0.0003\n",
      "Iteration 189 - Channel 96.0: Importance score: -0.0011\n",
      "Iteration 190 - Channel 387.0: Importance score: -0.0023\n",
      "Iteration 191 - Channel 386.0: Importance score: -0.0023\n",
      "Iteration 192 - Channel 398.0: Importance score: -0.0026\n",
      "Iteration 193 - Channel 459.0: Importance score: -0.0028\n",
      "Iteration 194 - Channel 222.0: Importance score: -0.0029\n",
      "Iteration 195 - Channel 196.0: Importance score: -0.0029\n",
      "Iteration 196 - Channel 509.0: Importance score: -0.0032\n",
      "Iteration 197 - Channel 142.0: Importance score: -0.0037\n",
      "Iteration 198 - Channel 309.0: Importance score: -0.0040\n",
      "Iteration 199 - Channel 120.0: Importance score: -0.0050\n",
      "Iteration 200 - Channel 226.0: Importance score: -0.0053\n",
      "Iteration 201 - Channel 45.0: Importance score: -0.0060\n",
      "Iteration 202 - Channel 442.0: Importance score: -0.0061\n",
      "Iteration 203 - Channel 456.0: Importance score: -0.0071\n",
      "Iteration 204 - Channel 212.0: Importance score: -0.0072\n",
      "Iteration 205 - Channel 445.0: Importance score: -0.0075\n",
      "Iteration 206 - Channel 388.0: Importance score: -0.0076\n",
      "Iteration 207 - Channel 235.0: Importance score: -0.0076\n",
      "Iteration 208 - Channel 256.0: Importance score: -0.0078\n",
      "Iteration 209 - Channel 105.0: Importance score: -0.0079\n",
      "Iteration 210 - Channel 511.0: Importance score: -0.0082\n",
      "Iteration 211 - Channel 470.0: Importance score: -0.0089\n",
      "Iteration 212 - Channel 185.0: Importance score: -0.0094\n",
      "Iteration 213 - Channel 345.0: Importance score: -0.0101\n",
      "Iteration 214 - Channel 8.0: Importance score: -0.0106\n",
      "Iteration 215 - Channel 378.0: Importance score: -0.0107\n",
      "Iteration 216 - Channel 382.0: Importance score: -0.0108\n",
      "Iteration 217 - Channel 292.0: Importance score: -0.0110\n",
      "Iteration 218 - Channel 451.0: Importance score: -0.0115\n",
      "Iteration 219 - Channel 371.0: Importance score: -0.0117\n",
      "Iteration 220 - Channel 276.0: Importance score: -0.0120\n",
      "Iteration 221 - Channel 7.0: Importance score: -0.0128\n",
      "Iteration 222 - Channel 488.0: Importance score: -0.0135\n",
      "Iteration 223 - Channel 159.0: Importance score: -0.0136\n",
      "Iteration 224 - Channel 162.0: Importance score: -0.0138\n",
      "Iteration 225 - Channel 44.0: Importance score: -0.0138\n",
      "Iteration 226 - Channel 473.0: Importance score: -0.0138\n",
      "Iteration 227 - Channel 277.0: Importance score: -0.0140\n",
      "Iteration 228 - Channel 441.0: Importance score: -0.0141\n",
      "Iteration 229 - Channel 90.0: Importance score: -0.0144\n",
      "Iteration 230 - Channel 211.0: Importance score: -0.0145\n",
      "Iteration 231 - Channel 68.0: Importance score: -0.0147\n",
      "Iteration 232 - Channel 394.0: Importance score: -0.0147\n",
      "Iteration 233 - Channel 219.0: Importance score: -0.0148\n",
      "Iteration 234 - Channel 336.0: Importance score: -0.0158\n",
      "Iteration 235 - Channel 274.0: Importance score: -0.0161\n",
      "Iteration 236 - Channel 425.0: Importance score: -0.0167\n",
      "Iteration 237 - Channel 193.0: Importance score: -0.0171\n",
      "Iteration 238 - Channel 98.0: Importance score: -0.0172\n",
      "Iteration 239 - Channel 485.0: Importance score: -0.0173\n",
      "Iteration 240 - Channel 74.0: Importance score: -0.0174\n",
      "Iteration 241 - Channel 476.0: Importance score: -0.0183\n",
      "Iteration 242 - Channel 353.0: Importance score: -0.0184\n",
      "Iteration 243 - Channel 282.0: Importance score: -0.0191\n",
      "Iteration 244 - Channel 416.0: Importance score: -0.0193\n",
      "Iteration 245 - Channel 373.0: Importance score: -0.0194\n",
      "Iteration 246 - Channel 150.0: Importance score: -0.0197\n",
      "Iteration 247 - Channel 207.0: Importance score: -0.0199\n",
      "Iteration 248 - Channel 460.0: Importance score: -0.0200\n",
      "Iteration 249 - Channel 405.0: Importance score: -0.0205\n",
      "Iteration 250 - Channel 87.0: Importance score: -0.0208\n",
      "Iteration 251 - Channel 400.0: Importance score: -0.0210\n",
      "Iteration 252 - Channel 245.0: Importance score: -0.0210\n",
      "Iteration 253 - Channel 83.0: Importance score: -0.0217\n",
      "Iteration 254 - Channel 334.0: Importance score: -0.0217\n",
      "Iteration 255 - Channel 32.0: Importance score: -0.0231\n",
      "Iteration 256 - Channel 420.0: Importance score: -0.0234\n",
      "Iteration 257 - Channel 463.0: Importance score: -0.0240\n",
      "Iteration 258 - Channel 128.0: Importance score: -0.0243\n",
      "Iteration 259 - Channel 299.0: Importance score: -0.0246\n",
      "Iteration 260 - Channel 48.0: Importance score: -0.0246\n",
      "Iteration 261 - Channel 380.0: Importance score: -0.0249\n",
      "Iteration 262 - Channel 102.0: Importance score: -0.0253\n",
      "Iteration 263 - Channel 36.0: Importance score: -0.0258\n",
      "Iteration 264 - Channel 94.0: Importance score: -0.0262\n",
      "Iteration 265 - Channel 28.0: Importance score: -0.0266\n",
      "Iteration 266 - Channel 85.0: Importance score: -0.0267\n",
      "Iteration 267 - Channel 22.0: Importance score: -0.0268\n",
      "Iteration 268 - Channel 191.0: Importance score: -0.0271\n",
      "Iteration 269 - Channel 391.0: Importance score: -0.0273\n",
      "Iteration 270 - Channel 427.0: Importance score: -0.0279\n",
      "Iteration 271 - Channel 118.0: Importance score: -0.0282\n",
      "Iteration 272 - Channel 228.0: Importance score: -0.0292\n",
      "Iteration 273 - Channel 407.0: Importance score: -0.0296\n",
      "Iteration 274 - Channel 346.0: Importance score: -0.0300\n",
      "Iteration 275 - Channel 264.0: Importance score: -0.0302\n",
      "Iteration 276 - Channel 395.0: Importance score: -0.0306\n",
      "Iteration 277 - Channel 269.0: Importance score: -0.0307\n",
      "Iteration 278 - Channel 6.0: Importance score: -0.0307\n",
      "Iteration 279 - Channel 356.0: Importance score: -0.0307\n",
      "Iteration 280 - Channel 215.0: Importance score: -0.0316\n",
      "Iteration 281 - Channel 154.0: Importance score: -0.0316\n",
      "Iteration 282 - Channel 81.0: Importance score: -0.0322\n",
      "Iteration 283 - Channel 35.0: Importance score: -0.0326\n",
      "Iteration 284 - Channel 186.0: Importance score: -0.0327\n",
      "Iteration 285 - Channel 151.0: Importance score: -0.0333\n",
      "Iteration 286 - Channel 204.0: Importance score: -0.0339\n",
      "Iteration 287 - Channel 399.0: Importance score: -0.0344\n",
      "Iteration 288 - Channel 16.0: Importance score: -0.0345\n",
      "Iteration 289 - Channel 233.0: Importance score: -0.0346\n",
      "Iteration 290 - Channel 389.0: Importance score: -0.0347\n",
      "Iteration 291 - Channel 411.0: Importance score: -0.0352\n",
      "Iteration 292 - Channel 109.0: Importance score: -0.0354\n",
      "Iteration 293 - Channel 125.0: Importance score: -0.0357\n",
      "Iteration 294 - Channel 294.0: Importance score: -0.0357\n",
      "Iteration 295 - Channel 436.0: Importance score: -0.0359\n",
      "Iteration 296 - Channel 138.0: Importance score: -0.0364\n",
      "Iteration 297 - Channel 70.0: Importance score: -0.0365\n",
      "Iteration 298 - Channel 367.0: Importance score: -0.0369\n",
      "Iteration 299 - Channel 313.0: Importance score: -0.0372\n",
      "Iteration 300 - Channel 116.0: Importance score: -0.0376\n",
      "Iteration 301 - Channel 315.0: Importance score: -0.0383\n",
      "Iteration 302 - Channel 230.0: Importance score: -0.0389\n",
      "Iteration 303 - Channel 461.0: Importance score: -0.0389\n",
      "Iteration 304 - Channel 60.0: Importance score: -0.0390\n",
      "Iteration 305 - Channel 288.0: Importance score: -0.0395\n",
      "Iteration 306 - Channel 248.0: Importance score: -0.0398\n",
      "Iteration 307 - Channel 101.0: Importance score: -0.0399\n",
      "Iteration 308 - Channel 149.0: Importance score: -0.0412\n",
      "Iteration 309 - Channel 273.0: Importance score: -0.0417\n",
      "Iteration 310 - Channel 77.0: Importance score: -0.0417\n",
      "Iteration 311 - Channel 393.0: Importance score: -0.0417\n",
      "Iteration 312 - Channel 289.0: Importance score: -0.0419\n",
      "Iteration 313 - Channel 139.0: Importance score: -0.0427\n",
      "Iteration 314 - Channel 188.0: Importance score: -0.0444\n",
      "Iteration 315 - Channel 58.0: Importance score: -0.0446\n",
      "Iteration 316 - Channel 327.0: Importance score: -0.0451\n",
      "Iteration 317 - Channel 65.0: Importance score: -0.0465\n",
      "Iteration 318 - Channel 506.0: Importance score: -0.0476\n",
      "Iteration 319 - Channel 218.0: Importance score: -0.0487\n",
      "Iteration 320 - Channel 111.0: Importance score: -0.0488\n",
      "Iteration 321 - Channel 2.0: Importance score: -0.0498\n",
      "Iteration 322 - Channel 295.0: Importance score: -0.0502\n",
      "Iteration 323 - Channel 169.0: Importance score: -0.0504\n",
      "Iteration 324 - Channel 454.0: Importance score: -0.0506\n",
      "Iteration 325 - Channel 381.0: Importance score: -0.0507\n",
      "Iteration 326 - Channel 383.0: Importance score: -0.0514\n",
      "Iteration 327 - Channel 203.0: Importance score: -0.0529\n",
      "Iteration 328 - Channel 141.0: Importance score: -0.0541\n",
      "Iteration 329 - Channel 52.0: Importance score: -0.0554\n",
      "Iteration 330 - Channel 275.0: Importance score: -0.0556\n",
      "Iteration 331 - Channel 392.0: Importance score: -0.0557\n",
      "Iteration 332 - Channel 328.0: Importance score: -0.0558\n",
      "Iteration 333 - Channel 406.0: Importance score: -0.0572\n",
      "Iteration 334 - Channel 301.0: Importance score: -0.0576\n",
      "Iteration 335 - Channel 280.0: Importance score: -0.0580\n",
      "Iteration 336 - Channel 500.0: Importance score: -0.0580\n",
      "Iteration 337 - Channel 258.0: Importance score: -0.0585\n",
      "Iteration 338 - Channel 335.0: Importance score: -0.0586\n",
      "Iteration 339 - Channel 283.0: Importance score: -0.0617\n",
      "Iteration 340 - Channel 446.0: Importance score: -0.0621\n",
      "Iteration 341 - Channel 88.0: Importance score: -0.0634\n",
      "Iteration 342 - Channel 259.0: Importance score: -0.0637\n",
      "Iteration 343 - Channel 155.0: Importance score: -0.0644\n",
      "Iteration 344 - Channel 357.0: Importance score: -0.0648\n",
      "Iteration 345 - Channel 374.0: Importance score: -0.0653\n",
      "Iteration 346 - Channel 106.0: Importance score: -0.0653\n",
      "Iteration 347 - Channel 25.0: Importance score: -0.0656\n",
      "Iteration 348 - Channel 189.0: Importance score: -0.0658\n",
      "Iteration 349 - Channel 267.0: Importance score: -0.0661\n",
      "Iteration 350 - Channel 495.0: Importance score: -0.0664\n",
      "Iteration 351 - Channel 358.0: Importance score: -0.0673\n",
      "Iteration 352 - Channel 369.0: Importance score: -0.0675\n",
      "Iteration 353 - Channel 318.0: Importance score: -0.0699\n",
      "Iteration 354 - Channel 50.0: Importance score: -0.0709\n",
      "Iteration 355 - Channel 17.0: Importance score: -0.0709\n",
      "Iteration 356 - Channel 433.0: Importance score: -0.0713\n",
      "Iteration 357 - Channel 493.0: Importance score: -0.0719\n",
      "Iteration 358 - Channel 316.0: Importance score: -0.0721\n",
      "Iteration 359 - Channel 330.0: Importance score: -0.0734\n",
      "Iteration 360 - Channel 297.0: Importance score: -0.0737\n",
      "Iteration 361 - Channel 507.0: Importance score: -0.0738\n",
      "Iteration 362 - Channel 216.0: Importance score: -0.0747\n",
      "Iteration 363 - Channel 477.0: Importance score: -0.0748\n",
      "Iteration 364 - Channel 182.0: Importance score: -0.0750\n",
      "Iteration 365 - Channel 108.0: Importance score: -0.0750\n",
      "Iteration 366 - Channel 409.0: Importance score: -0.0758\n",
      "Iteration 367 - Channel 408.0: Importance score: -0.0759\n",
      "Iteration 368 - Channel 15.0: Importance score: -0.0762\n",
      "Iteration 369 - Channel 67.0: Importance score: -0.0763\n",
      "Iteration 370 - Channel 10.0: Importance score: -0.0768\n",
      "Iteration 371 - Channel 46.0: Importance score: -0.0775\n",
      "Iteration 372 - Channel 229.0: Importance score: -0.0777\n",
      "Iteration 373 - Channel 469.0: Importance score: -0.0777\n",
      "Iteration 374 - Channel 268.0: Importance score: -0.0781\n",
      "Iteration 375 - Channel 254.0: Importance score: -0.0781\n",
      "Iteration 376 - Channel 329.0: Importance score: -0.0785\n",
      "Iteration 377 - Channel 21.0: Importance score: -0.0792\n",
      "Iteration 378 - Channel 166.0: Importance score: -0.0796\n",
      "Iteration 379 - Channel 104.0: Importance score: -0.0797\n",
      "Iteration 380 - Channel 107.0: Importance score: -0.0809\n",
      "Iteration 381 - Channel 131.0: Importance score: -0.0825\n",
      "Iteration 382 - Channel 270.0: Importance score: -0.0833\n",
      "Iteration 383 - Channel 0.0: Importance score: -0.0841\n",
      "Iteration 384 - Channel 354.0: Importance score: -0.0843\n",
      "Iteration 385 - Channel 161.0: Importance score: -0.0849\n",
      "Iteration 386 - Channel 403.0: Importance score: -0.0850\n",
      "Iteration 387 - Channel 492.0: Importance score: -0.0871\n",
      "Iteration 388 - Channel 244.0: Importance score: -0.0879\n",
      "Iteration 389 - Channel 172.0: Importance score: -0.0898\n",
      "Iteration 390 - Channel 143.0: Importance score: -0.0910\n",
      "Iteration 391 - Channel 376.0: Importance score: -0.0921\n",
      "Iteration 392 - Channel 231.0: Importance score: -0.0922\n",
      "Iteration 393 - Channel 192.0: Importance score: -0.0930\n",
      "Iteration 394 - Channel 504.0: Importance score: -0.0941\n",
      "Iteration 395 - Channel 173.0: Importance score: -0.0961\n",
      "Iteration 396 - Channel 200.0: Importance score: -0.0962\n",
      "Iteration 397 - Channel 56.0: Importance score: -0.0965\n",
      "Iteration 398 - Channel 18.0: Importance score: -0.0967\n",
      "Iteration 399 - Channel 247.0: Importance score: -0.0976\n",
      "Iteration 400 - Channel 487.0: Importance score: -0.0981\n",
      "Iteration 401 - Channel 89.0: Importance score: -0.0984\n",
      "Iteration 402 - Channel 272.0: Importance score: -0.0984\n",
      "Iteration 403 - Channel 187.0: Importance score: -0.0985\n",
      "Iteration 404 - Channel 314.0: Importance score: -0.0989\n",
      "Iteration 405 - Channel 122.0: Importance score: -0.0997\n",
      "Iteration 406 - Channel 140.0: Importance score: -0.1002\n",
      "Iteration 407 - Channel 475.0: Importance score: -0.1012\n",
      "Iteration 408 - Channel 181.0: Importance score: -0.1021\n",
      "Iteration 409 - Channel 91.0: Importance score: -0.1040\n",
      "Iteration 410 - Channel 471.0: Importance score: -0.1076\n",
      "Iteration 411 - Channel 415.0: Importance score: -0.1079\n",
      "Iteration 412 - Channel 246.0: Importance score: -0.1085\n",
      "Iteration 413 - Channel 243.0: Importance score: -0.1095\n",
      "Iteration 414 - Channel 201.0: Importance score: -0.1107\n",
      "Iteration 415 - Channel 359.0: Importance score: -0.1122\n",
      "Iteration 416 - Channel 271.0: Importance score: -0.1124\n",
      "Iteration 417 - Channel 349.0: Importance score: -0.1131\n",
      "Iteration 418 - Channel 266.0: Importance score: -0.1171\n",
      "Iteration 419 - Channel 241.0: Importance score: -0.1174\n",
      "Iteration 420 - Channel 72.0: Importance score: -0.1182\n",
      "Iteration 421 - Channel 286.0: Importance score: -0.1188\n",
      "Iteration 422 - Channel 414.0: Importance score: -0.1216\n",
      "Iteration 423 - Channel 123.0: Importance score: -0.1226\n",
      "Iteration 424 - Channel 41.0: Importance score: -0.1231\n",
      "Iteration 425 - Channel 317.0: Importance score: -0.1236\n",
      "Iteration 426 - Channel 31.0: Importance score: -0.1245\n",
      "Iteration 427 - Channel 370.0: Importance score: -0.1269\n",
      "Iteration 428 - Channel 178.0: Importance score: -0.1270\n",
      "Iteration 429 - Channel 431.0: Importance score: -0.1293\n",
      "Iteration 430 - Channel 160.0: Importance score: -0.1312\n",
      "Iteration 431 - Channel 148.0: Importance score: -0.1313\n",
      "Iteration 432 - Channel 195.0: Importance score: -0.1320\n",
      "Iteration 433 - Channel 466.0: Importance score: -0.1340\n",
      "Iteration 434 - Channel 167.0: Importance score: -0.1358\n",
      "Iteration 435 - Channel 429.0: Importance score: -0.1360\n",
      "Iteration 436 - Channel 337.0: Importance score: -0.1365\n",
      "Iteration 437 - Channel 440.0: Importance score: -0.1392\n",
      "Iteration 438 - Channel 410.0: Importance score: -0.1406\n",
      "Iteration 439 - Channel 170.0: Importance score: -0.1406\n",
      "Iteration 440 - Channel 132.0: Importance score: -0.1413\n",
      "Iteration 441 - Channel 145.0: Importance score: -0.1436\n",
      "Iteration 442 - Channel 199.0: Importance score: -0.1438\n",
      "Iteration 443 - Channel 449.0: Importance score: -0.1441\n",
      "Iteration 444 - Channel 75.0: Importance score: -0.1453\n",
      "Iteration 445 - Channel 179.0: Importance score: -0.1461\n",
      "Iteration 446 - Channel 319.0: Importance score: -0.1483\n",
      "Iteration 447 - Channel 351.0: Importance score: -0.1506\n",
      "Iteration 448 - Channel 306.0: Importance score: -0.1530\n",
      "Iteration 449 - Channel 103.0: Importance score: -0.1546\n",
      "Iteration 450 - Channel 78.0: Importance score: -0.1552\n",
      "Iteration 451 - Channel 20.0: Importance score: -0.1553\n",
      "Iteration 452 - Channel 365.0: Importance score: -0.1614\n",
      "Iteration 453 - Channel 57.0: Importance score: -0.1639\n",
      "Iteration 454 - Channel 127.0: Importance score: -0.1639\n",
      "Iteration 455 - Channel 252.0: Importance score: -0.1650\n",
      "Iteration 456 - Channel 156.0: Importance score: -0.1656\n",
      "Iteration 457 - Channel 508.0: Importance score: -0.1676\n",
      "Iteration 458 - Channel 339.0: Importance score: -0.1763\n",
      "Iteration 459 - Channel 412.0: Importance score: -0.1802\n",
      "Iteration 460 - Channel 350.0: Importance score: -0.1811\n",
      "Iteration 461 - Channel 84.0: Importance score: -0.1812\n",
      "Iteration 462 - Channel 114.0: Importance score: -0.1849\n",
      "Iteration 463 - Channel 38.0: Importance score: -0.1857\n",
      "Iteration 464 - Channel 133.0: Importance score: -0.1861\n",
      "Iteration 465 - Channel 503.0: Importance score: -0.1869\n",
      "Iteration 466 - Channel 66.0: Importance score: -0.1875\n",
      "Iteration 467 - Channel 240.0: Importance score: -0.1941\n",
      "Iteration 468 - Channel 53.0: Importance score: -0.1954\n",
      "Iteration 469 - Channel 465.0: Importance score: -0.1981\n",
      "Iteration 470 - Channel 366.0: Importance score: -0.2004\n",
      "Iteration 471 - Channel 12.0: Importance score: -0.2169\n",
      "Iteration 472 - Channel 348.0: Importance score: -0.2198\n",
      "Iteration 473 - Channel 37.0: Importance score: -0.2203\n",
      "Iteration 474 - Channel 491.0: Importance score: -0.2263\n",
      "Iteration 475 - Channel 11.0: Importance score: -0.2270\n",
      "Iteration 476 - Channel 232.0: Importance score: -0.2301\n",
      "Iteration 477 - Channel 298.0: Importance score: -0.2479\n",
      "Iteration 478 - Channel 71.0: Importance score: -0.2486\n",
      "Iteration 479 - Channel 342.0: Importance score: -0.2530\n",
      "Iteration 480 - Channel 379.0: Importance score: -0.2604\n",
      "Iteration 481 - Channel 419.0: Importance score: -0.2623\n",
      "Iteration 482 - Channel 502.0: Importance score: -0.2669\n",
      "Iteration 483 - Channel 158.0: Importance score: -0.2752\n",
      "Iteration 484 - Channel 253.0: Importance score: -0.2806\n",
      "Iteration 485 - Channel 363.0: Importance score: -0.2857\n",
      "Iteration 486 - Channel 355.0: Importance score: -0.2884\n",
      "Iteration 487 - Channel 97.0: Importance score: -0.2898\n",
      "Iteration 488 - Channel 397.0: Importance score: -0.3026\n",
      "Iteration 489 - Channel 42.0: Importance score: -0.3099\n",
      "Iteration 490 - Channel 82.0: Importance score: -0.3113\n",
      "Iteration 491 - Channel 194.0: Importance score: -0.3243\n",
      "Iteration 492 - Channel 225.0: Importance score: -0.3274\n",
      "Iteration 493 - Channel 279.0: Importance score: -0.3289\n",
      "Iteration 494 - Channel 208.0: Importance score: -0.3449\n",
      "Iteration 495 - Channel 462.0: Importance score: -0.3478\n",
      "Iteration 496 - Channel 171.0: Importance score: -0.3579\n",
      "Iteration 497 - Channel 33.0: Importance score: -0.3690\n",
      "Iteration 498 - Channel 119.0: Importance score: -0.3705\n",
      "Iteration 499 - Channel 63.0: Importance score: -0.3897\n",
      "Iteration 500 - Channel 372.0: Importance score: -0.3929\n",
      "Iteration 501 - Channel 23.0: Importance score: -0.4159\n",
      "Iteration 502 - Channel 117.0: Importance score: -0.4346\n",
      "Iteration 503 - Channel 206.0: Importance score: -0.4631\n",
      "Iteration 504 - Channel 439.0: Importance score: -0.4904\n",
      "Iteration 505 - Channel 291.0: Importance score: -0.5730\n",
      "Iteration 506 - Channel 210.0: Importance score: -0.6254\n",
      "Iteration 507 - Channel 135.0: Importance score: -0.6531\n",
      "Iteration 508 - Channel 468.0: Importance score: -0.8464\n",
      "Iteration 509 - Channel 479.0: Importance score: -0.8473\n",
      "Iteration 510 - Channel 450.0: Importance score: -0.9953\n",
      "Iteration 511 - Channel 444.0: Importance score: -1.0634\n",
      "{'removed_features': [96, 387, 386, 398, 459, 222, 196, 509, 142, 309, 120, 226, 45, 442, 456, 212, 445, 388, 235, 256, 105, 511, 470, 185, 345, 8, 378, 382, 292, 451, 371, 276, 7, 488, 159, 162, 44, 473, 277, 441, 90, 211, 68, 394, 219, 336, 274, 425, 193, 98, 485, 74, 476, 353, 282, 416, 373, 150, 207, 460, 405, 87, 400, 245, 83, 334, 32, 420, 463, 128, 299, 48, 380, 102, 36, 94, 28, 85, 22, 191, 391, 427, 118, 228, 407, 346, 264, 395, 269, 6, 356, 215, 154, 81, 35, 186, 151, 204, 399, 16, 233, 389, 411, 109, 125, 294, 436, 138, 70, 367, 313, 116, 315, 230, 461, 60, 288, 248, 101, 149, 273, 77, 393, 289, 139, 188, 58, 327, 65, 506, 218, 111, 2, 295, 169, 454, 381, 383, 203, 141, 52, 275, 392, 328, 406, 301, 280, 500, 258, 335, 283, 446, 88, 259, 155, 357, 374, 106, 25, 189, 267, 495, 358, 369, 318, 50, 17, 433, 493, 316, 330, 297, 507, 216, 477, 182, 108, 409, 408, 15, 67, 10, 46, 229, 469, 268, 254, 329, 21, 166, 104, 107, 131, 270, 0, 354, 161, 403, 492, 244, 172, 143, 376, 231, 192, 504, 173, 200, 56, 18, 247, 487, 89, 272, 187, 314, 122, 140, 475, 181, 91, 471, 415, 246, 243, 201, 359, 271, 349, 266, 241, 72, 286, 414, 123, 41, 317, 31, 370, 178, 431, 160, 148, 195, 466, 167, 429, 337, 440, 410, 170, 132, 145, 199, 449, 75, 179, 319, 351, 306, 103, 78, 20, 365, 57, 127, 252, 156, 508, 339, 412, 350, 84, 114, 38, 133, 503, 66, 240, 53, 465, 366, 12, 348, 37, 491, 11, 232, 298, 71, 342, 379, 419, 502, 158, 253, 363, 355, 97, 397, 42, 82, 194, 225, 279, 208, 462, 171, 33, 119, 63, 372, 23, 117, 206, 439, 291, 210, 135, 468, 479, 450, 444], 'baseline_rmse': tensor(11.1166), 'final_rmse': tensor(45.8172), 'improvement': tensor(-34.7006), 'reduction_percentage': 63.0859375}\n"
     ]
    }
   ],
   "source": [
    "# LAYER to prune\n",
    "LAYER = 'features.28'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# Base model for importance score computation\n",
    "base_model = AuthenticityPredictor()\n",
    "base_model.load_state_dict(torch.load('Models/VGG-16_real_authenticity_finetuned.pth'))\n",
    "base_model.eval()\n",
    "base_model.to(DEVICE)\n",
    "\n",
    "sorted_importance_scores = compute_feature_map_importance(base_model, train_dataloader, DEVICE, LAYER)\n",
    "np.save('Ranking_arrays/real_authenticity_importance_scores.npy', sorted_importance_scores)\n",
    "del base_model\n",
    "\n",
    "# Model for negative impact feature maps removal\n",
    "negative_impact_model = AuthenticityPredictor()\n",
    "negative_impact_model.load_state_dict(torch.load('Models/VGG-16_real_authenticity_finetuned.pth'))\n",
    "negative_impact_model.eval()\n",
    "negative_impact_model.to(DEVICE)\n",
    "\n",
    "negative_impact_subset = remove_negative_impact_feature_maps(negative_impact_model, train_dataloader, DEVICE, LAYER, sorted_importance_scores, model_path='Models/real_authenticity_negative_impact_pruned_model.pth')\n",
    "print(negative_impact_subset)\n",
    "\n",
    "del negative_impact_model\n",
    "\n",
    "\n",
    "# # Model for noisy feature maps removal\n",
    "# noisy_pruning_model = AuthenticityPredictor()\n",
    "# noisy_pruning_model.load_state_dict(torch.load('Models/VGG-16_real_authenticity_finetuned.pth'))\n",
    "# noisy_pruning_model.eval()\n",
    "# noisy_pruning_model.to(DEVICE)\n",
    "\n",
    "# noisy_optimal_subset = remove_noisy_feature_maps(noisy_pruning_model, train_dataloader, DEVICE, LAYER, sorted_importance_scores, model_path='Models/real_authenticity_noise_out_pruned_model.pth')\n",
    "\n",
    "# del noisy_pruning_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with already saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the baseline model\n",
      "Test Loss: 145.4769\n",
      "------------------\n",
      "Testing the negative impact pruned model\n",
      "Test Loss: 206.8012\n",
      "------------------\n",
      "Testing the noisy optimal subset pruned model\n",
      "Test Loss: 82.0128\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = 'Models/VGG-16_real_authenticity_finetuned.pth'\n",
    "NEGATIVE_IMPACT_PRUNED_MODEL_PATH = 'Models/real_authenticity_negative_impact_pruned_model.pth'\n",
    "NOISY_OPTIMAL_SUBSET_MODEL_PATH = 'Models/real_authenticity_noise_out_pruned_model.pth'\n",
    "\n",
    "base_model = AuthenticityPredictor()\n",
    "base_model.load_state_dict(torch.load(BASE_MODEL, weights_only=True))\n",
    "\n",
    "negative_impact_pruned_model = AuthenticityPredictor()\n",
    "negative_impact_pruned_model.load_state_dict(torch.load(NEGATIVE_IMPACT_PRUNED_MODEL_PATH,weights_only=True))\n",
    "\n",
    "noisy_optimal_subset = AuthenticityPredictor()\n",
    "noisy_optimal_subset.load_state_dict(torch.load(NOISY_OPTIMAL_SUBSET_MODEL_PATH,weights_only=True))\n",
    "\n",
    "\n",
    "\n",
    "# Test the baseline model\n",
    "print(\"Testing the baseline model\")\n",
    "test_model(base_model, test_dataloader, criterion, device)\n",
    "print(\"------------------\")\n",
    "\n",
    "# test the negative impact pruned model\n",
    "print(\"Testing the negative impact pruned model\")\n",
    "test_model(negative_impact_pruned_model, test_dataloader, criterion, device)\n",
    "print(\"------------------\")\n",
    "\n",
    "# test the noisy optimal subset pruned model\n",
    "print(\"Testing the noisy optimal subset pruned model\")\n",
    "test_model(noisy_optimal_subset, test_dataloader, criterion, device)\n",
    "print(\"------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Aanalysis - Comparing the models zeroed out weights & Correlations between the models predicitons and ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-out weights analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that extract the indices of the zeroed out feature maps in a convolutional layer\n",
    "\n",
    "def get_zeroed_feature_maps(model, layer_name):\n",
    "    \"\"\"\n",
    "    Get the indices of the zeroed out feature maps in a convolutional layer.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        layer_name (str): The name of the convolutional layer.\n",
    "\n",
    "    Returns:\n",
    "        list: The indices of the zeroed out feature maps.\n",
    "    \"\"\"\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    zeroed_feature_maps = []\n",
    "\n",
    "    for i, weight in enumerate(layer.weight):\n",
    "        if torch.all(weight == 0):\n",
    "            zeroed_feature_maps.append(i)\n",
    "    zeroed_feature_maps.sort()\n",
    "\n",
    "    num_zeroed = len(zeroed_feature_maps)\n",
    "\n",
    "    return zeroed_feature_maps, num_zeroed\n",
    "\n",
    "# Get the zeroed out feature maps in the 'features.28' layer of the noisy pruned model\n",
    "_, noisy_num_zeroed = get_zeroed_feature_maps(noisy_pruned_model, 'features.28')\n",
    "\n",
    "# Get the zeroed out feature maps in the 'features.28' layer of the best subset pruned model\n",
    "_, best_subset_num_zeroed = get_zeroed_feature_maps(best_subset_pruned_model, 'features.28')\n",
    "\n",
    "# Get the zeroed out feature maps in the 'features.28' layer of the negative impact pruned model\n",
    "_, negative_impact_num_zeroed = get_zeroed_feature_maps(negative_impact_pruned_model, 'features.28')\n",
    "\n",
    "print(f\"Noisy pruned model: {noisy_num_zeroed} zeroed out feature maps\")\n",
    "print(f\"Best subset pruned model: {best_subset_num_zeroed} zeroed out feature maps\")\n",
    "print(f\"Negative impact pruned model: {negative_impact_num_zeroed} zeroed out feature maps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test RSA on the models (baseline and pruned models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the models for RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_MODEL_NAME = 'Models/VGG-16_authenticity_finetuned.pth'\n",
    "NOISY_PRUNED_MODEL_PATH = 'Models/authenticity_noise_out_pruned_model.pth'\n",
    "NEGATIVE_IMPACT_PRUNED_MODEL_PATH = 'Models/authenticity_negative_impact_pruned_model.pth'\n",
    "\n",
    "baseline_model = AuthenticityPredictor()\n",
    "baseline_model.load_state_dict(torch.load(BASELINE_MODEL_NAME, weights_only=True))\n",
    "\n",
    "noisy_pruned_model = AuthenticityPredictor()\n",
    "noisy_pruned_model.load_state_dict(torch.load(NOISY_PRUNED_MODEL_PATH, weights_only=True))\n",
    "\n",
    "negative_impact_pruned_model = AuthenticityPredictor()\n",
    "negative_impact_pruned_model.load_state_dict(torch.load(NEGATIVE_IMPACT_PRUNED_MODEL_PATH,weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureMapHook:\n",
    "    \"\"\"Hook to extract feature maps from neural network layers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_maps = []\n",
    "    \n",
    "    def __call__(self, module, input, output):\n",
    "        # Detach from computation graph and move to CPU\n",
    "        self.feature_maps.append(output.detach().cpu())\n",
    "\n",
    "def get_feature_maps(model, dataloader, layer_name, device):\n",
    "    \"\"\"\n",
    "    Extracts the feature maps of a specific layer from a model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        dataloader (DataLoader): DataLoader for evaluation.\n",
    "        layer_name (str): The name of the layer to extract feature maps from.\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The feature maps as a numpy array with shape (240, num_features).\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Register a hook to extract feature maps\n",
    "    hook = FeatureMapHook()\n",
    "    target_layer = dict(model.named_modules())[layer_name]\n",
    "    hook_handle = target_layer.register_forward_hook(hook)\n",
    "    \n",
    "    # Forward pass to extract feature maps from the dataloader\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            model(inputs)\n",
    "\n",
    "    # Remove the hook\n",
    "    hook_handle.remove()\n",
    "    \n",
    "    # Process the feature maps to get the desired shape\n",
    "    all_features = []\n",
    "    \n",
    "    for batch_features in hook.feature_maps:\n",
    "        # Handle different possible output formats (accommodate different layer types)\n",
    "        if len(batch_features.shape) == 4:  # Conv layers: [batch_size, channels, height, width]\n",
    "            batch_size, channels, height, width = batch_features.shape\n",
    "            # Flatten spatial dimensions and create one feature vector per sample\n",
    "            batch_features = batch_features.reshape(batch_size, channels * height * width)\n",
    "        elif len(batch_features.shape) == 2:  # Linear layers: [batch_size, features]\n",
    "            pass  # Already in the right format\n",
    "        \n",
    "        # Add batch features to our collection\n",
    "        all_features.append(batch_features)\n",
    "    \n",
    "    # Concatenate all batches and convert to numpy\n",
    "    features_tensor = torch.cat(all_features, dim=0)\n",
    "    \n",
    "    # Ensure we have exactly the number of samples we expect in the dataloader \n",
    "    assert features_tensor.shape[0] == len(dataloader.dataset) \n",
    "    \n",
    "    # Convert to numpy array\n",
    "    features_array = features_tensor.numpy()\n",
    "    \n",
    "    return features_array\n",
    "def compute_similarity_matrix(features):\n",
    "    \"\"\"\n",
    "    Compute a similarity matrix from feature embeddings.\n",
    "    Works with both convolutional features (4D) and FC features (2D).\n",
    "    \n",
    "    Args:\n",
    "        features: numpy array - either shape (n_samples, n_channels, height, width)\n",
    "                 or shape (n_samples, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        similarity_matrix: numpy array of shape (n_samples, n_samples)\n",
    "    \"\"\"\n",
    "    # Check the dimensionality of features\n",
    "    n_samples = features.shape[0]\n",
    "    \n",
    "    # If features are from convolutional layer (4D), reshape to 2D\n",
    "    if len(features.shape) == 4:\n",
    "        features_flat = features.reshape(n_samples, -1)\n",
    "    else:\n",
    "        # Features are already 2D (from FC layer)\n",
    "        features_flat = features\n",
    "    \n",
    "    # Compute cosine similarity between all pairs\n",
    "    similarity_matrix = cosine_similarity(features_flat)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def compute_quality_difference_matrix(quality_scores):\n",
    "    \"\"\"\n",
    "    Compute a matrix of quality differences between all pairs of samples.\n",
    "    \n",
    "    Args:\n",
    "        quality_scores: numpy array of shape (n_samples,) containing quality scores\n",
    "        \n",
    "    Returns:\n",
    "        difference_matrix: numpy array of shape (n_samples, n_samples)\n",
    "    \"\"\"\n",
    "    n_samples = quality_scores.shape[0]\n",
    "    difference_matrix = np.zeros((n_samples, n_samples))\n",
    "    \n",
    "    # Compute absolute differences between all pairs\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            difference_matrix[i, j] = abs(quality_scores[i] - quality_scores[j])\n",
    "            \n",
    "    return difference_matrix\n",
    "\n",
    "def get_upper_triangle(matrix):\n",
    "    \"\"\"\n",
    "    Extract the upper triangle of a matrix (excluding diagonal).\n",
    "    \n",
    "    Args:\n",
    "        matrix: numpy array of shape (n, n)\n",
    "        \n",
    "    Returns:\n",
    "        upper_triangle: flattened upper triangle values\n",
    "    \"\"\"\n",
    "    indices = np.triu_indices_from(matrix, k=1)\n",
    "    return matrix[indices]\n",
    "\n",
    "def calculate_fit(similarity_matrix, quality_diff_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the fit between similarity and quality difference matrices.\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: numpy array of shape (n_samples, n_samples)\n",
    "        quality_diff_matrix: numpy array of shape (n_samples, n_samples)\n",
    "        \n",
    "    Returns:\n",
    "        correlation: Spearman correlation coefficient between the matrices\n",
    "        p_value: p-value of the correlation\n",
    "    \"\"\"\n",
    "    # Extract upper triangles (excluding diagonal)\n",
    "    sim_upper = get_upper_triangle(similarity_matrix)\n",
    "    qual_upper = get_upper_triangle(quality_diff_matrix)\n",
    "    \n",
    "    # Compute correlation (negative since higher similarity should correspond to lower difference)\n",
    "    correlation, p_value = spearmanr(sim_upper, qual_upper)\n",
    "    \n",
    "    # We're expecting a negative correlation (higher similarity → lower quality difference)\n",
    "    # so we return the negative correlation value for easier interpretation\n",
    "    return -correlation, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature maps\n",
    "baseline_features = get_feature_maps(baseline_model, test_dataloader, 'fc1.3', device)\n",
    "\n",
    "noisy_pruned_features = get_feature_maps(noisy_pruned_model, test_dataloader, 'fc1.3', device)\n",
    "\n",
    "negative_impact_features = get_feature_maps(negative_impact_pruned_model, test_dataloader, 'fc1.3', device)\n",
    "\n",
    "# Extract quality scores\n",
    "auth_scores_list = []\n",
    "with torch.no_grad():\n",
    "\tfor _, labels in test_dataloader:\n",
    "\t\tauth_scores_list.append(labels[:, 0])  # First column contains auth scores\n",
    "q_scores = torch.cat(auth_scores_list).numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrices\n",
    "baseline_similarity = compute_similarity_matrix(baseline_features)\n",
    "noisy_similarity = compute_similarity_matrix(noisy_pruned_features)\n",
    "negative_impact_similarity = compute_similarity_matrix(negative_impact_features)\n",
    "\n",
    "# Compute quality difference matrices\n",
    "quality_diff_matrix = compute_quality_difference_matrix(q_scores)\n",
    "\n",
    "# Calculate fit between similarity and quality difference matrices\n",
    "baseline_fit = calculate_fit(baseline_similarity, quality_diff_matrix)\n",
    "noisy_fit = calculate_fit(noisy_similarity, quality_diff_matrix)\n",
    "negative_impact_fit = calculate_fit(negative_impact_similarity, quality_diff_matrix)\n",
    "\n",
    "print(\"Baseline Model Fit:\")\n",
    "print(f\"Correlation: {baseline_fit[0]:.4f}\")\n",
    "print(\"------------------\")\n",
    "print(\"RSME Noise-out Pruned Model Fit:\")\n",
    "print(f\"Correlation: {noisy_fit[0]:.4f}\")\n",
    "print(\"------------------\")\n",
    "print(\"RSME Negative Impact Pruned Model Fit:\")\n",
    "print(f\"Correlation: {negative_impact_fit[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
