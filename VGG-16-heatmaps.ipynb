{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database creations using pytorch Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageQualityDataset(Dataset):\n",
    "    \"\"\"Dataset for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its labels by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (image, labels) where:\n",
    "                image (PIL.Image): The image.\n",
    "                labels (torch.Tensor): Tensor containing quality and authenticity scores.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(os.getcwd(), self.data.iloc[idx, 3])  # image_path column\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        quality = self.data.iloc[idx, 0]  # Quality column\n",
    "        authenticity = self.data.iloc[idx, 1]  # Authenticity column\n",
    "        labels = torch.tensor([quality, authenticity], dtype=torch.float)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in vgg.features.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Extract features up to fc2\n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        self.fc1 = vgg.classifier[:-1]  # Up to fc2 (4096 -> 128)\n",
    "        \n",
    "        # New regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2)  # Predict quality and realness\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        features = self.fc1(x)\n",
    "        predictions = self.regression_head(features)\n",
    "        return predictions, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations for the ImageNet dataset\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "annotations_file = 'Dataset/AIGCIQA2023/mos_data.csv'\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageQualityDataset(csv_file=annotations_file, transform=data_transforms)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Create a dictionary containing the data loaders\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader,\n",
    "    'test': test_dataloader\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASELINE_MODEL_PATH = 'Models/VGG-16_finetuned_regression.pth'\n",
    "NOISY_PRUNED_MODEL_PATH = 'Models/noise_out_pruned_model.pth'\n",
    "NEGATIVE_IMPACT_PRUNED_MODEL_PATH = 'Models/negative_impact_pruned_model.pth'\n",
    "\n",
    "noisy_pruned_model = QualityPredictor()\n",
    "noisy_pruned_model.load_state_dict(torch.load(NOISY_PRUNED_MODEL_PATH, weights_only=True))\n",
    "\n",
    "negative_impact_pruned_model = QualityPredictor()\n",
    "negative_impact_pruned_model.load_state_dict(torch.load(NEGATIVE_IMPACT_PRUNED_MODEL_PATH,weights_only=True))\n",
    "\n",
    "baseline_model = QualityPredictor()\n",
    "baseline_model.load_state_dict(torch.load(BASELINE_MODEL_PATH,weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "- Getting the activations of each channel in the last convolutional layer for your images \n",
    "- Weighting each channel's activation map by its computed importance score\n",
    "- Aggregating these weighted activations to form a final heatmap\n",
    "- Visualizing this heatmap overlaid on the original image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Sort the importance scores by channel ID and normalize them to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set numpy print options\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "importance_scores = np.load('Ranking_arrays/importance_scores.npy')\n",
    "importance_scores_sorted_by_index = importance_scores[importance_scores[:,0].argsort()][:,1]\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Normalize the importance scores using the softmax function to get the normalized importance scores (importance scores that sum to 1)\n",
    "normalized_importance_scores_sorted_by_index = softmax(importance_scores_sorted_by_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Extracting the activations of the last convolutional layer for the images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 28 feature maps shape (Baseline):  (1, 512, 14, 14)\n",
      "Layer 28 feature maps shape (Noisy Pruned):  (1, 512, 14, 14)\n",
      "Layer 28 feature maps shape (Negative Impact Pruned):  (1, 512, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class FeatureMapHook:\n",
    "    \"\"\"Hook to extract feature maps from neural network layers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_maps = []\n",
    "    \n",
    "    def __call__(self, module, input, output):\n",
    "        # Detach from computation graph and move to CPU\n",
    "        self.feature_maps.append(output.detach().cpu())\n",
    "\n",
    "def get_feature_maps(model, dataloader, layer_name, device):\n",
    "    \"\"\"\n",
    "    Extracts the feature maps of a specific layer from a model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        dataloader (DataLoader): DataLoader for evaluation.\n",
    "        layer_name (str): The name of the layer to extract feature maps from.\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The feature maps as a numpy array with shape (240, num_features).\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Register a hook to extract feature maps\n",
    "    hook = FeatureMapHook()\n",
    "    target_layer = dict(model.named_modules())[layer_name]\n",
    "    hook_handle = target_layer.register_forward_hook(hook)\n",
    "    \n",
    "    # Forward pass to extract feature maps from the dataloader\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            model(inputs)\n",
    "\n",
    "    # Remove the hook\n",
    "    hook_handle.remove()\n",
    "    \n",
    "    # Process the feature maps to get the desired shape\n",
    "    all_features = []\n",
    "    \n",
    "    for batch_features in hook.feature_maps:\n",
    "        # Add batch features to our collection\n",
    "        all_features.append(batch_features)\n",
    "    \n",
    "    # Concatenate all batches and convert to numpy\n",
    "    features_tensor = torch.cat(all_features, dim=0)\n",
    "    \n",
    "    # Ensure we have exactly the number of samples we expect in the dataloader \n",
    "    assert features_tensor.shape[0] == len(dataloader.dataset) \n",
    "    \n",
    "    # Convert to numpy array\n",
    "    features_array = features_tensor.numpy()\n",
    "    \n",
    "    return features_array\n",
    "\n",
    "test_functions_dataloader = DataLoader(torch.utils.data.Subset(val_dataset, range(1)), \n",
    "                                     batch_size=1, \n",
    "                                     shuffle=True, \n",
    "                                     num_workers=0)\n",
    "\n",
    "layer_28_feature_maps_base = get_feature_maps(baseline_model, test_functions_dataloader, 'features.28', device)\n",
    "layer_28_feature_maps_noisy = get_feature_maps(noisy_pruned_model, test_functions_dataloader, 'features.28', device)\n",
    "layer_28_feature_maps_neg_impact = get_feature_maps(negative_impact_pruned_model, test_functions_dataloader, 'features.28', device)\n",
    "\n",
    "\n",
    "# layer_28_feature_maps_base = get_feature_maps(baseline_model, dataloaders['val'], 'features.28', device)\n",
    "# layer_28_feature_maps_noisy = get_feature_maps(noisy_pruned_model, dataloaders['val'], 'features.28', device)\n",
    "# layer_28_feature_maps_neg_impact = get_feature_maps(negative_impact_pruned_model, dataloaders['val'], 'features.28', device)\n",
    "\n",
    "# check shapes\n",
    "print(\"Layer 28 feature maps shape (Baseline): \", layer_28_feature_maps_base.shape)\n",
    "print(\"Layer 28 feature maps shape (Noisy Pruned): \", layer_28_feature_maps_noisy.shape)\n",
    "print(\"Layer 28 feature maps shape (Negative Impact Pruned): \", layer_28_feature_maps_neg_impact.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pathlib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def compute_heatmap(feature_maps, scores):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of a set of feature maps using a set of corresponding scores.\n",
    "\n",
    "    Args:\n",
    "    - feature_maps (numpy.ndarray): A 4D array of feature maps (shape 144 x 512 x 14 x 14).\n",
    "    - scores (numpy.ndarray): A 2D array of scores for each feature map (shape 144 x 512).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 3D array representing the computed heatmap (shape 144 x 14 x 14).\n",
    "    \"\"\"\n",
    "    return np.average(feature_maps, weights=scores, axis=0)\n",
    "\n",
    "def compute_heatmap_normalized(feature_maps, scores):\n",
    "    # Calculate the minimum and maximum values along the (1, 2) axes\n",
    "    min_vals = np.min(feature_maps, axis=(1, 2), keepdims=True)\n",
    "    max_vals = np.max(feature_maps, axis=(1, 2), keepdims=True)\n",
    "\n",
    "    # Handle division by zero by replacing zero denominators with a small epsilon value\n",
    "    eps = 1e-10\n",
    "    denom = max_vals - min_vals\n",
    "    denom[denom == 0] = eps\n",
    "\n",
    "    # Normalize the feature maps within each 14x14 map\n",
    "    normalized_feature_maps = (feature_maps - min_vals) / denom\n",
    "\n",
    "    # Handle NaN values by replacing them with 0\n",
    "    normalized_feature_maps = np.nan_to_num(normalized_feature_maps)\n",
    "\n",
    "    # Compute the weighted average of the normalized feature maps using the scores\n",
    "    weighted_average = np.average(normalized_feature_maps, weights=scores, axis=0)\n",
    "\n",
    "    return weighted_average\n",
    "\n",
    "def overlay_mask(img: Image.Image, mask: np.array, colormap: str = \"jet_r\", alpha: float = 0.7) -> Image.Image:\n",
    "    \"\"\"Overlay a colormapped mask on a background image\n",
    "    Args:\n",
    "        img: background image\n",
    "        mask: mask to be overlayed in grayscale\n",
    "        colormap: colormap to be applied on the mask\n",
    "        alpha: transparency of the background image\n",
    "    Returns:\n",
    "        overlayed image\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(alpha, float) or alpha < 0 or alpha >= 1:\n",
    "        raise ValueError(\"alpha argument is expected to be of type float between 0 and 1\")\n",
    "\n",
    "    # Get the specified colormap\n",
    "    cmap = cm.get_cmap(colormap)\n",
    "    \n",
    "    # Resize mask to match the image dimensions\n",
    "    overlay = cv2.resize(mask, img.size)\n",
    "    \n",
    "    # Normalize the mask values to range [0,1]\n",
    "    overlay = overlay / overlay.max()\n",
    "    \n",
    "    # Apply colormap to the mask and convert to RGB\n",
    "    # The ** 2 applies a quadratic transformation to enhance contrast\n",
    "    overlay = (255 * cmap(np.asarray(overlay) ** 2)[:, :, :3]).astype(np.uint8)\n",
    "    \n",
    "    # Blend the original image with the colored mask\n",
    "    # alpha controls how much of the original image shows through\n",
    "    overlayed_img = Image.fromarray((alpha * np.asarray(img) + (1 - alpha) * np.asarray(overlay)).astype(np.uint8))\n",
    "\n",
    "    return overlayed_img\n",
    "\n",
    "def create_heatmaps_from_dataloader(dataloader, batch_feature_maps,channel_importance_scores, output_dir=None):\n",
    "    \"\"\"\n",
    "    Create and save heatmaps for images in a dataloader using channel importance scores.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to extract feature maps\n",
    "        dataloader: PyTorch DataLoader containing images\n",
    "        channel_importance_scores: Array of importance scores for each channel (512,)\n",
    "        output_dir: Directory to save the output heatmaps\n",
    "        device: Device to run model on ('cuda' or 'cpu')\n",
    "        feature_layer_name: Name of the layer to extract features from\n",
    "        batch_limit: Maximum number of batches to process (None = all)\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = pathlib.Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "\n",
    "    # Define inverse normalization to recover original images\n",
    "    inv_normalize = transforms.Compose([\n",
    "        transforms.Normalize(\n",
    "            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "            std=[1/0.229, 1/0.224, 1/0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Process each batch\n",
    "    heatmap_list = []\n",
    "    img_idx = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        # Process each image in the batch\n",
    "        for i in range(inputs.size(0)):\n",
    "            # Get feature maps for this image\n",
    "            feature_maps = batch_feature_maps[i]  # Shape: (512, H, W)\n",
    "            \n",
    "            # Compute heatmap\n",
    "            heatmap = compute_heatmap_normalized(feature_maps, channel_importance_scores)\n",
    "            heatmap_list.append(heatmap)\n",
    "            \n",
    "            # Convert tensor back to image\n",
    "            img_tensor = inputs[i].cpu()\n",
    "            # Denormalize the image\n",
    "            img_tensor = inv_normalize(img_tensor)\n",
    "            img_tensor = torch.clamp(img_tensor, 0, 1)\n",
    "            img_np = img_tensor.permute(1, 2, 0).numpy() * 255\n",
    "            original_img = Image.fromarray(img_np.astype(np.uint8))\n",
    "            \n",
    "            # Overlay heatmap on original image\n",
    "            overlaid_img = overlay_mask(original_img, heatmap)\n",
    "            \n",
    "            # Save the overlaid image\n",
    "            output_path = output_dir / f\"heatmap_idx{img_idx}.png\"\n",
    "            overlaid_img.save(output_path)\n",
    "            \n",
    "            print(f\"Processed image {img_idx}\")\n",
    "            img_idx += 1\n",
    "        \n",
    "        print(f\"Completed batch {batch_idx+1}/{len(dataloader)}\")\n",
    "\n",
    "    return heatmap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image 0\n",
      "Completed batch 1/1\n",
      "Processed image 0\n",
      "Completed batch 1/1\n",
      "Processed image 0\n",
      "Completed batch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_399399/1726644971.py:60: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = cm.get_cmap(colormap)\n"
     ]
    }
   ],
   "source": [
    "# Get just the first 5 elements of the validation dataset to test the functions above\n",
    "\n",
    "OUTPUT_DIR_BASE = 'Heatmaps_images/base_model'\n",
    "OUTPUT_DIR_NOISY = 'Heatmaps_images/noisy_pruned_model'\n",
    "OUTPUT_DIR_NEG_IMPACT = 'Heatmaps_images/negative_impact_pruned_model'\n",
    "\n",
    "batch_features_base = layer_28_feature_maps_base\n",
    "batch_features_noisy = layer_28_feature_maps_noisy\n",
    "channel_importance_scores = normalized_importance_scores_sorted_by_index\n",
    "\n",
    "heatmap_list_base = create_heatmaps_from_dataloader(test_functions_dataloader, batch_features_base, channel_importance_scores, OUTPUT_DIR_BASE)\n",
    "\n",
    "heatmap_list_noisy = create_heatmaps_from_dataloader(test_functions_dataloader, batch_features_noisy, channel_importance_scores, OUTPUT_DIR_NOISY)\n",
    "\n",
    "heatmap_list_neg_impact = create_heatmaps_from_dataloader(test_functions_dataloader, layer_28_feature_maps_neg_impact, channel_importance_scores, OUTPUT_DIR_NEG_IMPACT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
