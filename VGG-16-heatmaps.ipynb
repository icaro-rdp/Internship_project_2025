{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database creations using pytorch Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageQualityDataset(Dataset):\n",
    "    \"\"\"Dataset for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its labels by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (image, labels) where:\n",
    "                image (PIL.Image): The image.\n",
    "                labels (torch.Tensor): Tensor containing quality and authenticity scores.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(os.getcwd(), self.data.iloc[idx, 3])  # image_path column\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        quality = self.data.iloc[idx, 0]  # Quality column\n",
    "        authenticity = self.data.iloc[idx, 1]  # Authenticity column\n",
    "        labels = torch.tensor([quality, authenticity], dtype=torch.float)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in vgg.features.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Extract features up to fc2\n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        self.fc1 = vgg.classifier[:-1]  # Up to fc2 (4096 -> 128)\n",
    "        \n",
    "        # New regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2)  # Predict quality and realness\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        features = self.fc1(x)\n",
    "        predictions = self.regression_head(features)\n",
    "        return predictions, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations for the ImageNet dataset\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "annotations_file = 'Dataset/AIGCIQA2023/mos_data.csv'\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageQualityDataset(csv_file=annotations_file, transform=data_transforms)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Create a dictionary containing the data loaders\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader,\n",
    "    'test': test_dataloader\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOISY_PRUNED_MODEL_PATH = 'Models/noise_out_pruned_model.pth'\n",
    "NEGATIVE_IMPACT_PRUNED_MODEL_PATH = 'Models/negative_impact_pruned_model.pth'\n",
    "\n",
    "noisy_pruned_model = QualityPredictor()\n",
    "noisy_pruned_model.load_state_dict(torch.load(NOISY_PRUNED_MODEL_PATH, weights_only=True))\n",
    "\n",
    "negative_impact_pruned_model = QualityPredictor()\n",
    "negative_impact_pruned_model.load_state_dict(torch.load(NEGATIVE_IMPACT_PRUNED_MODEL_PATH,weights_only=True))\n",
    "\n",
    "baseline_model = QualityPredictor()\n",
    "baseline_model.load_state_dict(torch.load('Models/VGG-16_finetuned_regression.pth',weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Predictions:  tensor([[42.4086, 40.0079]], grad_fn=<AddmmBackward0>)\n",
      "Noisy Pruned Model Predictions:  tensor([[54.3555, 51.5364]], grad_fn=<AddmmBackward0>)\n",
      "Negative Impact Pruned Model Predictions:  tensor([[63.2352, 60.6522]], grad_fn=<AddmmBackward0>)\n",
      "Baseline Model Features shape  torch.Size([1, 4096])\n",
      "Noisy Pruned Model Features shape  torch.Size([1, 4096])\n",
      "Negative Impact Pruned Model Features shape  torch.Size([1, 4096])\n"
     ]
    }
   ],
   "source": [
    "# get an item from the dataset\n",
    "image, labels = dataset[0]\n",
    "image = image.unsqueeze(0)\n",
    "labels = labels.unsqueeze(0)\n",
    "\n",
    "# predicting labels using the models\n",
    "noisy_pruned_model.eval()\n",
    "negative_impact_pruned_model.eval()\n",
    "baseline_model.eval()\n",
    "\n",
    "baseline_predictions, baseline_features = baseline_model(image)\n",
    "noisy_pruned_predictions, noisy_features = noisy_pruned_model(image)\n",
    "negative_impact_pruned_predictions, neg_impact_features = negative_impact_pruned_model(image)\n",
    "\n",
    "print(\"Baseline Model Predictions: \", baseline_predictions)\n",
    "print(\"Noisy Pruned Model Predictions: \", noisy_pruned_predictions)\n",
    "print(\"Negative Impact Pruned Model Predictions: \", negative_impact_pruned_predictions)\n",
    "\n",
    "print(\"Baseline Model Features shape \", baseline_features.shape)\n",
    "print(\"Noisy Pruned Model Features shape \", noisy_features.shape)\n",
    "print(\"Negative Impact Pruned Model Features shape \", neg_impact_features.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "- Getting the activations of each channel in the last convolutional layer for your images \n",
    "- Weighting each channel's activation map by its computed importance score\n",
    "- Aggregating these weighted activations to form a final heatmap\n",
    "- Visualizing this heatmap overlaid on the original image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Sort the importance scores by channel ID and normalize them to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set numpy print options\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "importance_scores = np.load('Ranking_arrays/importance_scores.npy')\n",
    "importance_scores_sorted_by_index = importance_scores[importance_scores[:,0].argsort()][:,1]\n",
    "\n",
    "# Normalize the importance scores to sum to 1\n",
    "normalized_importance_scores_sorted_by_index = importance_scores_sorted_by_index / np.sum(importance_scores_sorted_by_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Extracting the activations of the last convolutional layer for the images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 28 feature maps shape (Baseline):  (480, 512, 14, 14)\n",
      "Layer 28 feature maps shape (Noisy Pruned):  (480, 512, 14, 14)\n",
      "Layer 28 feature maps shape (Negative Impact Pruned):  (480, 512, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class FeatureMapHook:\n",
    "    \"\"\"Hook to extract feature maps from neural network layers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_maps = []\n",
    "    \n",
    "    def __call__(self, module, input, output):\n",
    "        # Detach from computation graph and move to CPU\n",
    "        self.feature_maps.append(output.detach().cpu())\n",
    "\n",
    "def get_feature_maps(model, dataloader, layer_name, device):\n",
    "    \"\"\"\n",
    "    Extracts the feature maps of a specific layer from a model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        dataloader (DataLoader): DataLoader for evaluation.\n",
    "        layer_name (str): The name of the layer to extract feature maps from.\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The feature maps as a numpy array with shape (240, num_features).\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Register a hook to extract feature maps\n",
    "    hook = FeatureMapHook()\n",
    "    target_layer = dict(model.named_modules())[layer_name]\n",
    "    hook_handle = target_layer.register_forward_hook(hook)\n",
    "    \n",
    "    # Forward pass to extract feature maps from the dataloader\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            model(inputs)\n",
    "\n",
    "    # Remove the hook\n",
    "    hook_handle.remove()\n",
    "    \n",
    "    # Process the feature maps to get the desired shape\n",
    "    all_features = []\n",
    "    \n",
    "    for batch_features in hook.feature_maps:\n",
    "        # Add batch features to our collection\n",
    "        all_features.append(batch_features)\n",
    "    \n",
    "    # Concatenate all batches and convert to numpy\n",
    "    features_tensor = torch.cat(all_features, dim=0)\n",
    "    \n",
    "    # Ensure we have exactly the number of samples we expect in the dataloader \n",
    "    assert features_tensor.shape[0] == len(dataloader.dataset) \n",
    "    \n",
    "    # Convert to numpy array\n",
    "    features_array = features_tensor.numpy()\n",
    "    \n",
    "    return features_array\n",
    "\n",
    "\n",
    "layer_28_feature_maps_base = get_feature_maps(baseline_model, dataloaders['val'], 'features.28', device)\n",
    "layer_28_feature_maps_noisy = get_feature_maps(noisy_pruned_model, dataloaders['val'], 'features.28', device)\n",
    "layer_28_feature_maps_neg_impact = get_feature_maps(negative_impact_pruned_model, dataloaders['val'], 'features.28', device)\n",
    "\n",
    "# check shapes\n",
    "print(\"Layer 28 feature maps shape (Baseline): \", layer_28_feature_maps_base.shape)\n",
    "print(\"Layer 28 feature maps shape (Noisy Pruned): \", layer_28_feature_maps_noisy.shape)\n",
    "print(\"Layer 28 feature maps shape (Negative Impact Pruned): \", layer_28_feature_maps_neg_impact.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pathlib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def compute_heatmap(feature_maps, importance_scores):\n",
    "    \"\"\"\n",
    "    Compute a weighted heatmap using feature maps and importance scores.\n",
    "    \n",
    "    Args:\n",
    "        feature_maps: Feature maps from the last conv layer, shape (512, H, W)\n",
    "        importance_scores: Importance score for each channel, shape (512,)\n",
    "        \n",
    "    Returns:\n",
    "        A weighted heatmap\n",
    "    \"\"\"\n",
    "    # Ensure importance_scores has the right shape for broadcasting (512, 1, 1)\n",
    "    importance_scores = importance_scores.reshape(-1, 1, 1)\n",
    "    \n",
    "    # Weight each feature map by its importance score\n",
    "    weighted_maps = feature_maps * importance_scores\n",
    "    \n",
    "    # Sum along the channel dimension to get the final heatmap\n",
    "    heatmap = np.sum(weighted_maps, axis=0)\n",
    "    \n",
    "    return heatmap\n",
    "\n",
    "\n",
    "def overlay_mask(img, mask, alpha=0.6, colormap='jet'):\n",
    "    \"\"\"\n",
    "    Overlay a heatmap mask on an image.\n",
    "    \n",
    "    Args:\n",
    "        img: Original image (PIL Image)\n",
    "        mask: Heatmap mask (numpy array)\n",
    "        alpha: Transparency of the overlay (lower = more transparent)\n",
    "        colormap: Colormap name\n",
    "        \n",
    "    Returns:\n",
    "        Overlaid image as PIL Image\n",
    "    \"\"\"\n",
    "    # Convert PIL image to numpy array\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Normalize the mask\n",
    "    if np.max(mask) > 0:\n",
    "        mask = mask / np.max(mask)\n",
    "    \n",
    "    # Resize mask to match image dimensions\n",
    "    mask_resized = cv2.resize(mask, (img.width, img.height))\n",
    "    \n",
    "    # Apply colormap\n",
    "    # Use plt.cm instead of cm.get_cmap to avoid deprecation warning\n",
    "    cmap = plt.cm.get_cmap(colormap)\n",
    "    heatmap_colored = cmap(mask_resized)\n",
    "    # Convert to RGB (remove alpha channel) and scale to 0-255\n",
    "    heatmap_colored = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    \n",
    "    # Blend original image with heatmap\n",
    "    blended = cv2.addWeighted(img_array, alpha, heatmap_colored, 1-alpha, 0)\n",
    "    \n",
    "    # Convert back to PIL Image\n",
    "    return Image.fromarray(blended)\n",
    "\n",
    "\n",
    "def create_heatmaps_from_dataloader(model, dataloader, channel_importance_scores, output_dir, device='cuda',\n",
    "                                   feature_layer_name='features.28', batch_limit=None):\n",
    "    \"\"\"\n",
    "    Create and save heatmaps for images in a dataloader using channel importance scores.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to extract feature maps\n",
    "        dataloader: PyTorch DataLoader containing images\n",
    "        channel_importance_scores: Array of importance scores for each channel (512,)\n",
    "        output_dir: Directory to save the output heatmaps\n",
    "        device: Device to run model on ('cuda' or 'cpu')\n",
    "        feature_layer_name: Name of the layer to extract features from\n",
    "        batch_limit: Maximum number of batches to process (None = all)\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = pathlib.Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define hook to capture feature maps\n",
    "    activation = {}\n",
    "    def hook_fn(module, input, output):\n",
    "        activation['features'] = output.detach().cpu().numpy()\n",
    "    \n",
    "    # Register hook to get feature maps from the specified layer\n",
    "    for name, module in model.named_modules():\n",
    "        if name == feature_layer_name:\n",
    "            hook_handle = module.register_forward_hook(hook_fn)\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"Layer {feature_layer_name} not found in the model\")\n",
    "    \n",
    "    # Define inverse normalization to recover original images\n",
    "    inv_normalize = transforms.Compose([\n",
    "        transforms.Normalize(\n",
    "            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "            std=[1/0.229, 1/0.224, 1/0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Process each batch\n",
    "    heatmap_collection = []\n",
    "    img_idx = 0\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "                if batch_limit is not None and batch_idx >= batch_limit:\n",
    "                    break\n",
    "                    \n",
    "                # Move inputs to device\n",
    "                inputs = inputs.to(device)\n",
    "                \n",
    "                # Forward pass through the model\n",
    "                _ = model(inputs)\n",
    "                \n",
    "                # Get feature maps from hook\n",
    "                batch_feature_maps = activation['features']\n",
    "                \n",
    "                # Process each image in the batch\n",
    "                for i in range(inputs.size(0)):\n",
    "                    # Get feature maps for this image\n",
    "                    feature_maps = batch_feature_maps[i]  # Shape: (512, H, W)\n",
    "                    \n",
    "                    # Compute heatmap\n",
    "                    heatmap = compute_heatmap(feature_maps, channel_importance_scores)\n",
    "                    heatmap_collection.append(heatmap)\n",
    "                    \n",
    "                    # Convert tensor back to image\n",
    "                    img_tensor = inputs[i].cpu()\n",
    "                    # Denormalize the image\n",
    "                    img_tensor = inv_normalize(img_tensor)\n",
    "                    img_tensor = torch.clamp(img_tensor, 0, 1)\n",
    "                    img_np = img_tensor.permute(1, 2, 0).numpy() * 255\n",
    "                    original_img = Image.fromarray(img_np.astype(np.uint8))\n",
    "                    \n",
    "                    # Overlay heatmap on original image\n",
    "                    overlaid_img = overlay_mask(original_img, heatmap)\n",
    "                    \n",
    "                    # Get class label info if available\n",
    "                    if hasattr(labels[i], 'item'):\n",
    "                        if labels[i].dim() > 0:\n",
    "                            class_info = f\"_class{labels[i][0]:.1f}_{labels[i][1]:.1f}\"\n",
    "                        else:\n",
    "                            class_info = f\"_class{labels[i].item()}\"\n",
    "                    else:\n",
    "                        class_info = \"\"\n",
    "                    \n",
    "                    # Save the overlaid image\n",
    "                    output_path = output_dir / f\"heatmap_idx{img_idx}{class_info}.png\"\n",
    "                    overlaid_img.save(output_path)\n",
    "                    \n",
    "                    print(f\"Processed image {img_idx}\")\n",
    "                    img_idx += 1\n",
    "                \n",
    "                print(f\"Completed batch {batch_idx+1}/{len(dataloader)}\")\n",
    "    \n",
    "    finally:\n",
    "        # Always remove the hook\n",
    "        hook_handle.remove()\n",
    "    \n",
    "    # Save the heatmap collection for later use\n",
    "    np.save(f\"{output_dir}/heatmap_collection.npy\", np.array(heatmap_collection))\n",
    "    \n",
    "    return heatmap_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 512, 14, 14)\n",
      "(512,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_337739/277430050.py:50: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = cm.get_cmap(colormap)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (512,512,3) (512,512,3,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m channel_importance_scores \u001b[38;5;241m=\u001b[39m normalized_importance_scores_sorted_by_index\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create heatmaps for the validation set\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mcreate_heatmaps_for_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_importance_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 86\u001b[0m, in \u001b[0;36mcreate_heatmaps_for_batch\u001b[0;34m(model, img_paths, channel_importance_scores, feature_maps, output_dir)\u001b[0m\n\u001b[1;32m     83\u001b[0m original_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Overlay heatmap on original image\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m overlaid_img \u001b[38;5;241m=\u001b[39m \u001b[43moverlay_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheatmap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Save the overlaid image\u001b[39;00m\n\u001b[1;32m     89\u001b[0m output_path \u001b[38;5;241m=\u001b[39m output_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheatmap_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathlib\u001b[38;5;241m.\u001b[39mPath(img_path)\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[58], line 55\u001b[0m, in \u001b[0;36moverlay_mask\u001b[0;34m(img, mask, alpha, colormap)\u001b[0m\n\u001b[1;32m     52\u001b[0m overlay \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m cmap(overlay)[:, :, :\u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Overlay the mask on the image\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m overlaid_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray((\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moverlay\u001b[49m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8))\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m overlaid_img\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (512,512,3) (512,512,3,4) "
     ]
    }
   ],
   "source": [
    "# Get image paths from validation dataset\n",
    "model = baseline_model\n",
    "image_paths = [dataset.data.iloc[idx, 3] for idx in val_dataset.indices]\n",
    "output_dir = pathlib.Path('Heatmaps')\n",
    "features = layer_28_feature_maps_base\n",
    "channel_importance_scores = normalized_importance_scores_sorted_by_index\n",
    "\n",
    "# Create heatmaps for the validation set\n",
    "create_heatmaps_from_dataloader(model, val_dataloader, channel_importance_scores, output_dir, device=device, feature_layer_name='features.28', batch_limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
