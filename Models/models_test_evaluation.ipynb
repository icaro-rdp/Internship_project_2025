{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c8108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import torch.hub\n",
    "from torchvision.models import efficientnet_b3,densenet161, EfficientNet_B3_Weights, DenseNet161_Weights, resnet152, ResNet152_Weights, inception_v3, vgg16, VGG16_Weights, vgg19, VGG19_Weights,Inception_V3_Weights\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82109a6b",
   "metadata": {},
   "source": [
    "# Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4330780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAuthenticityDataset(Dataset):\n",
    "    \"\"\"Dataset for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.dir_path = os.path.dirname(csv_file)  # Directory of the CSV file\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its labels by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (image, labels) where:\n",
    "                image (PIL.Image): The image.\n",
    "                labels (torch.Tensor): Tensor containing quality and authenticity scores.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # TODO: to be fixed, right now is folder dependent\n",
    "        # Assuming your CSV has the relative path in the 4th column (index 3)\n",
    "        # And you need to adjust the path to be relative to where you run the script\n",
    "        img_name = self.data.iloc[idx, 3].replace(\"./\", \"../\")\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        # Assuming authenticity is in the 2nd column (index 1) and is a float score\n",
    "        authenticity = self.data.iloc[idx, 1]  \n",
    "        # Note: Your dataset returns a float tensor of shape [1]. \n",
    "        # This is suitable for regression tasks or specific binary setups.\n",
    "        # For standard binary classification (e.g., with CrossEntropyLoss), \n",
    "        # you might need an integer tensor (0 or 1). Adjust if needed.\n",
    "        labels = torch.tensor([authenticity], dtype=torch.float) \n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aec1641",
   "metadata": {},
   "source": [
    "# Models definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83bc0366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarlowTwinsAuthenticityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained BarlowTwins ResNet50 instead of ResNet-152\n",
    "        barlow_twins_resnet = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in barlow_twins_resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self.features = nn.Sequential(*list(barlow_twins_resnet.children())[:-2])\n",
    "        self.avgpool = barlow_twins_resnet.avgpool\n",
    "        \n",
    "        \n",
    "        self.regression_head = nn.Sequential(\n",
    "                nn.Linear(2048, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(512, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(128, 1)\n",
    "            )    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        predictions = self.regression_head(x)\n",
    "        return predictions, x \n",
    "    \n",
    "class EfficientNetB3AuthenticityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        efficent_net = efficientnet_b3(weights=EfficientNet_B3_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in efficent_net.features.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Extract features up to fc2\n",
    "        self.features = efficent_net.features\n",
    "        self.avgpool = efficent_net.avgpool\n",
    "        \n",
    "        \n",
    "        # New regression head for EfficientNet\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(1536, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1)  # Predict authenticity\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through the backbone features\n",
    "        x = self.features(x)\n",
    "        # Apply pooling\n",
    "        x = self.avgpool(x)\n",
    "        # Flatten the features\n",
    "        features = torch.flatten(x, 1)\n",
    "        # Pass through regression head\n",
    "        predictions = self.regression_head(features)\n",
    "        \n",
    "        return predictions, features\n",
    "    \n",
    "class DenseNet161AuthenticityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained DenseNet-161\n",
    "        densenet = densenet161(weights=DenseNet161_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in densenet.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Store the features\n",
    "        self.features = densenet.features\n",
    "        \n",
    "        # DenseNet already includes a ReLU and pooling after features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # DenseNet-161's output feature dimension is 2208 instead of 2048\n",
    "        self.regression_head = nn.Sequential(\n",
    "                nn.Linear(2208, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(512, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        predictions = self.regression_head(x)\n",
    "        return predictions, x  # Return predictions and features\n",
    "    \n",
    "class ResNet152AuthenticityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained ResNet-152 instead of VGG16\n",
    "        resnet = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Store the backbone (excluding the final fc layer)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.avgpool = resnet.avgpool\n",
    "        \n",
    "        self.regression_head = nn.Sequential(\n",
    "                nn.Linear(2048, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),  # Reduced dropout ratio\n",
    "                nn.Linear(512, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(128, 1)\n",
    "            )    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        predictions = self.regression_head(x)\n",
    "        return predictions, x \n",
    "        \n",
    "class VGG16AuthenticityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in vgg.features.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Extract features up to fc2\n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        self.fc1 = vgg.classifier[:-1]  # Up to fc2 (4096 -> 128)\n",
    "        \n",
    "        # New regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1),  \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        features = self.fc1(x)\n",
    "        predictions = self.regression_head(features)\n",
    "        return predictions, features\n",
    "    \n",
    "class VGG19AuthenticityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        vgg = vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in vgg.features.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Extract features up to fc2\n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        self.fc1 = vgg.classifier[:-1]  # Up to fc2 (4096 -> 128)\n",
    "        \n",
    "        # New regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1),  \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        features = self.fc1(x)\n",
    "        predictions = self.regression_head(features)\n",
    "        return predictions, features\n",
    "    \n",
    "class InceptionV3AuthenticityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained InceptionV3 instead of ResNet152\n",
    "        inception = inception_v3(weights=Inception_V3_Weights.DEFAULT)\n",
    "        \n",
    "        # Disable auxiliary outputs for inference\n",
    "        inception.aux_logits = False\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in inception.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Store the backbone (excluding the final fc layer)\n",
    "        # InceptionV3 structure is different from ResNet, so we need to adapt\n",
    "        self.features = nn.Sequential(\n",
    "            inception.Conv2d_1a_3x3,\n",
    "            inception.Conv2d_2a_3x3,\n",
    "            inception.Conv2d_2b_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            inception.Conv2d_3b_1x1,\n",
    "            inception.Conv2d_4a_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            inception.Mixed_5b,\n",
    "            inception.Mixed_5c,\n",
    "            inception.Mixed_5d,\n",
    "            inception.Mixed_6a,\n",
    "            inception.Mixed_6b,\n",
    "            inception.Mixed_6c,\n",
    "            inception.Mixed_6d,\n",
    "            inception.Mixed_6e,\n",
    "            inception.Mixed_7a,\n",
    "            inception.Mixed_7b,\n",
    "            inception.Mixed_7c\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  \n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1)\n",
    "        )    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        predictions = self.regression_head(x)\n",
    "        return predictions, x  # Return predictions and features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c8aaf4",
   "metadata": {},
   "source": [
    "# Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Tests the model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        dataloader (DataLoader): The test data loader.\n",
    "        criterion (nn.Module): The loss function.\n",
    "        device (str): Device to use for testing ('cuda' or 'cpu'). Defaults to 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing average loss, PLCC, and SRCC metrics.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs, _ = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Collect predictions and labels for correlation calculation\n",
    "            all_predictions.extend(outputs.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    test_loss = running_loss / len(dataloader.dataset)\n",
    "    \n",
    "    # Calculate correlation coefficients\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((all_predictions - all_labels) ** 2))\n",
    "    plcc, _ = pearsonr(all_predictions, all_labels)\n",
    "    srcc, _ = spearmanr(all_predictions, all_labels)\n",
    "    krcc, _  = kendalltau(all_predictions, all_labels) # this is Kendall's Tau correlation coefficient\n",
    "    \n",
    "    return {\n",
    "        'rmse': rmse,\n",
    "        'plcc': plcc,\n",
    "        'srcc': srcc,\n",
    "        'krcc': krcc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c65d96",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a5607dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet splits: Train=957, Val=273, Test=138\n",
      "DenseNet splits: Train=957, Val=273, Test=138\n",
      "ImageNet Test Dataloader length: 138\n",
      "DenseNet Test Dataloader length: 138\n"
     ]
    }
   ],
   "source": [
    "IMAGENET_TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "DENSENET_TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.CenterCrop(300),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "ANNOTATION_FILE =  '../Dataset/AIGCIQA2023/real_images_annotations.csv'\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 20\n",
    "\n",
    "\n",
    "# Create the datasets\n",
    "imageNet_dataset = ImageAuthenticityDataset(csv_file=ANNOTATION_FILE, transform=IMAGENET_TRANSFORM)\n",
    "denseNet_dataset = ImageAuthenticityDataset(csv_file=ANNOTATION_FILE, transform=DENSENET_TRANSFORM)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42) # Use torch.cuda.manual_seed_all(42) if using multiple GPUs\n",
    "np.random.seed(42)\n",
    "# Ensure generator state is consistent if using random_split multiple times with same seed desired\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "# --- Corrected Splitting Logic ---\n",
    "\n",
    "# Calculate split sizes for ImageNet dataset\n",
    "imagenet_total_size = len(imageNet_dataset)\n",
    "imagenet_train_size = int(0.7 * imagenet_total_size)\n",
    "imagenet_val_size = int(0.2 * imagenet_total_size)\n",
    "imagenet_test_size = imagenet_total_size - imagenet_train_size - imagenet_val_size\n",
    "\n",
    "# Split the ImageNet dataset using distinct variable names\n",
    "imagenet_train_ds, imagenet_val_ds, imagenet_test_ds = random_split(\n",
    "    imageNet_dataset,\n",
    "    [imagenet_train_size, imagenet_val_size, imagenet_test_size],\n",
    "    generator=generator # Use the generator for consistent splits across datasets if needed\n",
    ")\n",
    "\n",
    "# Calculate split sizes for DenseNet dataset (assuming same proportions on the same base data length)\n",
    "# Note: len(imageNet_dataset) should equal len(denseNet_dataset) if ANNOTATION_FILE is the same\n",
    "densenet_total_size = len(denseNet_dataset)\n",
    "densenet_train_size = int(0.7 * densenet_total_size)\n",
    "densenet_val_size = int(0.2 * densenet_total_size)\n",
    "densenet_test_size = densenet_total_size - densenet_train_size - densenet_val_size\n",
    "\n",
    "# Split the DenseNet dataset using distinct variable names\n",
    "# It's crucial that the indices corresponding to train/val/test are the same\n",
    "# across both splits if you intend to compare models fairly on the *exact* same subsets.\n",
    "# random_split with the same generator ensures this if the lengths are the same.\n",
    "densenet_train_ds, densenet_val_ds, densenet_test_ds = random_split(\n",
    "    denseNet_dataset,\n",
    "    [densenet_train_size, densenet_val_size, densenet_test_size],\n",
    "    generator=generator # Reuse the same generator\n",
    ")\n",
    "\n",
    "# --- Corrected DataLoader Creation ---\n",
    "\n",
    "# Create DataLoaders for ImageNet-compatible models using the correct splits\n",
    "imagenet_train_dataloader = DataLoader(\n",
    "    imagenet_train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "imagenet_val_dataloader = DataLoader(\n",
    "    imagenet_val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "imagenet_test_dataloader = DataLoader(\n",
    "    imagenet_test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Create DataLoaders for DenseNet models using the correct splits\n",
    "denseNet_train_dataloader = DataLoader(\n",
    "    densenet_train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "denseNet_val_dataloader = DataLoader(\n",
    "    densenet_val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "denseNet_test_dataloader = DataLoader(\n",
    "    densenet_test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "\n",
    "# Create dictionaries containing the correctly assigned data loaders\n",
    "imageNet_dataloaders = {\n",
    "    'train': imagenet_train_dataloader,\n",
    "    'val': imagenet_val_dataloader,\n",
    "    'test': imagenet_test_dataloader\n",
    "}\n",
    "\n",
    "denseNet_dataloaders = {\n",
    "    'train': denseNet_train_dataloader,\n",
    "    'val': denseNet_val_dataloader,\n",
    "    'test': denseNet_test_dataloader\n",
    "}\n",
    "\n",
    "# Optional: Verify lengths to be sure\n",
    "print(f\"ImageNet splits: Train={len(imagenet_train_ds)}, Val={len(imagenet_val_ds)}, Test={len(imagenet_test_ds)}\")\n",
    "print(f\"DenseNet splits: Train={len(densenet_train_ds)}, Val={len(densenet_val_ds)}, Test={len(densenet_test_ds)}\")\n",
    "print(f\"ImageNet Test Dataloader length: {len(imageNet_dataloaders['test'].dataset)}\")\n",
    "print(f\"DenseNet Test Dataloader length: {len(denseNet_dataloaders['test'].dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377c871",
   "metadata": {},
   "source": [
    "# Models load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c689d784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/icaro.redepaolini@unitn.it/.cache/torch/hub/facebookresearch_barlowtwins_main\n",
      "/home/icaro.redepaolini@unitn.it/.conda/envs/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/icaro.redepaolini@unitn.it/.conda/envs/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in /home/icaro.redepaolini@unitn.it/.cache/torch/hub/facebookresearch_barlowtwins_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "barlow_twins_base = BarlowTwinsAuthenticityPredictor(freeze_backbone=True)\n",
    "# Load weights only for security and if the file only contains the state_dict\n",
    "barlow_twins_base.load_state_dict(torch.load('../Models/BarlowTwins/Weights/BarlowTwins_real_authenticity_finetuned.pth', weights_only=True))\n",
    "\n",
    "# barlow_twins_negative_impact = BarlowTwinsAuthenticityPredictor(freeze_backbone=True)\n",
    "# barlow_twins_negative_impact.load_state_dict(torch.load('../Models/BarlowTwins/Weights/real_authenticity_negative_impact_pruned_model.pth', weights_only=True))\n",
    "\n",
    "barlow_twins_noise_out = BarlowTwinsAuthenticityPredictor(freeze_backbone=True)\n",
    "barlow_twins_noise_out.load_state_dict(torch.load('../Models/BarlowTwins/Weights/real_authenticity_noise_out_pruned_model.pth', weights_only=True))\n",
    "\n",
    "# DenseNet-161 Models\n",
    "densenet161_base = DenseNet161AuthenticityPredictor(freeze_backbone=True)\n",
    "densenet161_base.load_state_dict(torch.load('../Models/DenseNet-161/Weights/DenseNet-161_real_authenticity_finetuned.pth', weights_only=True))\n",
    "\n",
    "# densenet161_negative_impact = DenseNet161AuthenticityPredictor(freeze_backbone=True)\n",
    "# densenet161_negative_impact.load_state_dict(torch.load('../Models/DenseNet-161/Weights/real_authenticity_negative_impact_pruned_model.pth', weights_only=True))\n",
    "\n",
    "densenet161_noise_out = DenseNet161AuthenticityPredictor(freeze_backbone=True)\n",
    "densenet161_noise_out.load_state_dict(torch.load('../Models/DenseNet-161/Weights/real_authenticity_noise_out_pruned_model.pth', weights_only=True))\n",
    "\n",
    "# EfficientNet-B3 Models\n",
    "efficientnet_b3_base = EfficientNetB3AuthenticityPredictor(freeze_backbone=True)\n",
    "efficientnet_b3_base.load_state_dict(torch.load('../Models/EfficientNet-B3/Weights/EfficientNetB3_real_authenticity_finetuned.pth', weights_only=True))\n",
    "\n",
    "# efficientnet_b3_negative_impact = EfficientNetB3AuthenticityPredictor(freeze_backbone=True)\n",
    "# efficientnet_b3_negative_impact.load_state_dict(torch.load('../Models/EfficientNet-B3/Weights/real_authenticity_negative_impact_pruned_model.pth', weights_only=True))\n",
    "\n",
    "efficientnet_b3_noise_out = EfficientNetB3AuthenticityPredictor(freeze_backbone=True)\n",
    "efficientnet_b3_noise_out.load_state_dict(torch.load('../Models/EfficientNet-B3/Weights/real_authenticity_noise_out_pruned_model.pth', weights_only=True))\n",
    "\n",
    "# ResNet-152 Models\n",
    "resnet152_base = ResNet152AuthenticityPredictor(freeze_backbone=True)\n",
    "resnet152_base.load_state_dict(torch.load('../Models/ResNet-152/Weights/ResNet-152_real_authenticity_finetuned.pth', weights_only=True))\n",
    "\n",
    "# resnet152_negative_impact = ResNet152AuthenticityPredictor(freeze_backbone=True)\n",
    "# resnet152_negative_impact.load_state_dict(torch.load('../Models/ResNet-152/Weights/real_authenticity_negative_impact_pruned_model.pth', weights_only=True))\n",
    "\n",
    "resnet152_noise_out = ResNet152AuthenticityPredictor(freeze_backbone=True)\n",
    "resnet152_noise_out.load_state_dict(torch.load('../Models/ResNet-152/Weights/real_authenticity_noise_out_pruned_model.pth', weights_only=True))\n",
    "\n",
    "# VGG19 Models\n",
    "vgg19_base = VGG19AuthenticityPredictor(freeze_backbone=True)\n",
    "vgg19_base.load_state_dict(torch.load('../Models/VGG19/Weights/VGG19_real_authenticity_finetuned.pth', weights_only=True))\n",
    "\n",
    "# vgg19_negative_impact = VGG19AuthenticityPredictor(freeze_backbone=True)\n",
    "# vgg19_negative_impact.load_state_dict(torch.load('../Models/VGG19/Weights/real_authenticity_negative_impact_pruned_model.pth', weights_only=True))\n",
    "\n",
    "vgg19_noise_out = VGG19AuthenticityPredictor(freeze_backbone=True)\n",
    "vgg19_noise_out.load_state_dict(torch.load('../Models/VGG19/Weights/real_authenticity_noise_out_pruned_model.pth', weights_only=True))\n",
    "\n",
    "# VGG16 Models\n",
    "vgg16_base = VGG16AuthenticityPredictor(freeze_backbone=True)\n",
    "vgg16_base.load_state_dict(torch.load('../Models/VGG16/Weights/VGG-16_real_authenticity_finetuned.pth', weights_only=True))\n",
    "\n",
    "vgg16_negative_impact = VGG16AuthenticityPredictor(freeze_backbone=True)\n",
    "vgg16_negative_impact.load_state_dict(torch.load('../Models/VGG16/Weights/real_authenticity_negative_impact_pruned_model.pth', weights_only=True))\n",
    "\n",
    "vgg16_noise_out = VGG16AuthenticityPredictor(freeze_backbone=True)\n",
    "vgg16_noise_out.load_state_dict(torch.load('../Models/VGG16/Weights/real_authenticity_noise_out_pruned_model.pth', weights_only=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3325e2e7",
   "metadata": {},
   "source": [
    "# Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac6a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: BarlowTwins_base\n",
      "  RMSE: 7.0579\n",
      "  PLCC: 0.6516\n",
      "  SRCC: 0.5901\n",
      "  KRCC: 0.4228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_all_models(models, imageNet_dataloaders, denseNet_dataloaders, device='cuda'):\n",
    "    \"\"\"\n",
    "    Tests all models on the test dataset and stores the results in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        models (dict): Dictionary of model names and their corresponding model instances.\n",
    "        imageNet_dataloaders (dict): Dictionary of data loaders for ImageNet-compatible models.\n",
    "        denseNet_dataloaders (dict): Dictionary of data loaders for DenseNet models.\n",
    "        criterion (nn.Module): The loss function.\n",
    "        device (str): Device to use for testing ('cuda' or 'cpu'). Defaults to 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the metrics for each model on the test dataset.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        # Select the appropriate dataloader based on the model name\n",
    "        if 'DenseNet' in model_name:\n",
    "            \n",
    "            dataloader = denseNet_dataloaders['test']\n",
    "        else:\n",
    "            \n",
    "            dataloader = imageNet_dataloaders['test']\n",
    "\n",
    "        test_metrics = test_model(model, dataloader, device)\n",
    "        results[model_name] = test_metrics\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Define the models in a dictionary\n",
    "models = {\n",
    "    'BarlowTwins_base': barlow_twins_base,\n",
    "    # 'BarlowTwins_sd_base': barlow_twins_sd_base,\n",
    "    # 'BarlowTwins_negative_impact': barlow_twins_negative_impact,\n",
    "    'BarlowTwins_pruned': barlow_twins_noise_out,\n",
    "    'DenseNet161_base': densenet161_base,\n",
    "    # 'DenseNet161_negative_impact': densenet161_negative_impact,\n",
    "    'DenseNet161_pruned': densenet161_noise_out,\n",
    "    'EfficientNetB3_base': efficientnet_b3_base,\n",
    "    # 'EfficientNetB3_negative_impact': efficientnet_b3_negative_impact,\n",
    "    'EfficientNetB3_pruned': efficientnet_b3_noise_out,\n",
    "    'ResNet152_base': resnet152_base,\n",
    "    # 'ResNet152_negative_impact': resnet152_negative_impact,\n",
    "    'ResNet152_pruned': resnet152_noise_out,\n",
    "    'VGG19_base': vgg19_base,\n",
    "    # 'VGG19_negative_impact': vgg19_negative_impact,\n",
    "    'VGG19_pruned': vgg19_noise_out,\n",
    "    'VGG16_base': vgg16_base,\n",
    "    # 'VGG16_negative_impact': vgg16_negative_impact,\n",
    "    'VGG16_pruned': vgg16_noise_out,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Test all models and store the results\n",
    "results = test_all_models(models, imageNet_dataloaders, denseNet_dataloaders, device='cuda')\n",
    "\n",
    "# Print the results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"  PLCC: {metrics['plcc']:.4f}\")\n",
    "    print(f\"  SRCC: {metrics['srcc']:.4f}\")\n",
    "    print(f\"  KRCC: {metrics['krcc']:.4f}\")\n",
    "    print()  # Print a newline for better readability\n",
    "# Save the results to a markdown file\n",
    "with open('model_results.md', 'w') as f:\n",
    "    f.write(\"# Model Results\\n\\n\")\n",
    "    for model_name, metrics in results.items():\n",
    "        f.write(f\"## {model_name}\\n\")\n",
    "        f.write(f\"- RMSE: {metrics['rmse']:.4f}\\n\")\n",
    "        f.write(f\"- PLCC: {metrics['plcc']:.4f}\\n\")\n",
    "        f.write(f\"- SRCC: {metrics['srcc']:.4f}\\n\")\n",
    "        f.write(f\"- KRCC: {metrics['krcc']:.4f}\\n\\n\")\n",
    "    f.write(\"## Summary\\n\")\n",
    "    f.write(\"| Model Name | RMSE | PLCC | SRCC | KRCC |\\n\")\n",
    "    f.write(\"|------------|------|------|------|------|\\n\")\n",
    "    for model_name, metrics in results.items():\n",
    "        f.write(f\"| {model_name} | {metrics['rmse']:.4f} | {metrics['plcc']:.4f} | {metrics['srcc']:.4f} | {metrics['krcc']:.4f} |\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
