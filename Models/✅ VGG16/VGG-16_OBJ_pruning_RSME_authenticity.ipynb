{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database creations using pytorch Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAuthenticityDataset(Dataset):\n",
    "    \"\"\"Dataset for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its labels by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (image, labels) where:\n",
    "                image (PIL.Image): The image.\n",
    "                labels (torch.Tensor): Tensor containing quality and authenticity scores.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(os.getcwd(), self.data.iloc[idx, 3])  # image_path column\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        authenticity = self.data.iloc[idx, 1]  # Authenticity column\n",
    "        labels = torch.tensor([authenticity], dtype=torch.float)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthenticityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in vgg.features.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Extract features up to fc2\n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        self.fc1 = vgg.classifier[:-1]  # Up to fc2 (4096 -> 128)\n",
    "        \n",
    "        # New regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1)  # Predict authenticity\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        features = self.fc1(x)\n",
    "        predictions = self.regression_head(features)\n",
    "        return predictions, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations for the ImageNet dataset\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "annotations_file = 'Dataset/AIGCIQA2023/real_images_annotations.csv'\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageAuthenticityDataset(csv_file=annotations_file, transform=data_transforms)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 1 # Set to 1 for handling individual images\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# create a susset of the test dataset for testing\n",
    "test_subset_size = 10\n",
    "test_subset_indices = np.random.choice(len(test_dataset), test_subset_size, replace=False)\n",
    "test_subset = torch.utils.data.Subset(test_dataset, test_subset_indices)\n",
    "test_subset = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Create a dictionary containing the data loaders\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader,\n",
    "    'test': test_dataloader\n",
    "}\n",
    "\n",
    "model = AuthenticityPredictor()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss (regression)\n",
    "optimizer = optim.Adam(model.regression_head.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute for each image the importace scores of lastConv layer's channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_obj_x_obj_feature_map_importance(model, dataloader, device, layer_name):\n",
    "    \"\"\"Computes the importance of each feature map in a convolution layer by measuring the change in \n",
    "    predictions when the feature map is zeroed out, calculated per object.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained model\n",
    "        dataloader (DataLoader): Dataloader containing the images (with batch_size=1)\n",
    "        device (str): Device to run computation on ('cuda' or 'cpu')\n",
    "        layer_name (str): Name of the layer to analyze\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: An array where each element is a numpy array of channel importance scores\n",
    "                      for the corresponding image. Shape: [num_images]\n",
    "    \"\"\"\n",
    "    # Check if importance scores are already computed\n",
    "    if os.path.exists('Ranking_arrays/obj_x_obj_authenticity_importance_scores.npy'):\n",
    "        print(\"Per-object importance scores already computed, loading from file\")\n",
    "        return np.load('Ranking_arrays/obj_x_obj_authenticity_importance_scores.npy', allow_pickle=True)\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    named_modules = list(model.named_modules())\n",
    "    layer = None\n",
    "    \n",
    "    for name, module in named_modules:\n",
    "        if name == layer_name:\n",
    "            layer = module\n",
    "            break\n",
    "    \n",
    "    if layer is None:\n",
    "        raise ValueError(f\"Layer {layer_name} not found in model\")\n",
    "    \n",
    "    num_channels = layer.out_channels\n",
    "    \n",
    "    # Make sure batch_size=1 in the dataloader\n",
    "    if dataloader.batch_size != 1:\n",
    "        print(\"Warning: Dataloader batch size should be 1 for per-object importance computation\")\n",
    "    \n",
    "    # Create a directory for saving results if it doesn't exist\n",
    "    os.makedirs('Ranking_arrays', exist_ok=True)\n",
    "    \n",
    "    # Initialize array to store importance scores arrays for each image\n",
    "    num_images = len(dataloader)\n",
    "    importance_array = np.empty(num_images, dtype=object)\n",
    "    \n",
    "    # Process each image individually\n",
    "    for img_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        print(f\"Processing image {img_idx}/{num_images}\")\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Get baseline prediction\n",
    "        with torch.no_grad():\n",
    "            baseline_outputs, _ = model(inputs)\n",
    "            baseline_error = torch.abs(baseline_outputs - labels).item()\n",
    "        \n",
    "        # Initialize array for this image's channel importance scores\n",
    "        channel_scores = np.zeros(num_channels)\n",
    "        \n",
    "        # Compute importance for each feature map for this image\n",
    "        for channel_idx in range(num_channels):\n",
    "            # Create a backup of the weights and bias\n",
    "            backup_weights = layer.weight[channel_idx, ...].clone()\n",
    "            backup_bias = layer.bias[channel_idx].clone() if layer.bias is not None else None\n",
    "            \n",
    "            # Zero out the channel_idx-th output channel\n",
    "            with torch.no_grad():\n",
    "                layer.weight[channel_idx, ...] = 0\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias[channel_idx] = 0\n",
    "                \n",
    "            # Get prediction with the pruned feature map\n",
    "            with torch.no_grad():\n",
    "                pruned_outputs, _ = model(inputs)\n",
    "                pruned_error = torch.abs(pruned_outputs - labels).item()\n",
    "            \n",
    "            # Compute importance score\n",
    "            importance_score = pruned_error - baseline_error\n",
    "            \n",
    "            # Store the importance score in the array\n",
    "            channel_scores[channel_idx] = importance_score\n",
    "            \n",
    "            # Restore weights and bias\n",
    "            with torch.no_grad():\n",
    "                layer.weight[channel_idx, ...] = backup_weights\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias[channel_idx] = backup_bias\n",
    "        \n",
    "        # Store the channel scores array for this image\n",
    "        importance_array[img_idx] = channel_scores\n",
    "    \n",
    "    # Save results as a numpy array\n",
    "    np.save('Ranking_arrays/obj_x_obj_authenticity_importance_scores.npy', importance_array)\n",
    "    \n",
    "    # Return the array of arrays\n",
    "    return importance_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1774801/3334411481.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('Weights/VGG-16_real_authenticity_finetuned.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 0/138\n",
      "Processing image 1/138\n",
      "Processing image 2/138\n",
      "Processing image 3/138\n",
      "Processing image 4/138\n",
      "Processing image 5/138\n",
      "Processing image 6/138\n",
      "Processing image 7/138\n",
      "Processing image 8/138\n",
      "Processing image 9/138\n",
      "Processing image 10/138\n",
      "Processing image 11/138\n",
      "Processing image 12/138\n",
      "Processing image 13/138\n",
      "Processing image 14/138\n",
      "Processing image 15/138\n",
      "Processing image 16/138\n",
      "Processing image 17/138\n",
      "Processing image 18/138\n",
      "Processing image 19/138\n",
      "Processing image 20/138\n",
      "Processing image 21/138\n",
      "Processing image 22/138\n",
      "Processing image 23/138\n",
      "Processing image 24/138\n",
      "Processing image 25/138\n",
      "Processing image 26/138\n",
      "Processing image 27/138\n",
      "Processing image 28/138\n",
      "Processing image 29/138\n",
      "Processing image 30/138\n",
      "Processing image 31/138\n",
      "Processing image 32/138\n",
      "Processing image 33/138\n",
      "Processing image 34/138\n",
      "Processing image 35/138\n",
      "Processing image 36/138\n",
      "Processing image 37/138\n",
      "Processing image 38/138\n",
      "Processing image 39/138\n",
      "Processing image 40/138\n",
      "Processing image 41/138\n",
      "Processing image 42/138\n",
      "Processing image 43/138\n",
      "Processing image 44/138\n",
      "Processing image 45/138\n",
      "Processing image 46/138\n",
      "Processing image 47/138\n",
      "Processing image 48/138\n",
      "Processing image 49/138\n",
      "Processing image 50/138\n",
      "Processing image 51/138\n",
      "Processing image 52/138\n",
      "Processing image 53/138\n",
      "Processing image 54/138\n",
      "Processing image 55/138\n",
      "Processing image 56/138\n",
      "Processing image 57/138\n",
      "Processing image 58/138\n",
      "Processing image 59/138\n",
      "Processing image 60/138\n",
      "Processing image 61/138\n",
      "Processing image 62/138\n",
      "Processing image 63/138\n",
      "Processing image 64/138\n",
      "Processing image 65/138\n",
      "Processing image 66/138\n",
      "Processing image 67/138\n",
      "Processing image 68/138\n",
      "Processing image 69/138\n",
      "Processing image 70/138\n",
      "Processing image 71/138\n",
      "Processing image 72/138\n",
      "Processing image 73/138\n",
      "Processing image 74/138\n",
      "Processing image 75/138\n",
      "Processing image 76/138\n",
      "Processing image 77/138\n",
      "Processing image 78/138\n",
      "Processing image 79/138\n",
      "Processing image 80/138\n",
      "Processing image 81/138\n",
      "Processing image 82/138\n",
      "Processing image 83/138\n",
      "Processing image 84/138\n",
      "Processing image 85/138\n",
      "Processing image 86/138\n",
      "Processing image 87/138\n",
      "Processing image 88/138\n",
      "Processing image 89/138\n",
      "Processing image 90/138\n",
      "Processing image 91/138\n",
      "Processing image 92/138\n",
      "Processing image 93/138\n",
      "Processing image 94/138\n",
      "Processing image 95/138\n",
      "Processing image 96/138\n",
      "Processing image 97/138\n",
      "Processing image 98/138\n",
      "Processing image 99/138\n",
      "Processing image 100/138\n",
      "Processing image 101/138\n",
      "Processing image 102/138\n",
      "Processing image 103/138\n",
      "Processing image 104/138\n",
      "Processing image 105/138\n",
      "Processing image 106/138\n",
      "Processing image 107/138\n",
      "Processing image 108/138\n",
      "Processing image 109/138\n",
      "Processing image 110/138\n",
      "Processing image 111/138\n",
      "Processing image 112/138\n",
      "Processing image 113/138\n",
      "Processing image 114/138\n",
      "Processing image 115/138\n",
      "Processing image 116/138\n",
      "Processing image 117/138\n",
      "Processing image 118/138\n",
      "Processing image 119/138\n",
      "Processing image 120/138\n",
      "Processing image 121/138\n",
      "Processing image 122/138\n",
      "Processing image 123/138\n",
      "Processing image 124/138\n",
      "Processing image 125/138\n",
      "Processing image 126/138\n",
      "Processing image 127/138\n",
      "Processing image 128/138\n",
      "Processing image 129/138\n",
      "Processing image 130/138\n",
      "Processing image 131/138\n",
      "Processing image 132/138\n",
      "Processing image 133/138\n",
      "Processing image 134/138\n",
      "Processing image 135/138\n",
      "Processing image 136/138\n",
      "Processing image 137/138\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LAYER to prune \n",
    "LAYER = 'features.28'  # Last convolutional layer (assuming it has 512 channels)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Make sure batch size is 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = AuthenticityPredictor()\n",
    "model.load_state_dict(torch.load('Weights/VGG-16_real_authenticity_finetuned.pth'))\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Compute per-object feature importance\n",
    "obj_x_obj_importance = compute_obj_x_obj_feature_map_importance(model, dataloaders['test'], DEVICE, LAYER)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
