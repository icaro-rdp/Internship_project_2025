{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database creations using pytorch Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageQualityDataset(Dataset):\n",
    "    \"\"\"Dataset for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its labels by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (image, labels) where:\n",
    "                image (PIL.Image): The image.\n",
    "                labels (torch.Tensor): Tensor containing quality and authenticity scores.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(os.getcwd(), self.data.iloc[idx, 3])  # image_path column\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        quality = self.data.iloc[idx, 0]  # Quality column\n",
    "        authenticity = self.data.iloc[idx, 1]  # Authenticity column\n",
    "        labels = torch.tensor([quality, authenticity], dtype=torch.float)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    \"\"\"VGG16 model for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, num_outputs=2):\n",
    "        \"\"\"\n",
    "        Initializes the VGG16 model.\n",
    "\n",
    "        Args:\n",
    "            num_outputs (int): Number of output features. Defaults to 2 (quality and authenticity).\n",
    "        \"\"\"\n",
    "        super(VGG16, self).__init__()\n",
    "        # Load pre-trained VGG16 model\n",
    "        self.vgg16 = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "\n",
    "        # Freeze all layers\n",
    "        for param in self.vgg16.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Modify the classifier\n",
    "        num_features = self.vgg16.classifier[6].in_features\n",
    "        self.vgg16.classifier = nn.Sequential(\n",
    "            *list(self.vgg16.classifier.children())[:-1],  # Remove last layer with 1000 outputs\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_outputs)  # Add new layer with num_out outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.vgg16(x)\n",
    "    \n",
    "class QualityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in vgg.features.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Extract features up to fc2\n",
    "        self.features = vgg.features\n",
    "        self.avgpool = vgg.avgpool\n",
    "        self.fc1 = vgg.classifier[:-1]  # Up to fc2 (4096 -> 128)\n",
    "        \n",
    "        # New regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2)  # Predict quality and realness\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        features = self.fc1(x)\n",
    "        predictions = self.regression_head(features)\n",
    "        return predictions, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Trains the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloaders (dict): A dictionary containing the training and validation data loaders.\n",
    "        criterion (nn.Module): The loss function.\n",
    "        optimizer (optim.Optimizer): The optimizer.\n",
    "        num_epochs (int): Number of epochs to train for. Defaults to 10.\n",
    "        device (str): Device to use for training ('cuda' or 'cpu'). Defaults to 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The trained model.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:  # Iterate over training and validation phases\n",
    "            print(f'{phase} phase')\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:  # Iterate over data in the current phase\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):  # Enable gradients only during training\n",
    "                    outputs, _ = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}') # Print loss for the current phase\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return model\n",
    "\n",
    "def test_model(model, dataloader, criterion, device='cuda'):\n",
    "\n",
    "    \"\"\"\n",
    "    Tests the model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        dataloader (DataLoader): The test data loader.\n",
    "        criterion (nn.Module): The loss function.\n",
    "        device (str): Device to use for testing ('cuda' or 'cpu'). Defaults to 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss on the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs, _ = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    test_loss = running_loss / len(dataloader.dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    return test_loss\n",
    "\n",
    "def get_predictions(model, dataloader, device)-> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Get predictions from the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        dataloader (DataLoader): The data loader.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (predictions, labels) where:\n",
    "            predictions (torch.Tensor): Predictions from the model.\n",
    "            labels (torch.Tensor): Ground truth labels.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, target in dataloader:\n",
    "            outputs, _ = model(inputs.to(device))\n",
    "            predictions.append(outputs)\n",
    "            labels.append(target)\n",
    "\n",
    "    #move to cpu and concatenate\n",
    "    predictions = torch.cat(predictions).cpu()\n",
    "    labels = torch.cat(labels).cpu()\n",
    "\n",
    "    return predictions, labels\n",
    "\n",
    "def get_regression_errors(tuple: tuple[torch.Tensor, torch.Tensor]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Get regression errors.\n",
    "\n",
    "    Args:\n",
    "        tuple: A tuple (predictions, labels) where:\n",
    "            predictions (torch.Tensor): Predictions from the model.\n",
    "            labels (torch.Tensor): Ground truth labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (quality_errors, authenticity_errors) where:\n",
    "            quality_errors (torch.Tensor): Quality errors.\n",
    "            authenticity_errors (torch.Tensor): Authenticity errors.\n",
    "    \"\"\"\n",
    "    predictions, labels = tuple\n",
    "    quality_errors = predictions[:, 0] - labels[:, 0]\n",
    "    authenticity_errors = predictions[:, 1] - labels[:, 1]\n",
    "    return quality_errors, authenticity_errors\n",
    "\n",
    "def get_rmse(errors: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get the root mean squared error.\n",
    "\n",
    "    Args:\n",
    "        errors (torch.Tensor): Errors.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Root mean squared error.\n",
    "    \"\"\"\n",
    "    return torch.sqrt(torch.mean(errors ** 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations for the ImageNet dataset\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "annotations_file = 'Dataset/AIGCIQA2023/mos_data.csv'\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageQualityDataset(csv_file=annotations_file, transform=data_transforms)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Create a dictionary containing the data loaders\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader,\n",
    "    'test': test_dataloader\n",
    "}\n",
    "\n",
    "model = QualityPredictor()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss (regression)\n",
    "optimizer = optim.Adam(model.regression_head.parameters(), lr=0.001)\n",
    "\n",
    "model_path = 'Weights/VGG-16_finetuned_regression.pth'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_predictor_trained= train_model(model, dataloaders, criterion, optimizer, EPOCHS, device)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(quality_predictor_trained.state_dict(), model_path)\n",
    "\n",
    "# Load the trained model\n",
    "quality_predictor_trained = QualityPredictor()\n",
    "quality_predictor_trained.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities for pruning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_map_importance(model, dataloader, device, layer_name) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Computes the importance of each feature map in a convolution\n",
    "    layer by measuring the change in predictions when the feature map is zero\n",
    "    out.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (indices, importance_scores) where both are numpy arrays\n",
    "    \"\"\"\n",
    "    #if importance_scores.npy exists, load it\n",
    "    if os.path.exists('importance_scores.npy'):\n",
    "        return np.load('importance_scores.npy')\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    importance_scores = []\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    baseline_predictions = get_predictions(model, dataloader, device)\n",
    "    regression_errors = get_regression_errors(baseline_predictions)\n",
    "    quality_errors, authenticity_errors = regression_errors\n",
    "    baseline_quality_rmse = get_rmse(quality_errors)\n",
    "    baseline_authenticity_rmse = get_rmse(authenticity_errors)\n",
    "    average_baseline_rmse = (baseline_quality_rmse + baseline_authenticity_rmse) / 2\n",
    "\n",
    "    \n",
    "    print(f'Average baseline RMSE: {average_baseline_rmse:.4f}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(layer.out_channels):\n",
    "            # Create a backup of the weights and bias\n",
    "            backup_weights = layer.weight[i, ...].clone()\n",
    "            backup_bias = layer.bias[i].clone() if layer.bias is not None else None\n",
    "\n",
    "            # Zero out the i-th output channel\n",
    "            layer.weight[i, ...] = 0\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[i] = 0\n",
    "\n",
    "            # Get predictions with the pruned feature map\n",
    "            pruned_predictions = get_predictions(model, dataloader, device)\n",
    "            pruned_regression_errors = get_regression_errors(pruned_predictions)\n",
    "            pruned_quality_errors, pruned_authenticity_errors = pruned_regression_errors\n",
    "            pruned_quality_rmse = get_rmse(pruned_quality_errors)\n",
    "            pruned_authenticity_rmse = get_rmse(pruned_authenticity_errors)\n",
    "            average_pruned_rmse = (pruned_quality_rmse + pruned_authenticity_rmse) / 2\n",
    "    \n",
    "            # Compute importance score\n",
    "            importance_score = average_baseline_rmse - average_pruned_rmse\n",
    "            importance_scores.append([i, importance_score])\n",
    "            \n",
    "\n",
    "            print(f'Feature map {i}: Importance score: {importance_score:.4f}')\n",
    "            \n",
    "            # After computing importance, restore weights and bias\n",
    "            layer.weight[i, ...] = backup_weights\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[i] = backup_bias \n",
    "\n",
    "    sorted_importance_scores = sorted(importance_scores, key=lambda x: x[1], reverse=True)\n",
    "    # save np array \n",
    "    np.save('importance_scores.npy', sorted_importance_scores)\n",
    "    return np.array(sorted_importance_scores)\n",
    "\n",
    "def find_optimal_feature_subset(model, dataloader, device, layer_name, sorted_importance_scores, model_path='Weights/pruned_model.pth'):\n",
    "    \"\"\"\n",
    "    Find an optimal subset of feature maps by iteratively adding features in order of importance\n",
    "    and tracking model performance, keeping the subset that maximizes performance.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        dataloader: DataLoader for evaluation\n",
    "        device: Device to run the model on (cuda/cpu)\n",
    "        layer_name: Name of the layer to optimize\n",
    "        sorted_importance_scores: List of tuples (channel_index, importance_score) sorted by importance\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with optimal subset and performance metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Reverse the sorted importance scores\n",
    "    sorted_importance_scores = sorted_importance_scores[::-1]\n",
    "    \n",
    "    # Get the target layer\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    \n",
    "    # Create a backup of the original weights and bias\n",
    "    original_weights = layer.weight.clone()\n",
    "    original_bias = layer.bias.clone() if layer.bias is not None else None\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_rmse = float('inf')\n",
    "    best_subset = []\n",
    "    rmse_history = []\n",
    "    current_subset = []\n",
    "    \n",
    "    # Get baseline with no features (all zeroed out)\n",
    "    layer.weight.data.fill_(0)\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.fill_(0)\n",
    "        \n",
    "    baseline_predictions = get_predictions(model, dataloader, device)\n",
    "    baseline_regression_errors = get_regression_errors(baseline_predictions)\n",
    "    baseline_quality_errors, baseline_authenticity_errors = baseline_regression_errors\n",
    "    baseline_quality_rmse = get_rmse(baseline_quality_errors)\n",
    "    baseline_authenticity_rmse = get_rmse(baseline_authenticity_errors)\n",
    "    baseline_rmse = (baseline_quality_rmse + baseline_authenticity_rmse) / 2\n",
    "    \n",
    "    print(f\"Baseline RMSE (no features): {baseline_rmse:.4f}\")\n",
    "    print(\"------------------\")\n",
    "    \n",
    "    # Track performance with empty set\n",
    "    rmse_history.append(([], baseline_rmse))\n",
    "    \n",
    "    # Iteratively add feature maps in order of importance\n",
    "    for idx, (channel_idx, _) in enumerate(sorted_importance_scores):\n",
    "        channel_idx = int(channel_idx)\n",
    "        \n",
    "        # Add this feature map to the current subset\n",
    "        current_subset.append(channel_idx)\n",
    "        \n",
    "        # Reset all weights to zero first\n",
    "        layer.weight.data.fill_(0)\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0)\n",
    "        \n",
    "        # Enable only the feature maps in the current subset\n",
    "        for ch_idx in current_subset:\n",
    "            layer.weight[ch_idx, ...] = original_weights[ch_idx, ...]\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[ch_idx] = original_bias[ch_idx]\n",
    "        \n",
    "        # Evaluate model with current subset\n",
    "        predictions = get_predictions(model, dataloader, device)\n",
    "        regression_errors = get_regression_errors(predictions)\n",
    "        quality_errors, authenticity_errors = regression_errors\n",
    "        quality_rmse = get_rmse(quality_errors)\n",
    "        authenticity_rmse = get_rmse(authenticity_errors)\n",
    "        current_rmse = (quality_rmse + authenticity_rmse) / 2\n",
    "        \n",
    "        # Record performance\n",
    "        rmse_history.append((current_subset.copy(), current_rmse))\n",
    "        \n",
    "        print(f\"Iteration {idx+1}/{len(sorted_importance_scores)}: \" +\n",
    "              f\"Added channel {channel_idx}, \" +\n",
    "              f\"Subset size: {len(current_subset)}, \" +\n",
    "              f\"RMSE: {current_rmse:.4f}\")\n",
    "        \n",
    "        # Update best subset if this one is better\n",
    "        if current_rmse < best_rmse:\n",
    "            best_rmse = current_rmse\n",
    "            best_subset = current_subset.copy()\n",
    "            print(f\"  ✓ New best subset found! RMSE: {best_rmse:.4f}\")\n",
    "    \n",
    "    print(\"\\n------------------\")\n",
    "    print(f\"Best RMSE: {best_rmse:.4f} with {len(best_subset)} features\")\n",
    "    print(f\"Improvement over baseline: {baseline_rmse - best_rmse:.4f}\")\n",
    "    print(f\"Feature reduction: {(1 - len(best_subset)/len(sorted_importance_scores))*100:.1f}%\")\n",
    "    \n",
    "    # Apply the best subset to the model\n",
    "    layer.weight.data.fill_(0)\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.fill_(0)\n",
    "        \n",
    "    for ch_idx in best_subset:\n",
    "        layer.weight[ch_idx, ...] = original_weights[ch_idx, ...]\n",
    "        if layer.bias is not None:\n",
    "            layer.bias[ch_idx] = original_bias[ch_idx]\n",
    "    \n",
    "    # Save the pruned model\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Restore original weights for future use\n",
    "    layer.weight.data.copy_(original_weights)\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.copy_(original_bias)\n",
    "    \n",
    "    return {\n",
    "        'best_subset': best_subset,\n",
    "        'best_rmse': best_rmse,\n",
    "        'baseline_rmse': baseline_rmse,\n",
    "        'improvement': baseline_rmse - best_rmse,\n",
    "        'reduction_percentage': (1 - len(best_subset)/len(sorted_importance_scores))*100,\n",
    "        'rmse_history': rmse_history\n",
    "    }\n",
    "\n",
    "def remove_noisy_feature_maps(model, dataloader, device, layer_name, sorted_importance_scores, model_path='Weights/pruned_model.pth'):\n",
    "    \"\"\"\n",
    "    Remove noisy feature maps from a convolutional layer based on importance scores.\n",
    "    Feature maps are zeroed out one by one and kept zeroed only if model performance improves.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        dataloader: DataLoader for evaluation\n",
    "        device: Device to run the model on (cuda/cpu)\n",
    "        layer_name: Name of the layer to optimize\n",
    "        sorted_importance_scores: List of tuples (channel_index, importance_score) sorted by importance\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with pruning results and performance metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Get the target layer\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    \n",
    "    # Create a backup of the original weights and bias\n",
    "    original_weights = layer.weight.clone()\n",
    "    original_bias = layer.bias.clone() if layer.bias is not None else None\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    removed_features = []\n",
    "    rmse_history = []\n",
    "    \n",
    "    # Get baseline performance\n",
    "    baseline_predictions = get_predictions(model, dataloader, device)\n",
    "    baseline_regression_errors = get_regression_errors(baseline_predictions)\n",
    "    baseline_quality_errors, baseline_authenticity_errors = baseline_regression_errors\n",
    "    baseline_quality_rmse = get_rmse(baseline_quality_errors)\n",
    "    baseline_authenticity_rmse = get_rmse(baseline_authenticity_errors)\n",
    "    average_baseline_rmse = (baseline_quality_rmse + baseline_authenticity_rmse) / 2\n",
    "    \n",
    "    print(f\"Baseline RMSE: {average_baseline_rmse:.4f}\")\n",
    "    print(\"------------------\")\n",
    "    \n",
    "    # Track initial performance\n",
    "    rmse_history.append(([], average_baseline_rmse))\n",
    "    baseline_rmse = average_baseline_rmse\n",
    "    \n",
    "    # Iterate over the sorted indices and if removing a feature map improves performance, keep it removed\n",
    "    for idx, (channel_idx, importance_score) in enumerate(sorted_importance_scores):\n",
    "        channel_idx = int(channel_idx)\n",
    "        \n",
    "        # Temporarily zero out this feature map\n",
    "        layer.weight[channel_idx, ...] = 0\n",
    "        if layer.bias is not None:\n",
    "            layer.bias[channel_idx] = 0\n",
    "        \n",
    "        # Evaluate model with feature map removed\n",
    "        predictions = get_predictions(model, dataloader, device)\n",
    "        regression_errors = get_regression_errors(predictions)\n",
    "        quality_errors, authenticity_errors = regression_errors\n",
    "        quality_rmse = get_rmse(quality_errors)\n",
    "        authenticity_rmse = get_rmse(authenticity_errors)\n",
    "        average_new_rmse = (quality_rmse + authenticity_rmse) / 2\n",
    "        \n",
    "        print(f\"Iteration {idx+1}/{len(sorted_importance_scores)}: \" +\n",
    "              f\"Testing removal of channel {channel_idx}, \" +\n",
    "              f\"Importance: {importance_score:.4f}, \" +\n",
    "              f\"RMSE: {average_new_rmse:.4f}\")\n",
    "        \n",
    "        # Decide whether to keep this feature map removed\n",
    "        if average_new_rmse < baseline_rmse:\n",
    "            baseline_rmse = average_new_rmse if average_new_rmse < average_baseline_rmse else average_baseline_rmse\n",
    "            removed_features.append(channel_idx)\n",
    "            rmse_history.append((removed_features.copy(), baseline_rmse))\n",
    "            print(f\"  ✓ IMPROVING: Zeroing out feature map {channel_idx}\")\n",
    "        else:\n",
    "            # Restore the feature map\n",
    "            layer.weight[channel_idx, ...] = original_weights[channel_idx, ...]\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[channel_idx] = original_bias[channel_idx]\n",
    "            print(f\"  ✗ NOT IMPROVING: Keeping feature map {channel_idx}\")\n",
    "        \n",
    "        print(f\"  Current best RMSE: {baseline_rmse:.4f}\")\n",
    "        print(\"------------------\")\n",
    "    \n",
    "    # Final statistics\n",
    "    print(\"\\n------------------\")\n",
    "    print(f\"Final RMSE: {baseline_rmse:.4f} after removing {len(removed_features)} feature maps\")\n",
    "    print(f\"Improvement over baseline: {average_baseline_rmse - baseline_rmse:.4f}\")\n",
    "    print(f\"Feature reduction: {(len(removed_features)/len(sorted_importance_scores))*100:.1f}%\")\n",
    "    \n",
    "    # Save the pruned model\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    return {\n",
    "        'removed_features': removed_features,\n",
    "        'baseline_rmse': average_baseline_rmse,\n",
    "        'final_rmse': baseline_rmse,\n",
    "        'improvement': average_baseline_rmse - baseline_rmse,\n",
    "        'reduction_percentage': (len(removed_features)/len(sorted_importance_scores))*100,\n",
    "        'rmse_history': rmse_history\n",
    "    }\n",
    "\n",
    "def remove_negative_impact_feature_maps(model, dataloader, device, layer_name, sorted_importance_scores, model_path='Weights/negative_impact_pruned_model.pth'):\n",
    "    \"\"\"\n",
    "    Remove feature maps that have a negative impact on model performance based on importance scores (impotance score < 0).\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        dataloader: DataLoader for evaluation\n",
    "        device: Device to run the model on (cuda/cpu)\n",
    "        layer_name: Name of the layer to optimize\n",
    "        sorted_importance_scores: List of tuples (channel_index, importance_score) sorted by importance\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with pruning results and performance metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Get the target layer\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    \n",
    "    # Create a backup of the original weights and bias\n",
    "    original_weights = layer.weight.clone()\n",
    "    original_bias = layer.bias.clone() if layer.bias is not None else None\n",
    "    \n",
    "    # Get baseline performance\n",
    "    predictions = get_predictions(model, dataloader, device)\n",
    "    regression_errors = get_regression_errors(predictions)\n",
    "    quality_errors, authenticity_errors = regression_errors\n",
    "    quality_rmse = get_rmse(quality_errors)\n",
    "    authenticity_rmse = get_rmse(authenticity_errors)\n",
    "    baseline_rmse = (quality_rmse + authenticity_rmse) / 2\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    removed_features = []\n",
    "    \n",
    "    # Iterate over the sorted indices and zero out all the feature maps that have a negative impact (importance < 0)\n",
    "\n",
    "    for idx, (channel_idx, importance_score) in enumerate(sorted_importance_scores):\n",
    "        print(f\"Iteration {idx} - Channel {channel_idx}: Importance score: {importance_score:.4f}\")\n",
    "        if importance_score < 0:\n",
    "            channel_idx = int(channel_idx)\n",
    "            layer.weight[channel_idx, ...] = 0\n",
    "            if layer.bias is not None:\n",
    "                layer.bias[channel_idx] = 0\n",
    "            removed_features.append(channel_idx)\n",
    "\n",
    "    # Evaluate model with feature maps removed\n",
    "    new_predictions = get_predictions(model, dataloader, device)\n",
    "    new_regression_errors = get_regression_errors(new_predictions)\n",
    "    new_quality_errors, new_authenticity_errors = new_regression_errors\n",
    "    new_quality_rmse = get_rmse(new_quality_errors)\n",
    "    new_authenticity_rmse = get_rmse(new_authenticity_errors)\n",
    "    new_rmse = (new_quality_rmse + new_authenticity_rmse) / 2\n",
    "\n",
    "    # Save the pruned model\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # Restore original weights for future use\n",
    "    layer.weight.data.copy_(original_weights)\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.copy_(original_bias)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'removed_features': removed_features,\n",
    "        'baseline_rmse': baseline_rmse,\n",
    "        'final_rmse': new_rmse,\n",
    "        'improvement': baseline_rmse - new_rmse,\n",
    "        'reduction_percentage': (len(removed_features)/len(sorted_importance_scores))*100\n",
    "    }\n",
    "\n",
    "def remove_channels(model,device,layer_name,channels_indexes)->QualityPredictor:\n",
    "    \"\"\"\n",
    "    Remove channels, using an index list, from a convolutional layer in a model.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        device: Device to run the model on (cuda/cpu)\n",
    "        layer_name: Name of the layer to optimize\n",
    "        channels_indexes: List of channel indexes to remove\n",
    "        \n",
    "    Returns:\n",
    "        The pruned model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Get the target layer\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "        \n",
    "    # Zero out the specified channels\n",
    "    for channel_idx in channels_indexes:\n",
    "        layer.weight[channel_idx, ...] = 0\n",
    "        if layer.bias is not None:\n",
    "            layer.bias[channel_idx] = 0\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of diffrent models using different pruning techniques\n",
    "\n",
    "- Deletion of models is due to make sure that im not using the same model again and again (first draft, not sure if im correctlly restoring weights in each pruning technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYER to prune\n",
    "LAYER = 'features.28'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# Base model for importance score computation\n",
    "base_model = QualityPredictor()\n",
    "base_model.load_state_dict(torch.load('Weights/VGG-16_finetuned_regression.pth'))\n",
    "base_model.eval()\n",
    "base_model.to(DEVICE)\n",
    "\n",
    "sorted_importance_scores = compute_feature_map_importance(base_model, train_dataloader, DEVICE, LAYER)\n",
    "del base_model\n",
    "\n",
    "# Model for noisy feature maps removal\n",
    "noisy_pruning_model = QualityPredictor()\n",
    "noisy_pruning_model.load_state_dict(torch.load('Weights/VGG-16_finetuned_regression.pth'))\n",
    "noisy_pruning_model.eval()\n",
    "noisy_pruning_model.to(DEVICE)\n",
    "\n",
    "noisy_optimal_subset = remove_noisy_feature_maps(noisy_pruning_model, train_dataloader, DEVICE, LAYER, sorted_importance_scores, model_path='Weights/noise_out_pruned_model.pth')\n",
    "\n",
    "del noisy_pruning_model\n",
    "\n",
    "# Model for optimal subset selection\n",
    "optimal_subset_model = QualityPredictor()\n",
    "optimal_subset_model.load_state_dict(torch.load('Weights/VGG-16_finetuned_regression.pth'))\n",
    "optimal_subset_model.eval()\n",
    "optimal_subset_model.to(DEVICE)\n",
    "\n",
    "# Find the optimal subset of feature maps\n",
    "optimal_subset = find_optimal_feature_subset(optimal_subset_model, train_dataloader, DEVICE, LAYER, sorted_importance_scores, model_path='Weights/optimal_set_pruned_model.pth')\n",
    "\n",
    "# Model for negative impact feature maps removal\n",
    "negative_impact_model = QualityPredictor()\n",
    "negative_impact_model.load_state_dict(torch.load('Weights/VGG-16_finetuned_regression.pth'))\n",
    "negative_impact_model.eval()\n",
    "negative_impact_model.to(DEVICE)\n",
    "\n",
    "negative_impact_subset = remove_negative_impact_feature_maps(negative_impact_model, train_dataloader, DEVICE, LAYER, sorted_importance_scores, model_path='Weights/negative_impact_pruned_model.pth')\n",
    "\n",
    "del negative_impact_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with already saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISY_PRUNED_MODEL_PATH = 'Weights/noise_out_pruned_model.pth'\n",
    "BEST_SUBSET_PRUNED_MODEL_PATH = 'Weights/optimal_set_pruned_model.pth'\n",
    "NEGATIVE_IMPACT_PRUNED_MODEL_PATH = 'Weights/negative_impact_pruned_model.pth'\n",
    "\n",
    "noisy_pruned_model = QualityPredictor()\n",
    "noisy_pruned_model.load_state_dict(torch.load(NOISY_PRUNED_MODEL_PATH, weights_only=True))\n",
    "\n",
    "best_subset_pruned_model = QualityPredictor()\n",
    "best_subset_pruned_model.load_state_dict(torch.load(BEST_SUBSET_PRUNED_MODEL_PATH, weights_only=True))\n",
    "\n",
    "negative_impact_pruned_model = QualityPredictor()\n",
    "negative_impact_pruned_model.load_state_dict(torch.load(NEGATIVE_IMPACT_PRUNED_MODEL_PATH,weights_only=True))\n",
    "\n",
    "baseline_model = QualityPredictor()\n",
    "baseline_model.load_state_dict(torch.load('Weights/VGG-16_finetuned_regression.pth',weights_only=True))\n",
    "\n",
    "# Testing\n",
    "\n",
    "# Test the baseline model\n",
    "print(\"Testing the baseline model\")\n",
    "test_model(baseline_model, test_dataloader, criterion, device)\n",
    "print(\"------------------\")\n",
    "# test the noisy pruned model\n",
    "print(\"Testing the noisy pruned model\")\n",
    "test_model(noisy_pruned_model, test_dataloader, criterion, device)\n",
    "print(\"------------------\")\n",
    "# test the best subset pruned model\n",
    "print(\"Testing the best subset pruned model\")\n",
    "test_model(best_subset_pruned_model, test_dataloader, criterion, device)\n",
    "print(\"------------------\")\n",
    "# test the negative impact pruned model\n",
    "print(\"Testing the negative impact pruned model\")\n",
    "test_model(negative_impact_pruned_model, test_dataloader, criterion, device)\n",
    "print(\"------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Aanalysis - Comparing the models zeroed out weights & Correlations between the models predicitons and ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-out weights analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that extract the indices of the zeroed out feature maps in a convolutional layer\n",
    "\n",
    "def get_zeroed_feature_maps(model, layer_name):\n",
    "    \"\"\"\n",
    "    Get the indices of the zeroed out feature maps in a convolutional layer.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        layer_name (str): The name of the convolutional layer.\n",
    "\n",
    "    Returns:\n",
    "        list: The indices of the zeroed out feature maps.\n",
    "    \"\"\"\n",
    "    dict_modules = dict(model.named_modules())\n",
    "    layer = dict_modules[layer_name]\n",
    "    zeroed_feature_maps = []\n",
    "\n",
    "    for i, weight in enumerate(layer.weight):\n",
    "        if torch.all(weight == 0):\n",
    "            zeroed_feature_maps.append(i)\n",
    "    zeroed_feature_maps.sort()\n",
    "\n",
    "    num_zeroed = len(zeroed_feature_maps)\n",
    "\n",
    "    return zeroed_feature_maps, num_zeroed\n",
    "\n",
    "# Get the zeroed out feature maps in the 'features.28' layer of the noisy pruned model\n",
    "_, noisy_num_zeroed = get_zeroed_feature_maps(noisy_pruned_model, 'features.28')\n",
    "\n",
    "# Get the zeroed out feature maps in the 'features.28' layer of the best subset pruned model\n",
    "_, best_subset_num_zeroed = get_zeroed_feature_maps(best_subset_pruned_model, 'features.28')\n",
    "\n",
    "# Get the zeroed out feature maps in the 'features.28' layer of the negative impact pruned model\n",
    "_, negative_impact_num_zeroed = get_zeroed_feature_maps(negative_impact_pruned_model, 'features.28')\n",
    "\n",
    "print(f\"Noisy pruned model: {noisy_num_zeroed} zeroed out feature maps\")\n",
    "print(f\"Best subset pruned model: {best_subset_num_zeroed} zeroed out feature maps\")\n",
    "print(f\"Negative impact pruned model: {negative_impact_num_zeroed} zeroed out feature maps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "def compute_metrics(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Computes metrics for model evaluation including residuals.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained model to evaluate\n",
    "        dataloader (DataLoader): Test dataloader containing images and true scores\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing predictions, ground truth, and calculated metrics\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Collect predictions and ground truth\n",
    "    pred_list = []\n",
    "    true_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            pred_list.append(outputs.cpu())\n",
    "            true_list.append(labels)\n",
    "    \n",
    "    # Concatenate batches\n",
    "    predictions = torch.cat(pred_list, dim=0).numpy()\n",
    "    ground_truth = torch.cat(true_list, dim=0).numpy()\n",
    "    \n",
    "    # Extract quality and authenticity scores\n",
    "    pred_quality = predictions[:, 0]\n",
    "    true_quality = ground_truth[:, 0]\n",
    "    \n",
    "    pred_authenticity = predictions[:, 1]\n",
    "    true_authenticity = ground_truth[:, 1]\n",
    "    \n",
    "    # Calculate average scores\n",
    "    true_avg = (true_quality + true_authenticity) / 2\n",
    "    pred_avg = (pred_quality + pred_authenticity) / 2\n",
    "    \n",
    "    # Calculate residuals (important for understanding distribution)\n",
    "    quality_residuals = pred_quality - true_quality\n",
    "    auth_residuals = pred_authenticity - true_authenticity\n",
    "    avg_residuals = pred_avg - true_avg\n",
    "    \n",
    "    # Calculate correlation metrics for individual scores\n",
    "    quality_spearman, q_pvalue = spearmanr(pred_quality, true_quality)\n",
    "    auth_spearman, a_pvalue = spearmanr(pred_authenticity, true_authenticity)\n",
    "    avg_spearman, avg_pvalue = spearmanr(true_avg, pred_avg)\n",
    "    \n",
    "    # Calculate R² scores\n",
    "    quality_r2 = r2_score(true_quality, pred_quality)\n",
    "    auth_r2 = r2_score(true_authenticity, pred_authenticity)\n",
    "    avg_r2 = r2_score(true_avg, pred_avg)\n",
    "    \n",
    "    # Calculate RMSE scores\n",
    "    quality_rmse = np.sqrt(mean_squared_error(true_quality, pred_quality))\n",
    "    auth_rmse = np.sqrt(mean_squared_error(true_authenticity, pred_authenticity))\n",
    "    avg_rmse = np.sqrt(mean_squared_error(true_avg, pred_avg))\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'predictions': {\n",
    "            'quality': pred_quality,\n",
    "            'authenticity': pred_authenticity,\n",
    "            'average': pred_avg\n",
    "        },\n",
    "        'ground_truth': {\n",
    "            'quality': true_quality,\n",
    "            'authenticity': true_authenticity,\n",
    "            'average': true_avg\n",
    "        },\n",
    "        'residuals': {\n",
    "            'quality': quality_residuals,\n",
    "            'authenticity': auth_residuals,\n",
    "            'average': avg_residuals\n",
    "        },\n",
    "        'metrics': {\n",
    "            'quality': {\n",
    "                'spearman': quality_spearman,\n",
    "                'p_value': q_pvalue,\n",
    "                'r2': quality_r2,\n",
    "                'rmse': quality_rmse\n",
    "            },\n",
    "            'authenticity': {\n",
    "                'spearman': auth_spearman,\n",
    "                'p_value': a_pvalue,\n",
    "                'r2': auth_r2,\n",
    "                'rmse': auth_rmse\n",
    "            },\n",
    "            'average': {\n",
    "                'spearman': avg_spearman,\n",
    "                'p_value': avg_pvalue,\n",
    "                'r2': avg_r2,\n",
    "                'rmse': avg_rmse\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_correlations(results, save_path=None, title=\"\"):\n",
    "    \"\"\"\n",
    "    Visualizes correlations and distributions from computed metrics.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Results dictionary from compute_metrics function\n",
    "        save_path (str, optional): Path to save the plot. If None, plot is displayed instead.\n",
    "        title (str): Title for the plot\n",
    "    \"\"\"\n",
    "    # Extract data from results dictionary\n",
    "    pred_quality = results['predictions']['quality']\n",
    "    true_quality = results['ground_truth']['quality']\n",
    "    \n",
    "    pred_authenticity = results['predictions']['authenticity']\n",
    "    true_authenticity = results['ground_truth']['authenticity']\n",
    "    \n",
    "    pred_avg = results['predictions']['average']\n",
    "    true_avg = results['ground_truth']['average']\n",
    "    \n",
    "    quality_residuals = results['residuals']['quality']\n",
    "    auth_residuals = results['residuals']['authenticity']\n",
    "    avg_residuals = results['residuals']['average']\n",
    "    \n",
    "    # Get metrics\n",
    "    quality_metrics = results['metrics']['quality']\n",
    "    auth_metrics = results['metrics']['authenticity']\n",
    "    avg_metrics = results['metrics']['average']\n",
    "    \n",
    "    # Create a larger figure with 3 rows (metrics, scatter plots, residuals)\n",
    "    fig = plt.figure(figsize=(16, 18))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    gs = gridspec.GridSpec(3, 3, height_ratios=[1, 1, 1])\n",
    "    \n",
    "    # Row 1: Scatter plots for quality, authenticity, average\n",
    "    ax_quality = plt.subplot(gs[0, 0])\n",
    "    ax_auth = plt.subplot(gs[0, 1]) \n",
    "    ax_avg = plt.subplot(gs[0, 2])\n",
    "    \n",
    "    # Row 2: Residual plots (residuals vs. predicted values)\n",
    "    ax_quality_res = plt.subplot(gs[1, 0])\n",
    "    ax_auth_res = plt.subplot(gs[1, 1])\n",
    "    ax_avg_res = plt.subplot(gs[1, 2])\n",
    "    \n",
    "    # Row 3: Residual histograms\n",
    "    ax_quality_hist = plt.subplot(gs[2, 0])\n",
    "    ax_auth_hist = plt.subplot(gs[2, 1])\n",
    "    ax_avg_hist = plt.subplot(gs[2, 2])\n",
    "    \n",
    "    # Function to plot correlation scatter plot\n",
    "    def plot_correlation(ax, true_vals, pred_vals, metrics, title_suffix):\n",
    "        ax.scatter(true_vals, pred_vals, alpha=0.7, color='blue')\n",
    "        \n",
    "        # Add identity line\n",
    "        min_val = min(min(true_vals), min(pred_vals))\n",
    "        max_val = max(max(true_vals), max(pred_vals))\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect prediction')\n",
    "        \n",
    "        # Add regression line\n",
    "        z = np.polyfit(true_vals, pred_vals, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_sorted = np.sort(true_vals)\n",
    "        ax.plot(x_sorted, p(x_sorted), 'g-', label=f'Best fit (y = {z[0]:.3f}x + {z[1]:.3f})')\n",
    "        \n",
    "        ax.set_xlabel('True Value')\n",
    "        ax.set_ylabel('Predicted Value')\n",
    "        ax.set_title(f'{title_suffix}\\nSpearman ρ = {metrics[\"spearman\"]:.4f}, R² = {metrics[\"r2\"]:.4f}, RMSE = {metrics[\"rmse\"]:.4f}')\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.legend(loc='upper left')\n",
    "    \n",
    "    # Function to plot residuals vs predicted\n",
    "    def plot_residuals(ax, pred_vals, residuals, title_suffix):\n",
    "        ax.scatter(pred_vals, residuals, alpha=0.7, color='orange')\n",
    "        ax.axhline(y=0, color='r', linestyle='--')\n",
    "        ax.set_xlabel('Predicted Value')\n",
    "        ax.set_ylabel('Residual (Pred - True)')\n",
    "        ax.set_title(f'{title_suffix} Residuals vs Predicted')\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Function to plot residual histogram\n",
    "    def plot_residual_hist(ax, residuals, title_suffix):\n",
    "        ax.hist(residuals, bins=20, alpha=0.7, color='green')\n",
    "        ax.axvline(x=0, color='r', linestyle='--')\n",
    "        ax.set_xlabel('Residual Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title(f'{title_suffix} Residual Distribution')\n",
    "        # Add mean and std as text\n",
    "        mean_res = np.mean(residuals)\n",
    "        std_res = np.std(residuals)\n",
    "        ax.text(0.05, 0.95, f'Mean: {mean_res:.4f}\\nStd: {std_res:.4f}', \n",
    "                transform=ax.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    # Plot all charts\n",
    "    plot_correlation(ax_quality, true_quality, pred_quality, quality_metrics, 'Quality Score')\n",
    "    plot_correlation(ax_auth, true_authenticity, pred_authenticity, auth_metrics, 'Authenticity Score')\n",
    "    plot_correlation(ax_avg, true_avg, pred_avg, avg_metrics, 'Average Score')\n",
    "    \n",
    "    plot_residuals(ax_quality_res, pred_quality, quality_residuals, 'Quality')\n",
    "    plot_residuals(ax_auth_res, pred_authenticity, auth_residuals, 'Authenticity')\n",
    "    plot_residuals(ax_avg_res, pred_avg, avg_residuals, 'Average')\n",
    "    \n",
    "    plot_residual_hist(ax_quality_hist, quality_residuals, 'Quality')\n",
    "    plot_residual_hist(ax_auth_hist, auth_residuals, 'Authenticity')\n",
    "    plot_residual_hist(ax_avg_hist, avg_residuals, 'Average')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"MODEL EVALUATION: {title}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Quality Score - Spearman ρ: {quality_metrics['spearman']:.4f} (p-value: {quality_metrics['p_value']:.4g}), R²: {quality_metrics['r2']:.4f}, RMSE: {quality_metrics['rmse']:.4f}\")\n",
    "    print(f\"Authenticity Score - Spearman ρ: {auth_metrics['spearman']:.4f} (p-value: {auth_metrics['p_value']:.4g}), R²: {auth_metrics['r2']:.4f}, RMSE: {auth_metrics['rmse']:.4f}\")\n",
    "    print(f\"Average Score - Spearman ρ: {avg_metrics['spearman']:.4f} (p-value: {avg_metrics['p_value']:.4g}), R²: {avg_metrics['r2']:.4f}, RMSE: {avg_metrics['rmse']:.4f}\")\n",
    "\n",
    "def plot_correlations(model, dataloader, device, save_path=None, title=\"\"):\n",
    "    \"\"\"\n",
    "    Wrapper function that computes metrics and visualizes correlations.\n",
    "    Maintains backward compatibility with the original function.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained model to evaluate\n",
    "        dataloader (DataLoader): Test dataloader containing images and true scores\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu')\n",
    "        save_path (str, optional): Path to save the plot. If None, plot is displayed instead.\n",
    "        title (str): Title for the plot\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing correlation metrics\n",
    "    \"\"\"\n",
    "    # Compute metrics\n",
    "    results = compute_metrics(model, dataloader, device)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_correlations(results, save_path, title)\n",
    "    \n",
    "    # Return metrics for backward compatibility\n",
    "    return results['metrics']\n",
    "\n",
    "def true_vs_error_plot(results, save_path=None, title=\"\"):\n",
    "    \"\"\"\n",
    "    Create a plot of the true scores vs. the prediction errors (residuals) for the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained model to evaluate\n",
    "        dataloader (DataLoader): Test dataloader containing images and true scores\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu')\n",
    "        save_path (str, optional): Path to save the plot. If None, plot is displayed instead.\n",
    "        title (str): Title for the plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract residuals\n",
    "    quality_residuals = results['residuals']['quality']\n",
    "    auth_residuals = results['residuals']['authenticity']\n",
    "    avg_residuals = results['residuals']['average']\n",
    "    \n",
    "    # Create a figure with 3 subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    # Plot residuals\n",
    "    axes[0].scatter(results['ground_truth']['quality'], quality_residuals, alpha=0.7, color='blue')\n",
    "    axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0].set_xlabel('Quality True Value')\n",
    "    axes[0].set_ylabel('Quality Residual (Pred - True)')\n",
    "    axes[0].set_title('Quality Residuals vs True')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    axes[1].scatter(results['ground_truth']['authenticity'], auth_residuals, alpha=0.7, color='blue')\n",
    "    axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[1].set_xlabel('Authenticity True Value')\n",
    "    axes[1].set_ylabel('Authenticity Residual (Pred - True)')\n",
    "    axes[1].set_title('Authenticity Residuals vs True')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    axes[2].scatter(results['ground_truth']['average'], avg_residuals, alpha=0.7, color='blue')\n",
    "    axes[2].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[2].set_xlabel('Average True Value')\n",
    "    axes[2].set_ylabel('Average Residual (Pred - True)')\n",
    "    axes[2].set_title('Average Residuals vs True')\n",
    "    axes[2].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def pred_vs_error_plot(results, save_path=None, title=\"\"):\n",
    "    \"\"\"\n",
    "    Create a plot of the true scores vs. the prediction errors (residuals) for the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained model to evaluate\n",
    "        dataloader (DataLoader): Test dataloader containing images and true scores\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu')\n",
    "        save_path (str, optional): Path to save the plot. If None, plot is displayed instead.\n",
    "        title (str): Title for the plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract residuals\n",
    "    quality_residuals = results['residuals']['quality']\n",
    "    auth_residuals = results['residuals']['authenticity']\n",
    "    avg_residuals = results['residuals']['average']\n",
    "    \n",
    "    # Create a figure with 3 subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    # Plot residuals\n",
    "    axes[0].scatter(results['predictions']['quality'], quality_residuals, alpha=0.7, color='blue')\n",
    "    axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0].set_xlabel('Quality Predicted Value')\n",
    "    axes[0].set_ylabel('Quality Residual (Pred - True)')\n",
    "    axes[0].set_title('Quality Residuals vs Predicted')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    axes[1].scatter(results['predictions']['authenticity'], auth_residuals, alpha=0.7, color='blue')\n",
    "    axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[1].set_xlabel('Authenticity Predicted Value')\n",
    "    axes[1].set_ylabel('Authenticity Residual (Pred - True)')\n",
    "    axes[1].set_title('Authenticity Residuals vs Predicted')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    axes[2].scatter(results['predictions']['average'], avg_residuals, alpha=0.7, color='blue')\n",
    "    axes[2].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[2].set_xlabel('Average Predicted Value')\n",
    "    axes[2].set_ylabel('Average Residual (Pred - True)')\n",
    "    axes[2].set_title('Average Residuals vs Predicted')\n",
    "    axes[2].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "BASELINE_PATH_NAME = 'Plots/baseline_correlation.png'\n",
    "NOISY_PATH_NAME = 'Plots/noisy_pruned_correlation.png'\n",
    "NEGATIVE_PATH_NAME = 'Plots/negative_impact_correlation.png'\n",
    "\n",
    "baseline_results = compute_metrics(baseline_model, test_dataloader, device)\n",
    "noisy_results = compute_metrics(noisy_pruned_model, test_dataloader, device)\n",
    "negative_results = compute_metrics(negative_impact_pruned_model, test_dataloader, device)\n",
    "\n",
    "true_vs_error_plot(baseline_results,save_path=\"Plots/baseline_true_vs_error.png\", title=\"BASELINE MODEL\")\n",
    "true_vs_error_plot(noisy_results, save_path=\"Plots/noisy_pruned_true_vs_error.png\", title=\"NOISY PRUNED MODEL\")\n",
    "true_vs_error_plot(negative_results, save_path=\"Plots/best_subset_true_vs_error.png\", title=\"BEST SUBSET PRUNED MODEL\")\n",
    "\n",
    "pred_vs_error_plot(baseline_results,save_path=\"Plots/baseline_pred_vs_error.png\", title=\"BASELINE MODEL\")\n",
    "pred_vs_error_plot(noisy_results, save_path=\"Plots/noisy_pruned_pred_vs_error.png\", title=\"NOISY PRUNED MODEL\")\n",
    "pred_vs_error_plot(negative_results, save_path=\"Plots/best_subset_pred_vs_error.png\", title=\"BEST SUBSET PRUNED MODEL\")\n",
    "\n",
    "# For direct use with the original interface (backward compatibility):\n",
    "plot_correlations(baseline_model, test_dataloader, device, save_path=BASELINE_PATH_NAME, title=\"BASELINE MODEL\")\n",
    "plot_correlations(noisy_pruned_model, test_dataloader, device, save_path=NOISY_PATH_NAME, title=\"NOISY PRUNED MODEL\")\n",
    "plot_correlations(negative_impact_pruned_model, test_dataloader, device, save_path=NEGATIVE_PATH_NAME, title=\"NEGATIVE IMPACT PRUNED MODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test RSA on the models (baseline and pruned models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the models for RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_MODEL_NAME = 'Weights/VGG-16_finetuned_regression.pth'\n",
    "NOISY_PRUNED_MODEL_PATH = 'Weights/noise_out_pruned_model.pth'\n",
    "NEGATIVE_IMPACT_PRUNED_MODEL_PATH = 'Weights/negative_impact_pruned_model.pth'\n",
    "\n",
    "baseline_model = QualityPredictor()\n",
    "baseline_model.load_state_dict(torch.load(BASELINE_MODEL_NAME, weights_only=True))\n",
    "\n",
    "noisy_pruned_model = QualityPredictor()\n",
    "noisy_pruned_model.load_state_dict(torch.load(NOISY_PRUNED_MODEL_PATH, weights_only=True))\n",
    "\n",
    "negative_impact_pruned_model = QualityPredictor()\n",
    "negative_impact_pruned_model.load_state_dict(torch.load(NEGATIVE_IMPACT_PRUNED_MODEL_PATH,weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureMapHook:\n",
    "    \"\"\"Hook to extract feature maps from neural network layers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_maps = []\n",
    "    \n",
    "    def __call__(self, module, input, output):\n",
    "        # Detach from computation graph and move to CPU\n",
    "        self.feature_maps.append(output.detach().cpu())\n",
    "\n",
    "def get_feature_maps(model, dataloader, layer_name, device):\n",
    "    \"\"\"\n",
    "    Extracts the feature maps of a specific layer from a model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        dataloader (DataLoader): DataLoader for evaluation.\n",
    "        layer_name (str): The name of the layer to extract feature maps from.\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The feature maps as a numpy array with shape (240, num_features).\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Register a hook to extract feature maps\n",
    "    hook = FeatureMapHook()\n",
    "    target_layer = dict(model.named_modules())[layer_name]\n",
    "    hook_handle = target_layer.register_forward_hook(hook)\n",
    "    \n",
    "    # Forward pass to extract feature maps from the dataloader\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            model(inputs)\n",
    "\n",
    "    # Remove the hook\n",
    "    hook_handle.remove()\n",
    "    \n",
    "    # Process the feature maps to get the desired shape\n",
    "    all_features = []\n",
    "    \n",
    "    for batch_features in hook.feature_maps:\n",
    "        # Handle different possible output formats (accommodate different layer types)\n",
    "        if len(batch_features.shape) == 4:  # Conv layers: [batch_size, channels, height, width]\n",
    "            batch_size, channels, height, width = batch_features.shape\n",
    "            # Flatten spatial dimensions and create one feature vector per sample\n",
    "            batch_features = batch_features.reshape(batch_size, channels * height * width)\n",
    "        elif len(batch_features.shape) == 2:  # Linear layers: [batch_size, features]\n",
    "            pass  # Already in the right format\n",
    "        \n",
    "        # Add batch features to our collection\n",
    "        all_features.append(batch_features)\n",
    "    \n",
    "    # Concatenate all batches and convert to numpy\n",
    "    features_tensor = torch.cat(all_features, dim=0)\n",
    "    \n",
    "    # Ensure we have exactly the number of samples we expect in the dataloader \n",
    "    assert features_tensor.shape[0] == len(dataloader.dataset) \n",
    "    \n",
    "    # Convert to numpy array\n",
    "    features_array = features_tensor.numpy()\n",
    "    \n",
    "    return features_array\n",
    "def compute_similarity_matrix(features):\n",
    "    \"\"\"\n",
    "    Compute a similarity matrix from feature embeddings.\n",
    "    Works with both convolutional features (4D) and FC features (2D).\n",
    "    \n",
    "    Args:\n",
    "        features: numpy array - either shape (n_samples, n_channels, height, width)\n",
    "                 or shape (n_samples, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        similarity_matrix: numpy array of shape (n_samples, n_samples)\n",
    "    \"\"\"\n",
    "    # Check the dimensionality of features\n",
    "    n_samples = features.shape[0]\n",
    "    \n",
    "    # If features are from convolutional layer (4D), reshape to 2D\n",
    "    if len(features.shape) == 4:\n",
    "        features_flat = features.reshape(n_samples, -1)\n",
    "    else:\n",
    "        # Features are already 2D (from FC layer)\n",
    "        features_flat = features\n",
    "    \n",
    "    # Compute cosine similarity between all pairs\n",
    "    similarity_matrix = cosine_similarity(features_flat)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def compute_quality_difference_matrix(quality_scores):\n",
    "    \"\"\"\n",
    "    Compute a matrix of quality differences between all pairs of samples.\n",
    "    \n",
    "    Args:\n",
    "        quality_scores: numpy array of shape (n_samples,) containing quality scores\n",
    "        \n",
    "    Returns:\n",
    "        difference_matrix: numpy array of shape (n_samples, n_samples)\n",
    "    \"\"\"\n",
    "    n_samples = quality_scores.shape[0]\n",
    "    difference_matrix = np.zeros((n_samples, n_samples))\n",
    "    \n",
    "    # Compute absolute differences between all pairs\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            difference_matrix[i, j] = abs(quality_scores[i] - quality_scores[j])\n",
    "            \n",
    "    return difference_matrix\n",
    "\n",
    "def get_upper_triangle(matrix):\n",
    "    \"\"\"\n",
    "    Extract the upper triangle of a matrix (excluding diagonal).\n",
    "    \n",
    "    Args:\n",
    "        matrix: numpy array of shape (n, n)\n",
    "        \n",
    "    Returns:\n",
    "        upper_triangle: flattened upper triangle values\n",
    "    \"\"\"\n",
    "    indices = np.triu_indices_from(matrix, k=1)\n",
    "    return matrix[indices]\n",
    "\n",
    "def calculate_fit(similarity_matrix, quality_diff_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the fit between similarity and quality difference matrices.\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: numpy array of shape (n_samples, n_samples)\n",
    "        quality_diff_matrix: numpy array of shape (n_samples, n_samples)\n",
    "        \n",
    "    Returns:\n",
    "        correlation: Spearman correlation coefficient between the matrices\n",
    "        p_value: p-value of the correlation\n",
    "    \"\"\"\n",
    "    # Extract upper triangles (excluding diagonal)\n",
    "    sim_upper = get_upper_triangle(similarity_matrix)\n",
    "    qual_upper = get_upper_triangle(quality_diff_matrix)\n",
    "    \n",
    "    # Compute correlation (negative since higher similarity should correspond to lower difference)\n",
    "    correlation, p_value = spearmanr(sim_upper, qual_upper)\n",
    "    \n",
    "    # We're expecting a negative correlation (higher similarity → lower quality difference)\n",
    "    # so we return the negative correlation value for easier interpretation\n",
    "    return -correlation, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature maps\n",
    "baseline_features = get_feature_maps(baseline_model, test_dataloader, 'fc1.3', device)\n",
    "\n",
    "noisy_pruned_features = get_feature_maps(noisy_pruned_model, test_dataloader, 'fc1.3', device)\n",
    "\n",
    "negative_impact_features = get_feature_maps(negative_impact_pruned_model, test_dataloader, 'fc1.3', device)\n",
    "\n",
    "# Extract quality scores\n",
    "quality_scores_list = []\n",
    "with torch.no_grad():\n",
    "\tfor _, labels in test_dataloader:\n",
    "\t\tquality_scores_list.append(labels[:, 0])  # First column contains quality scores\n",
    "q_scores = torch.cat(quality_scores_list).numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrices\n",
    "baseline_similarity = compute_similarity_matrix(baseline_features)\n",
    "noisy_similarity = compute_similarity_matrix(noisy_pruned_features)\n",
    "negative_impact_similarity = compute_similarity_matrix(negative_impact_features)\n",
    "\n",
    "# Compute quality difference matrices\n",
    "quality_diff_matrix = compute_quality_difference_matrix(q_scores)\n",
    "\n",
    "# Calculate fit between similarity and quality difference matrices\n",
    "baseline_fit = calculate_fit(baseline_similarity, quality_diff_matrix)\n",
    "noisy_fit = calculate_fit(noisy_similarity, quality_diff_matrix)\n",
    "negative_impact_fit = calculate_fit(negative_impact_similarity, quality_diff_matrix)\n",
    "\n",
    "print(\"Baseline Model Fit:\")\n",
    "print(f\"Correlation: {baseline_fit[0]:.4f}\")\n",
    "print(\"------------------\")\n",
    "print(\"RSME Noise-out Pruned Model Fit:\")\n",
    "print(f\"Correlation: {noisy_fit[0]:.4f}\")\n",
    "print(\"------------------\")\n",
    "print(\"RSME Negative Impact Pruned Model Fit:\")\n",
    "print(f\"Correlation: {negative_impact_fit[0]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
