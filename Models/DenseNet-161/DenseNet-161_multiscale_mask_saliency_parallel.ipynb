{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2  # OpenCV for image processing\n",
    "import itertools # Added for saliency calculation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import math # For math.ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.models import densenet161, DenseNet161_Weights\n",
    "\n",
    "\n",
    "from torchvision import models, transforms\n",
    "import torchvision.transforms.functional as F # For potential use in visualization (though visualize_and_save_saliency uses different method)\n",
    "\n",
    "# Use tqdm.auto for better console/notebook detection and nesting\n",
    "from tqdm.auto import tqdm\n",
    "import time # Optional: Can add timing info to postfix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Database creations using pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAuthenticityDataset(Dataset):\n",
    "    \"\"\"Dataset for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.dir_path = os.path.dirname(csv_file)  # Directory of the CSV file\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx,):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its labels by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (image, labels) where:\n",
    "                image (PIL.Image): The image.\n",
    "                labels (torch.Tensor): Tensor containing quality and authenticity scores.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path_relative = self.data.iloc[idx, 3]\n",
    "        base_dir = os.path.abspath(os.path.join(self.dir_path, '../../')) \n",
    "        img_name = os.path.join(base_dir, img_path_relative.replace(\"./\", \"\"))\n",
    "\n",
    "        if not os.path.exists(img_name):\n",
    "             print(f\"Warning: Image path {img_name} not found directly. Trying original relative path logic...\")\n",
    "             img_name_fallback = self.data.iloc[idx, 3].replace(\"./\", \"../../\") \n",
    "             # Corrected fallback construction:\n",
    "             img_name_fallback_abs = os.path.abspath(os.path.join(self.dir_path, img_name_fallback))\n",
    "\n",
    "             if os.path.exists(img_name_fallback_abs):\n",
    "                 img_name = img_name_fallback_abs\n",
    "             elif os.path.exists(img_name_fallback): # Check original fallback relative to CWD if absolute fails\n",
    "                 img_name = img_name_fallback\n",
    "             else:\n",
    "                  raise FileNotFoundError(f\"Could not find image file at primary path: {os.path.join(base_dir, img_path_relative.replace('./', ''))} or fallback attempts: {img_name_fallback_abs}, {img_name_fallback}\")\n",
    "\n",
    "\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        authenticity = self.data.iloc[idx, 1]  # Authenticity column\n",
    "        labels = torch.tensor([authenticity], dtype=torch.float)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Definitions of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthenticityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained DenseNet-161\n",
    "        densenet = densenet161(weights=DenseNet161_Weights.DEFAULT)\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in densenet.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Store the features\n",
    "        self.features = densenet.features\n",
    "        \n",
    "        # DenseNet already includes a ReLU and pooling after features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # DenseNet-161's output feature dimension is 2208 instead of 2048\n",
    "        self.regression_head = nn.Sequential(\n",
    "                nn.Linear(2208, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(512, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        predictions = self.regression_head(x)\n",
    "        return predictions, x  # Return predictions and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Setup section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "try:\n",
    "    annotations_file = '../../Dataset/AIGCIQA2023/real_images_annotations.csv'\n",
    "    if not os.path.exists(annotations_file):\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__)) \n",
    "        annotations_file = os.path.abspath(os.path.join(script_dir, '../../Dataset/AIGCIQA2023/real_images_annotations.csv'))\n",
    "        if not os.path.exists(annotations_file):\n",
    "            raise FileNotFoundError(f\"Annotations file not found at relative or script-based path.\")\n",
    "except NameError:\n",
    "     annotations_file = '../../Dataset/AIGCIQA2023/real_images_annotations.csv'\n",
    "     print(\"Warning: __file__ not defined (e.g., running in Jupyter). Assuming relative path for annotations file from CWD.\")\n",
    "     if not os.path.exists(annotations_file):\n",
    "        # Try one level up if in a common 'notebooks' or 'scripts' subdir\n",
    "        annotations_file_alt = '../Dataset/AIGCIQA2023/real_images_annotations.csv'\n",
    "        if os.path.exists(annotations_file_alt):\n",
    "            annotations_file = annotations_file_alt\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Annotations file not found at '{annotations_file}' or '{annotations_file_alt}'. Please provide absolute path if needed.\")\n",
    "\n",
    "\n",
    "print(f\"Loading annotations from: {annotations_file}\")\n",
    "\n",
    "dataset = ImageAuthenticityDataset(csv_file=annotations_file, transform=data_transforms)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "print(f\"Dataset size: {len(dataset)}. Splitting into Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "NUM_WORKERS = min(os.cpu_count(), 4) if os.cpu_count() else 4 # Safer NUM_WORKERS\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# extract images based on the indices in the test dataset\n",
    "test_indices = test_dataset.indices\n",
    "indices_to_extract = [1,3,33,50,82]\n",
    "\n",
    "# Create a subset of the test dataset with only the specified indices\n",
    "extracted_dataset = torch.utils.data.Subset(test_dataset, indices_to_extract)\n",
    "# Create a DataLoader for the extracted dataset\n",
    "test_dataloader = DataLoader(extracted_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Models loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_MODEL_PATH = 'Weights/DenseNet-161_real_authenticity_finetuned.pth'\n",
    "PRUNED_MODEL_PATH = 'Weights/real_authenticity_noise_out_pruned_model.pth'    \n",
    "\n",
    "baseline_model = AuthenticityPredictor(freeze_backbone=True)\n",
    "baseline_model.load_state_dict(torch.load(BASELINE_MODEL_PATH, map_location=device))\n",
    "baseline_model.eval().to(device)\n",
    "print(\"Baseline model loaded and set to evaluation mode.\")\n",
    "\n",
    "pruned_model = AuthenticityPredictor(freeze_backbone=True)\n",
    "pruned_model.load_state_dict(torch.load(PRUNED_MODEL_PATH, map_location=device))\n",
    "pruned_model.eval().to(device)\n",
    "print(\"Pruned model loaded and set to evaluation mode.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Functions definitions (Image Utils & Saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(img_size, center, sigma):\n",
    "    \"\"\"Generates a binary mask with a square of zeros centered at 'center' with size 'sigma x sigma'.\"\"\"\n",
    "    mask = torch.ones(1, 1, img_size[0], img_size[1], device=device) # Ensure mask is on the correct device\n",
    "    start_x = max(0, int(center[0] - sigma // 2))\n",
    "    end_x = min(img_size[1], int(center[0] + (sigma + 1) // 2))\n",
    "    start_y = max(0, int(center[1] - sigma // 2))\n",
    "    end_y = min(img_size[0], int(center[1] + (sigma + 1) // 2))\n",
    "    if start_y < end_y and start_x < end_x:\n",
    "        mask[:, :, start_y:end_y, start_x:end_x] = 0\n",
    "    return mask\n",
    "\n",
    "def calculate_saliency_map(model, image, original_score, sigma_list, mask_value=0.0, pixel_batch_size=32): # Added pixel_batch_size\n",
    "    \"\"\"\n",
    "    Calculates the multiscale saliency map using the occlusion method\n",
    "    by summing scores across scales and normalizing the result,\n",
    "    with batched processing for pixel occlusions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Ensure image is on the correct device and has a batch dimension\n",
    "    if image.dim() == 3: # (C, H, W)\n",
    "        img_tensor_base = image.unsqueeze(0).to(device) # (1, C, H, W)\n",
    "    elif image.dim() == 4 and image.shape[0] == 1: # (1, C, H, W)\n",
    "        img_tensor_base = image.to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Input image tensor has unexpected dimensions: {image.shape}, expected (C,H,W) or (1,C,H,W)\")\n",
    "\n",
    "    img_size = img_tensor_base.shape[2:] # H, W\n",
    "    saliency_map_final = torch.zeros(img_size, dtype=torch.float32, device=device)\n",
    "\n",
    "    print(f\"Calculating saliency for image size {img_size} using {len(sigma_list)} sigmas: {sigma_list} with pixel_batch_size: {pixel_batch_size}\")\n",
    "\n",
    "    outer_progress = tqdm(\n",
    "        enumerate(sigma_list),\n",
    "        total=len(sigma_list),\n",
    "        desc=\"Overall Sigmas \",\n",
    "        unit=\"sigma\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for i, sigma in outer_progress:\n",
    "        saliency_map_sigma = torch.zeros(img_size, dtype=torch.float32, device=device)\n",
    "        \n",
    "        all_pixel_coords = list(itertools.product(range(img_size[0]), range(img_size[1]))) # (y, x)\n",
    "        total_pixels = len(all_pixel_coords)\n",
    "        num_batches = math.ceil(total_pixels / pixel_batch_size)\n",
    "\n",
    "        inner_progress_bar = tqdm(\n",
    "            range(num_batches),\n",
    "            total=num_batches,\n",
    "            desc=f\"  Sigma {i+1}/{len(sigma_list)} (val={sigma: >3}) Batches\",\n",
    "            leave=False,\n",
    "            unit=\"batch\",\n",
    "            position=1, # Nested progress bar\n",
    "            mininterval=0.1\n",
    "        )\n",
    "        \n",
    "        processed_pixels_count = 0\n",
    "        for batch_idx in inner_progress_bar:\n",
    "            batch_start_idx = batch_idx * pixel_batch_size\n",
    "            batch_end_idx = min(total_pixels, (batch_idx + 1) * pixel_batch_size)\n",
    "            \n",
    "            current_coords_batch = all_pixel_coords[batch_start_idx:batch_end_idx]\n",
    "            actual_batch_size = len(current_coords_batch)\n",
    "\n",
    "            if actual_batch_size == 0:\n",
    "                continue\n",
    "\n",
    "            masked_images_list = []\n",
    "            for y_coord, x_coord in current_coords_batch:\n",
    "                # generate_mask expects center as (x, y)\n",
    "                mask = generate_mask(img_size, (x_coord, y_coord), sigma) \n",
    "                # img_tensor_base is (1, C, H, W), mask is (1, 1, H, W)\n",
    "                # Broadcasting applies mask correctly: (1,C,H,W) * (1,1,H,W) -> (1,C,H,W)\n",
    "                masked_image = img_tensor_base * mask + mask_value * (1 - mask) # Occlusion happens here\n",
    "                masked_images_list.append(masked_image) # Appending (1,C,H,W) tensors\n",
    "            \n",
    "            # Stack along a new batch dimension, result shape: (actual_batch_size, C, H, W)\n",
    "            batch_of_masked_images = torch.cat(masked_images_list, dim=0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_batch, _ = model(batch_of_masked_images) # Model returns (predictions, features)\n",
    "                # Ensure scores are flat (actual_batch_size,)\n",
    "                masked_scores_tensor_batch = output_batch.squeeze()\n",
    "                if masked_scores_tensor_batch.dim() == 0: # If only one item in batch and squeeze made it scalar\n",
    "                    masked_scores_tensor_batch = masked_scores_tensor_batch.unsqueeze(0)\n",
    "\n",
    "            for k in range(actual_batch_size):\n",
    "                y, x = current_coords_batch[k]\n",
    "                masked_score_item = masked_scores_tensor_batch[k].item()\n",
    "                saliency_value = original_score - masked_score_item # Higher score drop = more salient\n",
    "                saliency_map_sigma[y, x] = saliency_value\n",
    "            \n",
    "            processed_pixels_count += actual_batch_size\n",
    "            inner_progress_bar.set_postfix_str(\n",
    "                f\"Pixels {processed_pixels_count}/{total_pixels}\",\n",
    "                refresh=True \n",
    "            )\n",
    "\n",
    "        saliency_map_final += saliency_map_sigma\n",
    "        elapsed_time_sigma = inner_progress_bar.format_dict['elapsed'] # Get elapsed time from tqdm\n",
    "        tqdm.write(f\"  Sigma {sigma} finished processing in {elapsed_time_sigma:.2f}s.\")\n",
    "        inner_progress_bar.close()\n",
    "\n",
    "\n",
    "    min_val = torch.min(saliency_map_final)\n",
    "    max_val = torch.max(saliency_map_final)\n",
    "\n",
    "    if max_val > min_val:\n",
    "        saliency_map_normalized = (saliency_map_final - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        saliency_map_normalized = torch.zeros_like(saliency_map_final)\n",
    "        print(\"Warning: Final saliency map was constant before normalization. Result is zero map.\")\n",
    "    \n",
    "    outer_progress.close()\n",
    "    return saliency_map_normalized.cpu().numpy()\n",
    "\n",
    "def denormalize_image(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"Denormalizes an image tensor.\"\"\"\n",
    "    if tensor.dim() != 3:\n",
    "        raise ValueError(f\"Input tensor must have 3 dimensions (C, H, W), but got {tensor.dim()}\")\n",
    "    \n",
    "    mean_used = mean\n",
    "    std_used = std\n",
    "    if tensor.shape[0] != len(mean) or tensor.shape[0] != len(std):\n",
    "        if tensor.shape[0] == 1: # Grayscale\n",
    "             print(\"Warning: Denormalizing grayscale with potentially RGB stats. Using first value of mean/std.\")\n",
    "             mean_used = [mean[0]] if isinstance(mean, list) else [mean]\n",
    "             std_used = [std[0]] if isinstance(std, list) else [std]\n",
    "        else:\n",
    "            raise ValueError(f\"Channel mismatch: Tensor has {tensor.shape[0]} channels, mean has {len(mean)}, std has {len(std)}\")\n",
    "\n",
    "    mean_t = torch.as_tensor(mean_used, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
    "    std_t = torch.as_tensor(std_used, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
    "\n",
    "    denormalized_tensor = tensor * std_t + mean_t\n",
    "    return torch.clamp(denormalized_tensor, 0., 1.)\n",
    "\n",
    "def visualize_and_save_saliency(\n",
    "    image_tensor,\n",
    "    saliency_map,\n",
    "    output_dir,\n",
    "    filename_prefix,\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    "    overlay_alpha=0.5,\n",
    "    cmap_name='bwr'):\n",
    "    \"\"\"\n",
    "    Visualizes saliency map, creates an overlay, and saves images.\n",
    "    \"\"\"\n",
    "    if image_tensor.is_cuda:\n",
    "        # print(\"Warning: image_tensor provided to visualize_and_save_saliency is on CUDA, moving to CPU.\")\n",
    "        image_tensor = image_tensor.cpu()\n",
    "\n",
    "    # Create a sub-folder for each image's visualizations\n",
    "    image_specific_output_dir = os.path.join(output_dir, filename_prefix)\n",
    "    os.makedirs(image_specific_output_dir, exist_ok=True)\n",
    "    \n",
    "    temp_dir = os.path.join(image_specific_output_dir, 'temp_heatmap_cache') \n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    if saliency_map.ndim != 2:\n",
    "        print(f\"Error: Saliency map has unexpected dimensions {saliency_map.shape}. Expected (H, W). Skipping visualization.\")\n",
    "        return\n",
    "    saliency_map = np.clip(saliency_map, 0.0, 1.0)\n",
    "    \n",
    "    NUMPY_DIR = os.path.join(output_dir, 'numpy_saliency_maps') # Centralized numpy maps\n",
    "    os.makedirs(NUMPY_DIR, exist_ok=True)\n",
    "    np.save(os.path.join(NUMPY_DIR, f\"{filename_prefix}_saliency_map.npy\"), saliency_map)\n",
    "\n",
    "    try:\n",
    "        img_denorm_tensor = denormalize_image(image_tensor, mean, std)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during denormalization for {filename_prefix}: {e}. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    img_np = img_denorm_tensor.numpy().transpose(1, 2, 0)\n",
    "    img_np = np.clip(img_np, 0.0, 1.0)\n",
    "    img_uint8 = (img_np * 255).astype(np.uint8)\n",
    "\n",
    "    if img_uint8.shape[2] == 1:\n",
    "        img_display = cv2.cvtColor(img_uint8, cv2.COLOR_GRAY2RGB)\n",
    "        img_bgr = cv2.cvtColor(img_uint8, cv2.COLOR_GRAY2BGR)\n",
    "    elif img_uint8.shape[2] == 3:\n",
    "        img_display = img_uint8\n",
    "        img_bgr = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2BGR)\n",
    "    else:\n",
    "         print(f\"Error: Unexpected number of channels ({img_uint8.shape[2]}) for {filename_prefix}. Skipping visualization.\")\n",
    "         return\n",
    "\n",
    "    orig_save_path = os.path.join(image_specific_output_dir, f\"{filename_prefix}_original.png\")\n",
    "    plt.figure(figsize=(img_display.shape[1]/100, img_display.shape[0]/100), dpi=100) # Match size\n",
    "    plt.imshow(img_display)\n",
    "    plt.axis('off'); plt.title(\"Original Image\")\n",
    "    plt.savefig(orig_save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    try:\n",
    "        cmap = cm.get_cmap(cmap_name)\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Colormap '{cmap_name}' not found. Using default 'viridis'.\")\n",
    "        cmap = cm.get_cmap('viridis')\n",
    "    norm = colors.Normalize(vmin=0, vmax=1)\n",
    "\n",
    "    heatmap_save_path = os.path.join(image_specific_output_dir, f\"{filename_prefix}_heatmap_{cmap_name}.png\")\n",
    "    plt.figure(figsize=(saliency_map.shape[1]/100, saliency_map.shape[0]/100), dpi=100) # Match size\n",
    "    plt.imshow(saliency_map, cmap=cmap, norm=norm)\n",
    "    plt.colorbar(label=f'Saliency')\n",
    "    plt.title(f\"Saliency Heatmap ({cmap_name})\"); plt.axis('off')\n",
    "    plt.savefig(heatmap_save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    temp_heatmap_path = os.path.join(temp_dir, f\"{filename_prefix}_temp_heatmap_for_overlay.png\")\n",
    "    fig_width_inches = img_display.shape[1] / 100.0\n",
    "    fig_height_inches = img_display.shape[0] / 100.0\n",
    "    plt.figure(figsize=(fig_width_inches, fig_height_inches), dpi=100)\n",
    "    plt.imshow(saliency_map, cmap=cmap, norm=norm); plt.axis('off')\n",
    "    plt.savefig(temp_heatmap_path, bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "    colored_heatmap_bgr = cv2.imread(temp_heatmap_path)\n",
    "    if os.path.exists(temp_heatmap_path): os.remove(temp_heatmap_path)\n",
    "    if os.path.exists(temp_dir) and not os.listdir(temp_dir): \n",
    "        try: os.rmdir(temp_dir)\n",
    "        except OSError: pass # Might fail if another process/thread is accessing\n",
    "\n",
    "    if colored_heatmap_bgr is None:\n",
    "        print(f\"Error: Could not read temporary heatmap file for {filename_prefix}: {temp_heatmap_path}. Skipping overlay.\")\n",
    "        return\n",
    "\n",
    "    if colored_heatmap_bgr.shape[:2] != img_bgr.shape[:2]:\n",
    "         # print(f\"Warning: Resizing heatmap from {colored_heatmap_bgr.shape[:2]} to {img_bgr.shape[:2]} for {filename_prefix}\")\n",
    "         colored_heatmap_bgr = cv2.resize(colored_heatmap_bgr, (img_bgr.shape[1], img_bgr.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    overlay = cv2.addWeighted(img_bgr, 1.0 - overlay_alpha, colored_heatmap_bgr, overlay_alpha, 0.0)\n",
    "    overlay_rgb = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    overlay_save_path = os.path.join(image_specific_output_dir, f\"{filename_prefix}_overlay_{cmap_name}.png\")\n",
    "    success = cv2.imwrite(overlay_save_path, cv2.cvtColor(overlay_rgb, cv2.COLOR_RGB2BGR))\n",
    "    if not success:\n",
    "        print(f\"Error: cv2.imwrite failed for overlay {overlay_save_path}. Trying plt.savefig.\")\n",
    "        plt.figure(figsize=(overlay_rgb.shape[1]/100, overlay_rgb.shape[0]/100), dpi=100)\n",
    "        plt.imshow(overlay_rgb); plt.axis('off'); plt.title(f\"Saliency Overlay ({cmap_name})\")\n",
    "        plt.savefig(overlay_save_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "def run_saliency_analysis(\n",
    "    model,\n",
    "    dataloader,\n",
    "    output_dir,\n",
    "    num_images_to_process,\n",
    "    sigma_list,\n",
    "    pixel_batch_size, # Added\n",
    "    mask_value=0.0,\n",
    "    vis_cmap='bwr',\n",
    "    vis_alpha=0.6,\n",
    "    device='cpu', \n",
    "    model_name=\"Model\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Facade function to run saliency map generation (with batched pixel processing)\n",
    "    and visualization for a given model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Saliency Analysis for {model_name} ---\")\n",
    "    # Central output directory for this model run\n",
    "    model_output_dir = os.path.join(output_dir, model_name.replace(\" \", \"_\"))\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    print(f\"Output visualizations will be saved in subfolders within: {model_output_dir}\")\n",
    "    print(f\"Processing up to {num_images_to_process} images.\")\n",
    "    print(f\"Using sigmas: {sigma_list}, Pixel batch size: {pixel_batch_size}\")\n",
    "\n",
    "    processed_count = 0\n",
    "    model.eval()\n",
    "\n",
    "    # Determine the actual number of images to process\n",
    "    num_to_iterate = min(num_images_to_process, len(dataloader.dataset) if BATCH_SIZE == 1 else len(dataloader))\n",
    "\n",
    "\n",
    "    test_iterator = tqdm(\n",
    "        dataloader,\n",
    "        total=num_to_iterate,\n",
    "        desc=f\"{model_name} Image Progress\"\n",
    "    )\n",
    "\n",
    "    for i, (images, labels) in enumerate(test_iterator):\n",
    "        if processed_count >= num_images_to_process:\n",
    "            print(f\"\\nReached limit of {num_images_to_process} images for {model_name}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        image_tensor = images[0].to(device) # (C, H, W) as BATCH_SIZE=1\n",
    "        label = labels[0]                   # scalar tensor\n",
    "\n",
    "        print(f\"\\nProcessing image {processed_count + 1}/{num_images_to_process} (DataLoader index: {i}) for {model_name}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            original_output, _ = model(image_tensor.unsqueeze(0)) # Model expects batch\n",
    "            original_score = original_output.item()\n",
    "\n",
    "        true_authenticity = label.item()\n",
    "        print(f\"  True Authenticity: {true_authenticity:.4f}\")\n",
    "        print(f\"  {model_name} Predicted Authenticity (Original): {original_score:.4f}\")\n",
    "\n",
    "        if not sigma_list:\n",
    "             print(\"  Warning: No sigma values provided. Skipping saliency calculation.\")\n",
    "             processed_count += 1\n",
    "             test_iterator.set_postfix_str(f\"Image {processed_count}/{num_images_to_process} (Skipped)\")\n",
    "             continue\n",
    "        \n",
    "        print(f\"  Calculating saliency map...\")\n",
    "        saliency_map_np = calculate_saliency_map( # Calls the modified function\n",
    "            model=model,\n",
    "            image=image_tensor, # Pass (C,H,W) tensor\n",
    "            original_score=original_score,\n",
    "            sigma_list=sigma_list,\n",
    "            mask_value=mask_value,\n",
    "            pixel_batch_size=pixel_batch_size # Pass new arg\n",
    "        )\n",
    "        print(f\"  Saliency map calculated with shape: {saliency_map_np.shape}\")\n",
    "\n",
    "        filename_prefix = f\"img_{processed_count:03d}_auth_{true_authenticity:.2f}_pred_{original_score:.2f}\"\n",
    "        print(f\"  Visualizing and saving results with prefix: {filename_prefix}...\")\n",
    "        \n",
    "        # Pass the model_output_dir for this specific model run\n",
    "        visualize_and_save_saliency(\n",
    "            image_tensor=image_tensor.cpu(), # Ensure tensor is on CPU for visualization\n",
    "            saliency_map=saliency_map_np,\n",
    "            output_dir=model_output_dir, # Pass the specific dir for this model\n",
    "            filename_prefix=filename_prefix,\n",
    "            overlay_alpha=vis_alpha,\n",
    "            cmap_name=vis_cmap\n",
    "        )\n",
    "        print(f\"  Visualization saved for {filename_prefix}.\")\n",
    "\n",
    "        processed_count += 1\n",
    "        test_iterator.set_postfix_str(f\"Image {processed_count}/{num_images_to_process} Done\")\n",
    "\n",
    "    test_iterator.close()\n",
    "    print(f\"\\n--- Saliency Analysis for {model_name} Finished ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Main Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "NUM_IMAGES_TO_PROCESS = 5 # Reduced for quicker testing, adjust as needed\n",
    "SIGMA_LIST = [3, 5, 9, 17, 33, 65] # Fixed list\n",
    "MASK_VALUE = 0.0 \n",
    "VIS_CMAP = 'bwr' \n",
    "VIS_ALPHA = 0.6 \n",
    "PIXEL_BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "# Define a main output directory\n",
    "MAIN_OUTPUT_DIR = '5_imgs_masking_experiment_outputs'\n",
    "os.makedirs(MAIN_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Run for Baseline Model\n",
    "run_saliency_analysis(\n",
    "    model=baseline_model,\n",
    "    dataloader=test_dataloader,\n",
    "    output_dir=MAIN_OUTPUT_DIR, # Pass the main output directory\n",
    "    num_images_to_process=NUM_IMAGES_TO_PROCESS,\n",
    "    sigma_list=SIGMA_LIST,\n",
    "    pixel_batch_size=PIXEL_BATCH_SIZE, \n",
    "    mask_value=MASK_VALUE,\n",
    "    vis_cmap=VIS_CMAP,\n",
    "    vis_alpha=VIS_ALPHA,\n",
    "    device=device,\n",
    "    model_name=\"Baseline_Model\" \n",
    ")\n",
    "\n",
    "# Run for Pruned Model\n",
    "run_saliency_analysis(\n",
    "    model=pruned_model,\n",
    "    dataloader=test_dataloader,\n",
    "    output_dir=MAIN_OUTPUT_DIR, # Pass the main output directory\n",
    "    num_images_to_process=NUM_IMAGES_TO_PROCESS,\n",
    "    sigma_list=SIGMA_LIST,\n",
    "    pixel_batch_size=PIXEL_BATCH_SIZE,\n",
    "    mask_value=MASK_VALUE,\n",
    "    vis_cmap=VIS_CMAP,\n",
    "    vis_alpha=VIS_ALPHA,\n",
    "    device=device,\n",
    "    model_name=\"Pruned_Model\" \n",
    ")\n",
    "\n",
    "print(f\"\\n--- All Saliency Analyses Completed. Outputs in '{MAIN_OUTPUT_DIR}' ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
