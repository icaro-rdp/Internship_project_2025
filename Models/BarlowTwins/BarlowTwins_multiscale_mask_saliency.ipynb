{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2  # OpenCV for image processing\n",
    "import itertools # Added for saliency calculation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.hub\n",
    "# import torch.nn.functional as F # Already imported via torchvision below\n",
    "\n",
    "from torchvision import models, transforms\n",
    "import torchvision.transforms.functional as F # For potential use in visualization (though visualize_and_save_saliency uses different method)\n",
    "\n",
    "# Use tqdm.auto for better console/notebook detection and nesting\n",
    "from tqdm.auto import tqdm\n",
    "import time # Optional: Can add timing info to postfix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Database creations using pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAuthenticityDataset(Dataset):\n",
    "    \"\"\"Dataset for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.dir_path = os.path.dirname(csv_file)  # Directory of the CSV file\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx,):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its labels by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (image, labels) where:\n",
    "                image (PIL.Image): The image.\n",
    "                labels (torch.Tensor): Tensor containing quality and authenticity scores.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # TODO: to be fixed, right now is folder dependent\n",
    "        img_path_relative = self.data.iloc[idx, 3]\n",
    "        # Construct absolute path based on CSV location\n",
    "        base_dir = os.path.abspath(os.path.join(self.dir_path, '../../')) # Go up two levels from CSV dir\n",
    "        img_name = os.path.join(base_dir, img_path_relative.replace(\"./\", \"\")) # Combine and remove './'\n",
    "\n",
    "        # Ensure path exists before opening\n",
    "        if not os.path.exists(img_name):\n",
    "             # Fallback or error handling if path logic is complex\n",
    "             print(f\"Warning: Image path {img_name} not found directly. Trying original relative path logic...\")\n",
    "             img_name = self.data.iloc[idx, 3].replace(\"./\", \"../../\") # Original logic as fallback\n",
    "             if not os.path.exists(img_name):\n",
    "                  raise FileNotFoundError(f\"Could not find image file at primary path: {os.path.join(base_dir, img_path_relative.replace('./', ''))} or fallback: {img_name}\")\n",
    "\n",
    "\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        authenticity = self.data.iloc[idx, 1]  # Authenticity column\n",
    "        labels = torch.tensor([authenticity], dtype=torch.float)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Definitions of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthenticityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained BarlowTwins ResNet50 instead of ResNet-152\n",
    "        barlow_twins_resnet = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')\n",
    "\n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in barlow_twins_resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Model backbone frozen.\")\n",
    "        else:\n",
    "            print(\"Model backbone NOT frozen (trainable).\")\n",
    "\n",
    "\n",
    "        self.features = nn.Sequential(*list(barlow_twins_resnet.children())[:-2])\n",
    "        self.avgpool = barlow_twins_resnet.avgpool\n",
    "\n",
    "\n",
    "        self.regression_head = nn.Sequential(\n",
    "                nn.Linear(2048, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(512, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        predictions = self.regression_head(x)\n",
    "        # Return both predictions and flattened features (useful for some saliency methods)\n",
    "        return predictions, x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Setup section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading annotations from: ../../Dataset/AIGCIQA2023/real_images_annotations.csv\n",
      "Dataset size: 1368. Splitting into Train: 957, Val: 273, Test: 138\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data transformations for the ImageNet dataset\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define path relative to the script location or use an absolute path\n",
    "# Assuming the script is run from a location where '../../Dataset/...' is valid\n",
    "try:\n",
    "    # Try relative path first\n",
    "    annotations_file = '../../Dataset/AIGCIQA2023/real_images_annotations.csv'\n",
    "    # Check if the file exists using the relative path\n",
    "    if not os.path.exists(annotations_file):\n",
    "        # If relative path fails, try constructing from script directory\n",
    "        script_dir = os.path.dirname(__file__) # Get directory of the script\n",
    "        annotations_file = os.path.abspath(os.path.join(script_dir, '../../Dataset/AIGCIQA2023/real_images_annotations.csv'))\n",
    "        if not os.path.exists(annotations_file):\n",
    "            raise FileNotFoundError(f\"Annotations file not found at relative or script-based path: {annotations_file}\")\n",
    "except NameError:\n",
    "     # __file__ is not defined (e.g., running in interactive environment like Jupyter)\n",
    "     # Fallback to assuming relative path from CWD or specify absolute path directly\n",
    "     annotations_file = '../../Dataset/AIGCIQA2023/real_images_annotations.csv' # Or provide absolute path\n",
    "     print(\"Warning: __file__ not defined. Assuming relative path for annotations file.\")\n",
    "     if not os.path.exists(annotations_file):\n",
    "        raise FileNotFoundError(f\"Annotations file not found at relative path: {annotations_file}. Please provide absolute path if needed.\")\n",
    "\n",
    "\n",
    "print(f\"Loading annotations from: {annotations_file}\")\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageAuthenticityDataset(csv_file=annotations_file, transform=data_transforms)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42) # Use manual_seed_all for multi-GPU setups if relevant\n",
    "np.random.seed(42)\n",
    "# Potentially add for DataLoader determinism (might impact performance)\n",
    "# def seed_worker(worker_id):\n",
    "#     worker_seed = torch.initial_seed() % 2**32\n",
    "#     numpy.random.seed(worker_seed)\n",
    "#     random.seed(worker_seed)\n",
    "# g = torch.Generator()\n",
    "# g.manual_seed(42)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "print(f\"Dataset size: {len(dataset)}. Splitting into Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)) # Add generator for split reproducibility\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 1 # Set batch size to 1 for easier processing of individual images for saliency\n",
    "NUM_WORKERS = 4 # Adjust based on your system\n",
    "# Consider adding pin_memory=True if using GPU for potentially faster data transfer\n",
    "# Create only the data loader for test\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)#, worker_init_fn=seed_worker, generator=g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Models loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: __file__ not defined. Assuming relative path for model weights.\n",
      "Loading baseline model weights from: Weights/BarlowTwins_real_authenticity_finetuned.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/icaro.redepaolini@unitn.it/.cache/torch/hub/facebookresearch_barlowtwins_main\n",
      "/home/icaro.redepaolini@unitn.it/.conda/envs/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/icaro.redepaolini@unitn.it/.conda/envs/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model backbone frozen.\n",
      "Baseline model loaded and set to evaluation mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3879636/2297781723.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  baseline_model.load_state_dict(torch.load(BASELINE_MODEL_PATH, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# Define path relative to script or use absolute path\n",
    "try:\n",
    "    script_dir = os.path.dirname(__file__)\n",
    "    BASELINE_MODEL_PATH = os.path.abspath(os.path.join(script_dir, 'Weights/BarlowTwins_real_authenticity_finetuned.pth'))\n",
    "except NameError:\n",
    "    # Fallback for interactive environments\n",
    "    BASELINE_MODEL_PATH = 'Weights/BarlowTwins_real_authenticity_finetuned.pth'\n",
    "    print(\"Warning: __file__ not defined. Assuming relative path for model weights.\")\n",
    "\n",
    "print(f\"Loading baseline model weights from: {BASELINE_MODEL_PATH}\")\n",
    "if not os.path.exists(BASELINE_MODEL_PATH):\n",
    "     raise FileNotFoundError(f\"Model weights file not found at {BASELINE_MODEL_PATH}\")\n",
    "\n",
    "# Instantiate model with frozen backbone by default\n",
    "baseline_model = AuthenticityPredictor(freeze_backbone=True)\n",
    "# Load weights - ensure map_location handles CPU/GPU loading correctly\n",
    "baseline_model.load_state_dict(torch.load(BASELINE_MODEL_PATH, map_location=device))\n",
    "baseline_model.eval().to(device) # Set to evaluation mode and move to device\n",
    "print(\"Baseline model loaded and set to evaluation mode.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Functions definitions (Image Utils & Saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sigma_list_from_image(image_tensor, percentages=[0.05, 0.10, 0.20], min_sigma=3):\n",
    "    \"\"\"\n",
    "    Generates a list of sigma values based on image dimensions.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): The input image tensor (e.g., shape [C, H, W]).\n",
    "        percentages (list, optional): List of percentages of the smaller dimension\n",
    "                                      to use for sigma values. Defaults to [0.05, 0.10, 0.20].\n",
    "        min_sigma (int, optional): The minimum allowed sigma value. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of unique integer sigma values.\n",
    "    \"\"\"\n",
    "    if image_tensor.dim() < 2:\n",
    "        raise ValueError(\"image_tensor must have at least 2 dimensions (H, W)\")\n",
    "\n",
    "    # Get height and width (assuming shape [..., H, W])\n",
    "    img_height = image_tensor.shape[-2]\n",
    "    img_width = image_tensor.shape[-1]\n",
    "    min_dim = min(img_height, img_width)\n",
    "\n",
    "    print(f\"Image dimensions: H={img_height}, W={img_width}. Using min_dim={min_dim} for sigma calculation.\")\n",
    "\n",
    "    sigma_list_calculated = []\n",
    "    for p in percentages:\n",
    "        # Calculate sigma based on percentage\n",
    "        sigma = int(min_dim * p) # Truncate to integer\n",
    "\n",
    "        # Ensure sigma is at least min_sigma\n",
    "        sigma = max(min_sigma, sigma)\n",
    "\n",
    "        # Ensure sigma is odd (can help with centering)\n",
    "        if sigma % 2 == 0:\n",
    "            sigma += 1\n",
    "\n",
    "        # Ensure sigma doesn't exceed image dimensions (unlikely if based on min_dim, but safe check)\n",
    "        sigma = min(sigma, min_dim)\n",
    "\n",
    "        sigma_list_calculated.append(sigma)\n",
    "\n",
    "    # Remove duplicates and sort the list\n",
    "    sigma_list_final = sorted(list(set(sigma_list_calculated)))\n",
    "\n",
    "    print(f\"Generated sigma list: {sigma_list_final}\")\n",
    "    return sigma_list_final\n",
    "\n",
    "def generate_mask(img_size, center, sigma):\n",
    "    \"\"\"Generates a binary mask with a square of zeros centered at 'center' with size 'sigma x sigma'.\"\"\"\n",
    "    mask = torch.ones(1, 1, img_size[0], img_size[1], device=device)\n",
    "    start_x = max(0, int(center[0] - sigma // 2))\n",
    "    end_x = min(img_size[1], int(center[0] + (sigma + 1) // 2))\n",
    "    start_y = max(0, int(center[1] - sigma // 2))\n",
    "    end_y = min(img_size[0], int(center[1] + (sigma + 1) // 2))\n",
    "    if start_y < end_y and start_x < end_x:\n",
    "        mask[:, :, start_y:end_y, start_x:end_x] = 0\n",
    "    return mask\n",
    "\n",
    "def calculate_saliency_map(model, image, original_score, sigma_list, mask_value=0.0):\n",
    "    \"\"\"\n",
    "    Calculates the multiscale saliency map using the occlusion method\n",
    "    by summing scores across scales and normalizing the result,\n",
    "    with nested progress bars and showing current pixel impact in the postfix.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to use for inference. Must be on the correct device.\n",
    "        image (torch.Tensor): The input image tensor (C, H, W), must be on the correct device.\n",
    "        original_score (float): The model's score for the original, unoccluded image.\n",
    "        sigma_list (list): List of integers representing the sizes (side length) of the occlusion squares.\n",
    "        mask_value (float, optional): Value to use for occluded regions. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A normalized saliency map (H, W) as a NumPy array on the CPU.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Ensure image is on the correct device and add batch dimension\n",
    "    img_tensor = image.unsqueeze(0).to(device)\n",
    "    img_size = img_tensor.shape[2:] # H, W\n",
    "    # Initialize the final map to store the sum of per-scale saliencies\n",
    "    saliency_map_final = torch.zeros(img_size, dtype=torch.float32, device=device)\n",
    "\n",
    "    print(f\"Calculating saliency for image size {img_size} using {len(sigma_list)} sigmas: {sigma_list}\")\n",
    "\n",
    "    # --- Outer Progress Bar for Sigmas ---\n",
    "    outer_progress = tqdm(\n",
    "        enumerate(sigma_list),\n",
    "        total=len(sigma_list),\n",
    "        desc=\"Overall Sigmas \", # Add space for better alignment\n",
    "        unit=\"sigma\",\n",
    "        position=0,\n",
    "        leave=True # Keep after finishing\n",
    "    )\n",
    "\n",
    "    for i, sigma in outer_progress:\n",
    "        # Temporary map for the current sigma scale\n",
    "        saliency_map_sigma = torch.zeros(img_size, dtype=torch.float32, device=device)\n",
    "\n",
    "        # --- Inner Progress Bar for Pixels ---\n",
    "        pixel_iterator = itertools.product(range(img_size[0]), range(img_size[1]))\n",
    "        total_pixels = img_size[0] * img_size[1]\n",
    "        inner_progress_bar = tqdm(\n",
    "            pixel_iterator,\n",
    "            total=total_pixels,\n",
    "            desc=f\"  Sigma {i+1}/{len(sigma_list)} (val={sigma: >3}) Pixels\",\n",
    "            leave=False, # Remove after each sigma finishes\n",
    "            unit=\"pixel\",\n",
    "            \n",
    "            position=1,\n",
    "            mininterval=0.1 # Refresh rate throttle (optional)\n",
    "        )\n",
    "\n",
    "        start_time = time.time() # For calculating rate\n",
    "        for y, x in inner_progress_bar:\n",
    "            # Generate mask for the current pixel and sigma\n",
    "            mask = generate_mask(img_size, (x, y), sigma)\n",
    "            # Apply mask\n",
    "            masked_image = img_tensor * mask + mask_value * (1 - mask)\n",
    "\n",
    "            # Get model prediction for the masked image\n",
    "            with torch.no_grad():\n",
    "                output = model(masked_image)\n",
    "                # Handle cases where model returns multiple outputs (e.g., prediction, features)\n",
    "                if isinstance(output, tuple) and len(output) > 0:\n",
    "                    masked_score_tensor = output[0]\n",
    "                else:\n",
    "                    masked_score_tensor = output\n",
    "                # Ensure score is a scalar on CPU\n",
    "                masked_score_item = masked_score_tensor.detach().cpu().item()\n",
    "\n",
    "            # Calculate saliency value for this pixel and sigma\n",
    "            saliency_value = original_score - masked_score_item\n",
    "            saliency_map_sigma[y, x] = saliency_value\n",
    "\n",
    "            # --- Update Postfix with Current Pixel Info ---\n",
    "            inner_progress_bar.set_postfix(\n",
    "                pixel=f\"({y},{x})\",\n",
    "                impact=f\"{saliency_value:.4f}\", # Format saliency value\n",
    "                refresh=False # Update display on tqdm's schedule\n",
    "            )\n",
    "            # -----------------------------------------------\n",
    "\n",
    "        saliency_map_final += saliency_map_sigma\n",
    "        # ---------------------------------------------------------------\n",
    "\n",
    "        # Optional: Print time taken per sigma\n",
    "        elapsed_time = time.time() - start_time\n",
    "        pixels_per_sec = total_pixels / elapsed_time if elapsed_time > 0 else float('inf')\n",
    "        tqdm.write(f\"  Sigma {sigma} finished in {elapsed_time:.2f}s ({pixels_per_sec:.1f} pixels/sec)\")\n",
    "\n",
    "    # --- Normalization ---\n",
    "    # Normalize the *summed* final map\n",
    "    min_val = torch.min(saliency_map_final)\n",
    "    max_val = torch.max(saliency_map_final)\n",
    "\n",
    "    if max_val > min_val:\n",
    "        # Perform min-max normalization to range [0, 1]\n",
    "        saliency_map_normalized = (saliency_map_final - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        # Handle the case where the map is constant (all saliency values were the same)\n",
    "        saliency_map_normalized = torch.zeros_like(saliency_map_final)\n",
    "        print(\"Warning: Final saliency map was constant before normalization. Result is zero map.\")\n",
    "\n",
    "    # Return the normalized map as a NumPy array on the CPU\n",
    "    return saliency_map_normalized.cpu().numpy()\n",
    "def denormalize_image(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"Denormalizes an image tensor.\"\"\"\n",
    "    if tensor.dim() != 3:\n",
    "        raise ValueError(f\"Input tensor must have 3 dimensions (C, H, W), but got {tensor.dim()}\")\n",
    "    if tensor.shape[0] != len(mean) or tensor.shape[0] != len(std):\n",
    "         # Handle grayscale - assume mean/std are single values or adaptable\n",
    "        if tensor.shape[0] == 1 and len(mean) == 3 and len(std) == 3:\n",
    "             print(\"Warning: Denormalizing grayscale with potentially RGB stats. Using first value.\")\n",
    "             mean_used = [mean[0]]\n",
    "             std_used = [std[0]]\n",
    "        elif tensor.shape[0] == 1 and isinstance(mean, (int, float)) and isinstance(std, (int, float)):\n",
    "             mean_used = [mean]\n",
    "             std_used = [std]\n",
    "        elif tensor.shape[0] == 1 and isinstance(mean, (list, tuple)) and isinstance(std, (list, tuple)) and len(mean) > 0 and len(std) > 0:\n",
    "             print(\"Warning: Denormalizing grayscale with potentially multi-channel stats. Using first value.\")\n",
    "             mean_used = [mean[0]]\n",
    "             std_used = [std[0]]\n",
    "        else:\n",
    "            raise ValueError(f\"Channel mismatch: Tensor has {tensor.shape[0]} channels, mean has {len(mean)}, std has {len(std)}\")\n",
    "    else:\n",
    "        mean_used = mean\n",
    "        std_used = std\n",
    "\n",
    "    mean_t = torch.as_tensor(mean_used, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
    "    std_t = torch.as_tensor(std_used, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
    "\n",
    "    denormalized_tensor = tensor * std_t + mean_t\n",
    "    return torch.clamp(denormalized_tensor, 0., 1.)\n",
    "\n",
    "def visualize_and_save_saliency(\n",
    "    image_tensor,\n",
    "    saliency_map,\n",
    "    output_dir,\n",
    "    filename_prefix,\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    "    overlay_alpha=0.5,\n",
    "    cmap_name='bwr'): # Added cmap_name parameter\n",
    "    \"\"\"\n",
    "    Visualizes saliency map using a specified colormap, creates an overlay\n",
    "    using OpenCV, and saves the original, heatmap, and overlay images.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): Original image tensor (C, H, W), must be on CPU.\n",
    "        saliency_map (numpy.ndarray): Calculated saliency map (H, W), normalized [0, 1].\n",
    "        output_dir (str): Directory to save the output images.\n",
    "        filename_prefix (str): Prefix for the saved filenames (e.g., 'sample_01').\n",
    "        mean (list, optional): Mean used for image normalization.\n",
    "        std (list, optional): Standard deviation used for image normalization.\n",
    "        overlay_alpha (float, optional): Opacity of the heatmap in the overlay. Defaults to 0.5.\n",
    "        cmap_name (str, optional): Name of the matplotlib colormap to use. Defaults to 'bwr'.\n",
    "    \"\"\"\n",
    "    if image_tensor.is_cuda:\n",
    "        print(\"Warning: image_tensor provided to visualize_and_save_saliency is on CUDA, moving to CPU.\")\n",
    "        image_tensor = image_tensor.cpu()\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    temp_dir = os.path.join(output_dir, 'temp_heatmap') # For temporary heatmap file\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Prepare Original Image\n",
    "    try:\n",
    "        img_denorm_tensor = denormalize_image(image_tensor, mean, std)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during denormalization: {e}\")\n",
    "        print(f\"Image tensor shape: {image_tensor.shape}\")\n",
    "        return # Skip visualization for this image if denormalization fails\n",
    "\n",
    "    img_np = img_denorm_tensor.numpy().transpose(1, 2, 0) # H, W, C\n",
    "    # Ensure image values are in [0, 1] before scaling to [0, 255]\n",
    "    img_np = np.clip(img_np, 0.0, 1.0)\n",
    "    img_uint8 = (img_np * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "    # Handle grayscale conversion for saving/display if needed\n",
    "    if img_uint8.shape[2] == 1:\n",
    "        img_display = cv2.cvtColor(img_uint8, cv2.COLOR_GRAY2RGB) # Keep 3 channels for consistency\n",
    "        img_bgr = cv2.cvtColor(img_uint8, cv2.COLOR_GRAY2BGR) # For OpenCV overlay\n",
    "    elif img_uint8.shape[2] == 3:\n",
    "        img_display = img_uint8 # Already RGB H,W,C\n",
    "        img_bgr = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2BGR) # Convert to BGR for OpenCV\n",
    "    else:\n",
    "         print(f\"Error: Unexpected number of channels ({img_uint8.shape[2]}) in denormalized image.\")\n",
    "         return\n",
    "\n",
    "\n",
    "    # Save original image\n",
    "    orig_save_path = os.path.join(output_dir, f\"{filename_prefix}_original.png\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img_display)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.savefig(orig_save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    # print(f\"Saved original image to {orig_save_path}\") # Less verbose\n",
    "\n",
    "    # 2. Prepare and Save Standalone Heatmap\n",
    "    try:\n",
    "        cmap = cm.get_cmap(cmap_name)\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Colormap '{cmap_name}' not found. Using default 'viridis'.\")\n",
    "        cmap = cm.get_cmap('viridis')\n",
    "    norm = colors.Normalize(vmin=0, vmax=1) # Normalize from 0 to 1\n",
    "\n",
    "    heatmap_save_path = os.path.join(output_dir, f\"{filename_prefix}_heatmap_{cmap_name}.png\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # Ensure saliency map has correct dimensions (H, W)\n",
    "    if saliency_map.ndim != 2:\n",
    "        print(f\"Error: Saliency map has unexpected dimensions {saliency_map.shape}. Expected (H, W).\")\n",
    "        plt.close()\n",
    "        # Clean up temp dir if created\n",
    "        if os.path.exists(temp_heatmap_path): os.remove(temp_heatmap_path)\n",
    "        if os.path.exists(temp_dir) and not os.listdir(temp_dir): os.rmdir(temp_dir)\n",
    "        return\n",
    "\n",
    "    plt.imshow(saliency_map, cmap=cmap, norm=norm)\n",
    "    plt.colorbar(label=f'Normalized Saliency (0: Low/{cmap(0.0)[:3]}, 1: High/{cmap(1.0)[:3]})') # Indicate colors\n",
    "    plt.title(f\"Saliency Heatmap ({cmap_name})\")\n",
    "    plt.axis('off')\n",
    "    plt.savefig(heatmap_save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    # print(f\"Saved heatmap to {heatmap_save_path}\") # Less verbose\n",
    "\n",
    "\n",
    "    # 3. Create Overlay using OpenCV\n",
    "    # Generate colored heatmap image *without* axes/colorbar\n",
    "    temp_heatmap_path = os.path.join(temp_dir, f\"{filename_prefix}_temp_heatmap.png\")\n",
    "    # Match aspect ratio and use known DPI for predictable sizing\n",
    "    fig_width_inches = img_display.shape[1] / 100.0\n",
    "    fig_height_inches = img_display.shape[0] / 100.0\n",
    "    plt.figure(figsize=(fig_width_inches, fig_height_inches), dpi=100)\n",
    "    plt.imshow(saliency_map, cmap=cmap, norm=norm)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(temp_heatmap_path, bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "    # Read the saved heatmap with OpenCV\n",
    "    colored_heatmap_bgr = cv2.imread(temp_heatmap_path)\n",
    "\n",
    "    # Clean up temporary file\n",
    "    if os.path.exists(temp_heatmap_path):\n",
    "        os.remove(temp_heatmap_path)\n",
    "        try:\n",
    "            # Attempt to remove temp dir only if it's empty\n",
    "            if not os.listdir(temp_dir):\n",
    "                 os.rmdir(temp_dir)\n",
    "        except OSError:\n",
    "            pass # Ignore if not empty (e.g., race condition in parallel runs)\n",
    "\n",
    "    if colored_heatmap_bgr is None:\n",
    "        print(f\"Error: Could not read temporary heatmap file: {temp_heatmap_path}\")\n",
    "        return\n",
    "\n",
    "    # Resize heatmap to match original image size (important safety check)\n",
    "    if colored_heatmap_bgr.shape[:2] != img_bgr.shape[:2]:\n",
    "         print(f\"Warning: Resizing heatmap from {colored_heatmap_bgr.shape[:2]} to {img_bgr.shape[:2]}\")\n",
    "         colored_heatmap_bgr = cv2.resize(colored_heatmap_bgr, (img_bgr.shape[1], img_bgr.shape[0]),\n",
    "                                          interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "\n",
    "    # Blend the images using cv2.addWeighted\n",
    "    overlay = cv2.addWeighted(\n",
    "        src1=img_bgr,             # Original image (BGR)\n",
    "        alpha=1.0 - overlay_alpha,# Weight for original image\n",
    "        src2=colored_heatmap_bgr, # Colored heatmap (BGR)\n",
    "        beta=overlay_alpha,       # Weight for heatmap\n",
    "        gamma=0.0                 # Scalar added to each sum\n",
    "    )\n",
    "\n",
    "    # Convert overlay back to RGB for saving with matplotlib/saving directly\n",
    "    overlay_rgb = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Save overlay image\n",
    "    overlay_save_path = os.path.join(output_dir, f\"{filename_prefix}_overlay_{cmap_name}.png\")\n",
    "    # Save directly using OpenCV for potentially better fidelity than matplotlib savefig\n",
    "    try:\n",
    "         # Convert RGB back to BGR for cv2.imwrite\n",
    "         success = cv2.imwrite(overlay_save_path, cv2.cvtColor(overlay_rgb, cv2.COLOR_RGB2BGR))\n",
    "         if not success:\n",
    "              print(f\"Error: cv2.imwrite failed to save overlay to {overlay_save_path}\")\n",
    "              # Fallback to matplotlib saving if cv2 fails\n",
    "              plt.figure(figsize=(8, 8))\n",
    "              plt.imshow(overlay_rgb)\n",
    "              plt.axis('off')\n",
    "              plt.title(f\"Saliency Overlay ({cmap_name})\")\n",
    "              plt.savefig(overlay_save_path, bbox_inches='tight', pad_inches=0)\n",
    "              plt.close()\n",
    "\n",
    "         # print(f\"Saved overlay to {overlay_save_path}\") # Less verbose\n",
    "    except Exception as e:\n",
    "         print(f\"Exception during overlay saving: {e}\")\n",
    "         # Fallback just in case\n",
    "         plt.figure(figsize=(8, 8))\n",
    "         plt.imshow(overlay_rgb)\n",
    "         plt.axis('off')\n",
    "         plt.title(f\"Saliency Overlay ({cmap_name})\")\n",
    "         plt.savefig(overlay_save_path, bbox_inches='tight', pad_inches=0)\n",
    "         plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Main Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Saliency Map Generation and Visualization ---\n",
      "Output visualizations will be saved in: saliency_visualizations_real\n",
      "Processing 1 images from the test set.\n",
      "Using device: cuda\n",
      "Saliency generation sigmas based on percentages: None, min_sigma: 5\n",
      "Occlusion mask value: 0.0\n",
      "Visualization colormap: jet, alpha: 0.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c211dbd0d12a434fb1cfd3ea4b125323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Progress:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image 1/1 (DataLoader index: 0)\n",
      "  True Authenticity: 36.9786\n",
      "  Original Predicted Authenticity: 33.1730\n",
      "  Sigma list for this image: [3, 5, 9, 17, 33, 65]\n",
      "  Calculating saliency map...\n",
      "Calculating saliency for image size torch.Size([224, 224]) using 6 sigmas: [3, 5, 9, 17, 33, 65]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c2dceabeea4ee28a8ffb8923c2f7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Sigmas :   0%|          | 0/6 [00:00<?, ?sigma/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8bc82a0a8240008dba6dde8de8196e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Sigma 1/6 (val=  3) Pixels:   0%|          | 0/50176 [00:00<?, ?pixel/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sigma 3 finished in 218.27s (229.9 pixels/sec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb07f88d12e44d9982686ea16ada751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Sigma 2/6 (val=  5) Pixels:   0%|          | 0/50176 [00:00<?, ?pixel/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sigma 5 finished in 218.20s (230.0 pixels/sec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04af8bbddbc4493a9f381db9b0c117c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Sigma 3/6 (val=  9) Pixels:   0%|          | 0/50176 [00:00<?, ?pixel/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sigma 9 finished in 218.21s (229.9 pixels/sec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa59bb546bb045beb0a64c5fda861794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Sigma 4/6 (val= 17) Pixels:   0%|          | 0/50176 [00:00<?, ?pixel/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sigma 17 finished in 218.78s (229.3 pixels/sec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4de2f0fdc04befabae3d01dfcfa59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Sigma 5/6 (val= 33) Pixels:   0%|          | 0/50176 [00:00<?, ?pixel/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sigma 33 finished in 218.62s (229.5 pixels/sec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da240c41a3c4905ad731332f8008349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Sigma 6/6 (val= 65) Pixels:   0%|          | 0/50176 [00:00<?, ?pixel/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sigma 65 finished in 218.32s (229.8 pixels/sec)\n",
      "  Saliency map calculated with shape: (224, 224)\n",
      "  Visualizing and saving results with prefix: image_000_auth_36.98_pred_33.17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3879636/4217433283.py:263: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = cm.get_cmap(cmap_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Resizing heatmap from (172, 172) to (224, 224)\n",
      "  Visualization saved.\n",
      "\n",
      "Reached limit of 1 images. Stopping.\n",
      "\n",
      "--- Saliency Map Generation and Visualization Finished ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Starting Saliency Map Generation and Visualization ---\")\n",
    "    # Define the directory to save output images\n",
    "    output_visualization_dir = 'saliency_visualizations_real'\n",
    "    os.makedirs(output_visualization_dir, exist_ok=True)\n",
    "    print(f\"Output visualizations will be saved in: {output_visualization_dir}\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    NUM_IMAGES_TO_PROCESS = 1 # Number of images to process from the test set\n",
    "    # Sigma generation parameters\n",
    "    SIGMA_PERCENTAGES = None\n",
    "    SIGMA_LIST = [3,5,9,17,33,65] # Extracted from the original code, but can be None to generate based on percentages\n",
    "    MIN_SIGMA = 5 # Minimum occlusion size in pixels\n",
    "    MASK_VALUE = 0.0 # Value for occluded pixels (0.0 for black, could be mean image value)\n",
    "    VIS_CMAP = 'jet' # Colormap for visualization ('jet', 'bwr', 'viridis', etc.)\n",
    "    VIS_ALPHA = 0.6 # Overlay opacity\n",
    "\n",
    "    print(f\"Processing {NUM_IMAGES_TO_PROCESS} images from the test set.\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Saliency generation sigmas based on percentages: {SIGMA_PERCENTAGES}, min_sigma: {MIN_SIGMA}\")\n",
    "    print(f\"Occlusion mask value: {MASK_VALUE}\")\n",
    "    print(f\"Visualization colormap: {VIS_CMAP}, alpha: {VIS_ALPHA}\")\n",
    "\n",
    "\n",
    "    # --- Processing Loop ---\n",
    "    processed_count = 0\n",
    "    # Ensure model is in eval mode (already done after loading, but good practice)\n",
    "    baseline_model.eval()\n",
    "\n",
    "    # Iterate through the test dataloader\n",
    "    # Wrap the dataloader with tqdm for overall progress\n",
    "    test_iterator = tqdm(test_dataloader, total=min(NUM_IMAGES_TO_PROCESS, len(test_dataloader)), desc=\"Overall Progress\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(test_iterator):\n",
    "        if processed_count >= NUM_IMAGES_TO_PROCESS:\n",
    "            print(f\"\\nReached limit of {NUM_IMAGES_TO_PROCESS} images. Stopping.\")\n",
    "            break\n",
    "\n",
    "        # Since BATCH_SIZE=1, images and labels contain single items\n",
    "        image_tensor = images[0].to(device) # Get the image tensor, move to device\n",
    "        label = labels[0]                   # Get the corresponding label tensor\n",
    "\n",
    "        print(f\"\\nProcessing image {processed_count + 1}/{NUM_IMAGES_TO_PROCESS} (DataLoader index: {i})\")\n",
    "\n",
    "        # 1. Get Original Model Score\n",
    "        with torch.no_grad():\n",
    "            # Add batch dimension for the model, ensure image is on device\n",
    "            original_output = baseline_model(image_tensor.unsqueeze(0))\n",
    "            # Unpack score based on model's forward method\n",
    "            if isinstance(original_output, tuple) and len(original_output) > 0:\n",
    "                original_score_tensor = original_output[0]\n",
    "            else:\n",
    "                original_score_tensor = original_output\n",
    "            # Get the scalar score value\n",
    "            original_score = original_score_tensor.item()\n",
    "\n",
    "        true_authenticity = label.item() # Get scalar value from label tensor\n",
    "        print(f\"  True Authenticity: {true_authenticity:.4f}\")\n",
    "        print(f\"  Original Predicted Authenticity: {original_score:.4f}\")\n",
    "\n",
    "\n",
    "        # 2. Generate Sigma List for this image\n",
    "        sigma_list = SIGMA_LIST if SIGMA_LIST else create_sigma_list_from_image(\n",
    "            image_tensor=image_tensor, # Pass the single image tensor (C, H, W) on device\n",
    "            percentages=SIGMA_PERCENTAGES if SIGMA_PERCENTAGES else [0.05, 0.10, 0.20],\n",
    "            min_sigma=MIN_SIGMA\n",
    "        )\n",
    "        print(f\"  Sigma list for this image: {sigma_list}\")\n",
    "        \n",
    "        if not sigma_list:\n",
    "             print(\"  Warning: No sigma values generated. Skipping saliency calculation.\")\n",
    "             processed_count += 1\n",
    "             test_iterator.set_postfix_str(f\"Image {processed_count}/{NUM_IMAGES_TO_PROCESS} (Skipped)\")\n",
    "             continue # Skip to the next image\n",
    "\n",
    "\n",
    "        # 3. Calculate Saliency Map\n",
    "        print(f\"  Calculating saliency map...\")\n",
    "        saliency_map_np = calculate_saliency_map(\n",
    "            model=baseline_model,\n",
    "            image=image_tensor, # Pass the single image tensor (C, H, W) on device\n",
    "            original_score=original_score,\n",
    "            sigma_list=sigma_list,\n",
    "            mask_value=MASK_VALUE\n",
    "        )\n",
    "        print(f\"  Saliency map calculated with shape: {saliency_map_np.shape}\")\n",
    "\n",
    "\n",
    "        # 4. Visualization and Saving\n",
    "        filename_prefix = f\"image_{processed_count:03d}_auth_{true_authenticity:.2f}_pred_{original_score:.2f}\"\n",
    "        print(f\"  Visualizing and saving results with prefix: {filename_prefix}...\")\n",
    "\n",
    "        # Ensure the image tensor passed to visualize is on CPU and without batch dim\n",
    "        image_to_visualize = image_tensor.cpu()\n",
    "\n",
    "        visualize_and_save_saliency(\n",
    "            image_tensor=image_to_visualize,\n",
    "            saliency_map=saliency_map_np, # Pass the calculated saliency map (numpy array)\n",
    "            output_dir=output_visualization_dir,\n",
    "            filename_prefix=filename_prefix,\n",
    "            overlay_alpha=VIS_ALPHA,\n",
    "            cmap_name=VIS_CMAP\n",
    "        )\n",
    "        print(f\"  Visualization saved.\")\n",
    "\n",
    "        processed_count += 1\n",
    "        # Update overall progress bar postfix\n",
    "        test_iterator.set_postfix_str(f\"Image {processed_count}/{NUM_IMAGES_TO_PROCESS}\")\n",
    "\n",
    "\n",
    "    # Close the main progress bar upon completion\n",
    "    test_iterator.close()\n",
    "    print(\"\\n--- Saliency Map Generation and Visualization Finished ---\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
