{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2  # OpenCV for image processing\n",
    "import itertools # Added for saliency calculation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.hub\n",
    "# import torch.nn.functional as F # Already imported via torchvision below\n",
    "\n",
    "from torchvision import models, transforms\n",
    "import torchvision.transforms.functional as F # For potential use in visualization (though visualize_and_save_saliency uses different method)\n",
    "\n",
    "# Use tqdm.auto for better console/notebook detection and nesting\n",
    "from tqdm.auto import tqdm\n",
    "import time # Optional: Can add timing info to postfix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Database creations using pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAuthenticityDataset(Dataset):\n",
    "    \"\"\"Dataset for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.dir_path = os.path.dirname(csv_file)  # Directory of the CSV file\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx,):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its labels by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (image, labels) where:\n",
    "                image (PIL.Image): The image.\n",
    "                labels (torch.Tensor): Tensor containing quality and authenticity scores.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # TODO: to be fixed, right now is folder dependent\n",
    "        img_path_relative = self.data.iloc[idx, 3]\n",
    "        # Construct absolute path based on CSV location\n",
    "        base_dir = os.path.abspath(os.path.join(self.dir_path, '../../')) # Go up two levels from CSV dir\n",
    "        img_name = os.path.join(base_dir, img_path_relative.replace(\"./\", \"\")) # Combine and remove './'\n",
    "\n",
    "        # Ensure path exists before opening\n",
    "        if not os.path.exists(img_name):\n",
    "             # Fallback or error handling if path logic is complex\n",
    "             print(f\"Warning: Image path {img_name} not found directly. Trying original relative path logic...\")\n",
    "             img_name = self.data.iloc[idx, 3].replace(\"./\", \"../../\") # Original logic as fallback\n",
    "             if not os.path.exists(img_name):\n",
    "                  raise FileNotFoundError(f\"Could not find image file at primary path: {os.path.join(base_dir, img_path_relative.replace('./', ''))} or fallback: {img_name}\")\n",
    "\n",
    "\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        authenticity = self.data.iloc[idx, 1]  # Authenticity column\n",
    "        labels = torch.tensor([authenticity], dtype=torch.float)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Definitions of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthenticityPredictor(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained BarlowTwins ResNet50 instead of ResNet-152\n",
    "        barlow_twins_resnet = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')\n",
    "\n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in barlow_twins_resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Model backbone frozen.\")\n",
    "        else:\n",
    "            print(\"Model backbone NOT frozen (trainable).\")\n",
    "\n",
    "\n",
    "        self.features = nn.Sequential(*list(barlow_twins_resnet.children())[:-2])\n",
    "        self.avgpool = barlow_twins_resnet.avgpool\n",
    "\n",
    "\n",
    "        self.regression_head = nn.Sequential(\n",
    "                nn.Linear(2048, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(512, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        predictions = self.regression_head(x)\n",
    "        # Return both predictions and flattened features (useful for some saliency methods)\n",
    "        return predictions, x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Setup section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data transformations for the ImageNet dataset\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define path relative to the script location or use an absolute path\n",
    "# Assuming the script is run from a location where '../../Dataset/...' is valid\n",
    "try:\n",
    "    # Try relative path first\n",
    "    annotations_file = '../../Dataset/AIGCIQA2023/real_images_annotations.csv'\n",
    "    # Check if the file exists using the relative path\n",
    "    if not os.path.exists(annotations_file):\n",
    "        # If relative path fails, try constructing from script directory\n",
    "        script_dir = os.path.dirname(__file__) # Get directory of the script\n",
    "        annotations_file = os.path.abspath(os.path.join(script_dir, '../../Dataset/AIGCIQA2023/real_images_annotations.csv'))\n",
    "        if not os.path.exists(annotations_file):\n",
    "            raise FileNotFoundError(f\"Annotations file not found at relative or script-based path: {annotations_file}\")\n",
    "except NameError:\n",
    "     # __file__ is not defined (e.g., running in interactive environment like Jupyter)\n",
    "     # Fallback to assuming relative path from CWD or specify absolute path directly\n",
    "     annotations_file = '../../Dataset/AIGCIQA2023/real_images_annotations.csv' # Or provide absolute path\n",
    "     print(\"Warning: __file__ not defined. Assuming relative path for annotations file.\")\n",
    "     if not os.path.exists(annotations_file):\n",
    "        raise FileNotFoundError(f\"Annotations file not found at relative path: {annotations_file}. Please provide absolute path if needed.\")\n",
    "\n",
    "\n",
    "print(f\"Loading annotations from: {annotations_file}\")\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageAuthenticityDataset(csv_file=annotations_file, transform=data_transforms)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42) # Use manual_seed_all for multi-GPU setups if relevant\n",
    "np.random.seed(42)\n",
    "# Potentially add for DataLoader determinism (might impact performance)\n",
    "# def seed_worker(worker_id):\n",
    "#     worker_seed = torch.initial_seed() % 2**32\n",
    "#     numpy.random.seed(worker_seed)\n",
    "#     random.seed(worker_seed)\n",
    "# g = torch.Generator()\n",
    "# g.manual_seed(42)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "print(f\"Dataset size: {len(dataset)}. Splitting into Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)) # Add generator for split reproducibility\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 1 # Set batch size to 1 for easier processing of individual images for saliency\n",
    "NUM_WORKERS = 4 # Adjust based on your system\n",
    "# Consider adding pin_memory=True if using GPU for potentially faster data transfer\n",
    "# Create only the data loader for test\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)#, worker_init_fn=seed_worker, generator=g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Models loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASELINE_MODEL_PATH = 'Weights/BarlowTwins_real_authenticity_finetuned.pth'\n",
    "PRUNED_MODEL_PATH = 'Weights/real_authenticity_noise_out_pruned_model.pth'    \n",
    "\n",
    "# ----------------------------------------#\n",
    "\n",
    "# Instantiate model with frozen backbone by default\n",
    "baseline_model = AuthenticityPredictor(freeze_backbone=True)\n",
    "# Load weights - ensure map_location handles CPU/GPU loading correctly\n",
    "baseline_model.load_state_dict(torch.load(BASELINE_MODEL_PATH, map_location=device))\n",
    "baseline_model.eval().to(device) # Set to evaluation mode and move to device\n",
    "print(\"Baseline model loaded and set to evaluation mode.\")\n",
    "\n",
    "pruned_model = AuthenticityPredictor(freeze_backbone=True)\n",
    "# Load pruned model weights\n",
    "pruned_model.load_state_dict(torch.load(PRUNED_MODEL_PATH, map_location=device))\n",
    "pruned_model.eval().to(device) # Set to evaluation mode and move to device\n",
    "print(\"Pruned model loaded and set to evaluation mode.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Functions definitions (Image Utils & Saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(img_size, center, sigma):\n",
    "    \"\"\"Generates a binary mask with a square of zeros centered at 'center' with size 'sigma x sigma'.\"\"\"\n",
    "    mask = torch.ones(1, 1, img_size[0], img_size[1], device=device)\n",
    "    start_x = max(0, int(center[0] - sigma // 2))\n",
    "    end_x = min(img_size[1], int(center[0] + (sigma + 1) // 2))\n",
    "    start_y = max(0, int(center[1] - sigma // 2))\n",
    "    end_y = min(img_size[0], int(center[1] + (sigma + 1) // 2))\n",
    "    if start_y < end_y and start_x < end_x:\n",
    "        mask[:, :, start_y:end_y, start_x:end_x] = 0\n",
    "    return mask\n",
    "\n",
    "def calculate_saliency_map(model, image, original_score, sigma_list, mask_value=0.0):\n",
    "    \"\"\"\n",
    "    Calculates the multiscale saliency map using the occlusion method\n",
    "    by summing scores across scales and normalizing the result,\n",
    "    with nested progress bars and showing current pixel impact in the postfix.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to use for inference. Must be on the correct device.\n",
    "        image (torch.Tensor): The input image tensor (C, H, W), must be on the correct device.\n",
    "        original_score (float): The model's score for the original, unoccluded image.\n",
    "        sigma_list (list): List of integers representing the sizes (side length) of the occlusion squares.\n",
    "        mask_value (float, optional): Value to use for occluded regions. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A normalized saliency map (H, W) as a NumPy array on the CPU.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Ensure image is on the correct device and add batch dimension\n",
    "    img_tensor = image.unsqueeze(0).to(device)\n",
    "    img_size = img_tensor.shape[2:] # H, W\n",
    "    # Initialize the final map to store the sum of per-scale saliencies\n",
    "    saliency_map_final = torch.zeros(img_size, dtype=torch.float32, device=device)\n",
    "\n",
    "    print(f\"Calculating saliency for image size {img_size} using {len(sigma_list)} sigmas: {sigma_list}\")\n",
    "\n",
    "    # --- Outer Progress Bar for Sigmas ---\n",
    "    outer_progress = tqdm(\n",
    "        enumerate(sigma_list),\n",
    "        total=len(sigma_list),\n",
    "        desc=\"Overall Sigmas \", # Add space for better alignment\n",
    "        unit=\"sigma\",\n",
    "        position=0,\n",
    "        leave=True # Keep after finishing\n",
    "    )\n",
    "\n",
    "    for i, sigma in outer_progress:\n",
    "        # Temporary map for the current sigma scale\n",
    "        saliency_map_sigma = torch.zeros(img_size, dtype=torch.float32, device=device)\n",
    "\n",
    "        # --- Inner Progress Bar for Pixels ---\n",
    "        pixel_iterator = itertools.product(range(img_size[0]), range(img_size[1]))\n",
    "        total_pixels = img_size[0] * img_size[1]\n",
    "        inner_progress_bar = tqdm(\n",
    "            pixel_iterator,\n",
    "            total=total_pixels,\n",
    "            desc=f\"  Sigma {i+1}/{len(sigma_list)} (val={sigma: >3}) Pixels\",\n",
    "            leave=False, # Remove after each sigma finishes\n",
    "            unit=\"pixel\",\n",
    "            \n",
    "            position=1,\n",
    "            mininterval=0.1 # Refresh rate throttle (optional)\n",
    "        )\n",
    "\n",
    "        start_time = time.time() # For calculating rate\n",
    "        for y, x in inner_progress_bar:\n",
    "            # Generate mask for the current pixel and sigma\n",
    "            mask = generate_mask(img_size, (x, y), sigma)\n",
    "            # Apply mask\n",
    "            masked_image = img_tensor * mask + mask_value * (1 - mask)\n",
    "\n",
    "            # Get model prediction for the masked image\n",
    "            with torch.no_grad():\n",
    "                output = model(masked_image)\n",
    "                # Handle cases where model returns multiple outputs (e.g., prediction, features)\n",
    "                if isinstance(output, tuple) and len(output) > 0:\n",
    "                    masked_score_tensor = output[0]\n",
    "                else:\n",
    "                    masked_score_tensor = output\n",
    "                # Ensure score is a scalar on CPU\n",
    "                masked_score_item = masked_score_tensor.detach().cpu().item()\n",
    "\n",
    "            # Calculate saliency value for this pixel and sigma\n",
    "            saliency_value = original_score - masked_score_item\n",
    "            saliency_map_sigma[y, x] = saliency_value\n",
    "\n",
    "            # --- Update Postfix with Current Pixel Info ---\n",
    "            inner_progress_bar.set_postfix(\n",
    "                pixel=f\"({y},{x})\",\n",
    "                impact=f\"{saliency_value:.4f}\", # Format saliency value\n",
    "                refresh=False # Update display on tqdm's schedule\n",
    "            )\n",
    "            # -----------------------------------------------\n",
    "\n",
    "        saliency_map_final += saliency_map_sigma\n",
    "        # ---------------------------------------------------------------\n",
    "\n",
    "        # Optional: Print time taken per sigma\n",
    "        elapsed_time = time.time() - start_time\n",
    "        pixels_per_sec = total_pixels / elapsed_time if elapsed_time > 0 else float('inf')\n",
    "        tqdm.write(f\"  Sigma {sigma} finished in {elapsed_time:.2f}s ({pixels_per_sec:.1f} pixels/sec)\")\n",
    "\n",
    "    # --- Normalization ---\n",
    "    # Normalize the *summed* final map\n",
    "    min_val = torch.min(saliency_map_final)\n",
    "    max_val = torch.max(saliency_map_final)\n",
    "\n",
    "    if max_val > min_val:\n",
    "        # Perform min-max normalization to range [0, 1]\n",
    "        saliency_map_normalized = (saliency_map_final - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        # Handle the case where the map is constant (all saliency values were the same)\n",
    "        saliency_map_normalized = torch.zeros_like(saliency_map_final)\n",
    "        print(\"Warning: Final saliency map was constant before normalization. Result is zero map.\")\n",
    "\n",
    "    # save the saliency map to a numpy array\n",
    "    saliency_map_normalized = saliency_map_normalized.cpu().numpy()\n",
    "    return saliency_map_normalized\n",
    "\n",
    "def denormalize_image(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"Denormalizes an image tensor.\"\"\"\n",
    "    if tensor.dim() != 3:\n",
    "        raise ValueError(f\"Input tensor must have 3 dimensions (C, H, W), but got {tensor.dim()}\")\n",
    "    if tensor.shape[0] != len(mean) or tensor.shape[0] != len(std):\n",
    "         # Handle grayscale - assume mean/std are single values or adaptable\n",
    "        if tensor.shape[0] == 1 and len(mean) == 3 and len(std) == 3:\n",
    "             print(\"Warning: Denormalizing grayscale with potentially RGB stats. Using first value.\")\n",
    "             mean_used = [mean[0]]\n",
    "             std_used = [std[0]]\n",
    "        elif tensor.shape[0] == 1 and isinstance(mean, (int, float)) and isinstance(std, (int, float)):\n",
    "             mean_used = [mean]\n",
    "             std_used = [std]\n",
    "        elif tensor.shape[0] == 1 and isinstance(mean, (list, tuple)) and isinstance(std, (list, tuple)) and len(mean) > 0 and len(std) > 0:\n",
    "             print(\"Warning: Denormalizing grayscale with potentially multi-channel stats. Using first value.\")\n",
    "             mean_used = [mean[0]]\n",
    "             std_used = [std[0]]\n",
    "        else:\n",
    "            raise ValueError(f\"Channel mismatch: Tensor has {tensor.shape[0]} channels, mean has {len(mean)}, std has {len(std)}\")\n",
    "    else:\n",
    "        mean_used = mean\n",
    "        std_used = std\n",
    "\n",
    "    mean_t = torch.as_tensor(mean_used, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
    "    std_t = torch.as_tensor(std_used, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
    "\n",
    "    denormalized_tensor = tensor * std_t + mean_t\n",
    "    return torch.clamp(denormalized_tensor, 0., 1.)\n",
    "\n",
    "def visualize_and_save_saliency(\n",
    "    image_tensor,\n",
    "    saliency_map,\n",
    "    output_dir,\n",
    "    filename_prefix,\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    "    overlay_alpha=0.5,\n",
    "    cmap_name='bwr'): # Added cmap_name parameter\n",
    "    \"\"\"\n",
    "    Visualizes saliency map using a specified colormap, creates an overlay\n",
    "    using OpenCV, and saves the original, heatmap, and overlay images.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): Original image tensor (C, H, W), must be on CPU.\n",
    "        saliency_map (numpy.ndarray): Calculated saliency map (H, W), normalized [0, 1].\n",
    "        output_dir (str): Directory to save the output images.\n",
    "        filename_prefix (str): Prefix for the saved filenames (e.g., 'sample_01').\n",
    "        mean (list, optional): Mean used for image normalization.\n",
    "        std (list, optional): Standard deviation used for image normalization.\n",
    "        overlay_alpha (float, optional): Opacity of the heatmap in the overlay. Defaults to 0.5.\n",
    "        cmap_name (str, optional): Name of the matplotlib colormap to use. Defaults to 'bwr'.\n",
    "    \"\"\"\n",
    "    if image_tensor.is_cuda:\n",
    "        print(\"Warning: image_tensor provided to visualize_and_save_saliency is on CUDA, moving to CPU.\")\n",
    "        image_tensor = image_tensor.cpu()\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    temp_dir = os.path.join(output_dir, 'temp_heatmap') # For temporary heatmap file\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # 0. save the saliency map to a numpy array\n",
    "    if saliency_map.ndim != 2:\n",
    "        print(f\"Error: Saliency map has unexpected dimensions {saliency_map.shape}. Expected (H, W).\")\n",
    "        return\n",
    "    # Ensure saliency map is in [0, 1] range before saving\n",
    "    saliency_map = np.clip(saliency_map, 0.0, 1.0)\n",
    "    # Save the saliency map as a numpy file for later use\n",
    "    # Check if the folder exists, if not create it\n",
    "    NUMPY_DIR = os.path.join(output_dir, 'numpy_saliency_maps')\n",
    "    os.makedirs(NUMPY_DIR, exist_ok=True)\n",
    "    np.save(os.path.join(NUMPY_DIR, f\"{filename_prefix}_saliency_map.npy\"), saliency_map)\n",
    "    # 1. Prepare Original Image\n",
    "    try:\n",
    "        img_denorm_tensor = denormalize_image(image_tensor, mean, std)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during denormalization: {e}\")\n",
    "        print(f\"Image tensor shape: {image_tensor.shape}\")\n",
    "        return # Skip visualization for this image if denormalization fails\n",
    "\n",
    "    img_np = img_denorm_tensor.numpy().transpose(1, 2, 0) # H, W, C\n",
    "    # Ensure image values are in [0, 1] before scaling to [0, 255]\n",
    "    img_np = np.clip(img_np, 0.0, 1.0)\n",
    "    img_uint8 = (img_np * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "    # Handle grayscale conversion for saving/display if needed\n",
    "    if img_uint8.shape[2] == 1:\n",
    "        img_display = cv2.cvtColor(img_uint8, cv2.COLOR_GRAY2RGB) # Keep 3 channels for consistency\n",
    "        img_bgr = cv2.cvtColor(img_uint8, cv2.COLOR_GRAY2BGR) # For OpenCV overlay\n",
    "    elif img_uint8.shape[2] == 3:\n",
    "        img_display = img_uint8 # Already RGB H,W,C\n",
    "        img_bgr = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2BGR) # Convert to BGR for OpenCV\n",
    "    else:\n",
    "         print(f\"Error: Unexpected number of channels ({img_uint8.shape[2]}) in denormalized image.\")\n",
    "         return\n",
    "\n",
    "    # Create a folder with filename prefix to save images \n",
    "    os.makedirs(os.path.join(output_dir, filename_prefix), exist_ok=True)\n",
    "    # Save original image in the new folder\n",
    "    output_dir = os.path.join(output_dir, filename_prefix)\n",
    "\n",
    "    # Save original image\n",
    "    orig_save_path = os.path.join(output_dir, f\"{filename_prefix}_original.png\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img_display)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.savefig(orig_save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    # print(f\"Saved original image to {orig_save_path}\") # Less verbose\n",
    "\n",
    "    # 2. Prepare and Save Standalone Heatmap\n",
    "    try:\n",
    "        cmap = cm.get_cmap(cmap_name)\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Colormap '{cmap_name}' not found. Using default 'viridis'.\")\n",
    "        cmap = cm.get_cmap('viridis')\n",
    "    norm = colors.Normalize(vmin=0, vmax=1) # Normalize from 0 to 1\n",
    "\n",
    "    heatmap_save_path = os.path.join(output_dir, f\"{filename_prefix}_heatmap_{cmap_name}.png\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # Ensure saliency map has correct dimensions (H, W)\n",
    "    if saliency_map.ndim != 2:\n",
    "        print(f\"Error: Saliency map has unexpected dimensions {saliency_map.shape}. Expected (H, W).\")\n",
    "        plt.close()\n",
    "        # Clean up temp dir if created\n",
    "        if os.path.exists(temp_heatmap_path): os.remove(temp_heatmap_path)\n",
    "        if os.path.exists(temp_dir) and not os.listdir(temp_dir): os.rmdir(temp_dir)\n",
    "        return\n",
    "\n",
    "    plt.imshow(saliency_map, cmap=cmap, norm=norm)\n",
    "    plt.colorbar(label=f'Normalized Saliency (0: Low/{cmap(0.0)[:3]}, 1: High/{cmap(1.0)[:3]})') # Indicate colors\n",
    "    plt.title(f\"Saliency Heatmap ({cmap_name})\")\n",
    "    plt.axis('off')\n",
    "    plt.savefig(heatmap_save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    # print(f\"Saved heatmap to {heatmap_save_path}\") # Less verbose\n",
    "\n",
    "\n",
    "    # 3. Create Overlay using OpenCV\n",
    "    # Generate colored heatmap image *without* axes/colorbar\n",
    "    temp_heatmap_path = os.path.join(temp_dir, f\"{filename_prefix}_temp_heatmap.png\")\n",
    "    # Match aspect ratio and use known DPI for predictable sizing\n",
    "    fig_width_inches = img_display.shape[1] / 100.0\n",
    "    fig_height_inches = img_display.shape[0] / 100.0\n",
    "    plt.figure(figsize=(fig_width_inches, fig_height_inches), dpi=100)\n",
    "    plt.imshow(saliency_map, cmap=cmap, norm=norm)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(temp_heatmap_path, bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "    # Read the saved heatmap with OpenCV\n",
    "    colored_heatmap_bgr = cv2.imread(temp_heatmap_path)\n",
    "\n",
    "    # Clean up temporary file\n",
    "    if os.path.exists(temp_heatmap_path):\n",
    "        os.remove(temp_heatmap_path)\n",
    "        try:\n",
    "            # Attempt to remove temp dir only if it's empty\n",
    "            if not os.listdir(temp_dir):\n",
    "                 os.rmdir(temp_dir)\n",
    "        except OSError:\n",
    "            pass # Ignore if not empty (e.g., race condition in parallel runs)\n",
    "\n",
    "    if colored_heatmap_bgr is None:\n",
    "        print(f\"Error: Could not read temporary heatmap file: {temp_heatmap_path}\")\n",
    "        return\n",
    "\n",
    "    # Resize heatmap to match original image size (important safety check)\n",
    "    if colored_heatmap_bgr.shape[:2] != img_bgr.shape[:2]:\n",
    "         print(f\"Warning: Resizing heatmap from {colored_heatmap_bgr.shape[:2]} to {img_bgr.shape[:2]}\")\n",
    "         colored_heatmap_bgr = cv2.resize(colored_heatmap_bgr, (img_bgr.shape[1], img_bgr.shape[0]),\n",
    "                                          interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "\n",
    "    # Blend the images using cv2.addWeighted\n",
    "    overlay = cv2.addWeighted(\n",
    "        src1=img_bgr,             # Original image (BGR)\n",
    "        alpha=1.0 - overlay_alpha,# Weight for original image\n",
    "        src2=colored_heatmap_bgr, # Colored heatmap (BGR)\n",
    "        beta=overlay_alpha,       # Weight for heatmap\n",
    "        gamma=0.0                 # Scalar added to each sum\n",
    "    )\n",
    "\n",
    "    # Convert overlay back to RGB for saving with matplotlib/saving directly\n",
    "    overlay_rgb = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Save overlay image\n",
    "    overlay_save_path = os.path.join(output_dir, f\"{filename_prefix}_overlay_{cmap_name}.png\")\n",
    "    # Save directly using OpenCV for potentially better fidelity than matplotlib savefig\n",
    "    try:\n",
    "         # Convert RGB back to BGR for cv2.imwrite\n",
    "         success = cv2.imwrite(overlay_save_path, cv2.cvtColor(overlay_rgb, cv2.COLOR_RGB2BGR))\n",
    "         if not success:\n",
    "              print(f\"Error: cv2.imwrite failed to save overlay to {overlay_save_path}\")\n",
    "              # Fallback to matplotlib saving if cv2 fails\n",
    "              plt.figure(figsize=(8, 8))\n",
    "              plt.imshow(overlay_rgb)\n",
    "              plt.axis('off')\n",
    "              plt.title(f\"Saliency Overlay ({cmap_name})\")\n",
    "              plt.savefig(overlay_save_path, bbox_inches='tight', pad_inches=0)\n",
    "              plt.close()\n",
    "\n",
    "         # print(f\"Saved overlay to {overlay_save_path}\") # Less verbose\n",
    "    except Exception as e:\n",
    "         print(f\"Exception during overlay saving: {e}\")\n",
    "         # Fallback just in case\n",
    "         plt.figure(figsize=(8, 8))\n",
    "         plt.imshow(overlay_rgb)\n",
    "         plt.axis('off')\n",
    "         plt.title(f\"Saliency Overlay ({cmap_name})\")\n",
    "         plt.savefig(overlay_save_path, bbox_inches='tight', pad_inches=0)\n",
    "         plt.close()\n",
    "\n",
    "def run_saliency_analysis(\n",
    "    model,\n",
    "    dataloader,\n",
    "    output_dir,\n",
    "    num_images_to_process,\n",
    "    sigma_list,\n",
    "    mask_value=0.0,\n",
    "    vis_cmap='bwr',\n",
    "    vis_alpha=0.6,\n",
    "    device='cpu',\n",
    "    model_name=\"Model\" # Added for clearer logging\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Facade function to run saliency map generation and visualization for a given model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to analyze (already loaded and on the correct device).\n",
    "        dataloader (DataLoader): DataLoader providing test images and labels.\n",
    "        output_dir (str): Directory to save the output visualizations.\n",
    "        num_images_to_process (int): Maximum number of images to process.\n",
    "        sigma_list (list): List of sigma values for occlusion.\n",
    "        mask_value (float, optional): Value for occluded pixels. Defaults to 0.0.\n",
    "        vis_cmap (str, optional): Colormap for visualization. Defaults to 'bwr'.\n",
    "        vis_alpha (float, optional): Overlay opacity for visualization. Defaults to 0.6.\n",
    "        device (torch.device or str, optional): Device to run calculations on. Defaults to 'cpu'.\n",
    "        model_name (str, optional): Name of the model for logging purposes. Defaults to \"Model\".\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Saliency Analysis for {model_name} ---\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Output visualizations will be saved in: {output_dir}\")\n",
    "    print(f\"Processing up to {num_images_to_process} images.\")\n",
    "    print(f\"Using sigmas: {sigma_list}\")\n",
    "\n",
    "    processed_count = 0\n",
    "    model.eval() # Ensure model is in evaluation mode\n",
    "\n",
    "    # Wrap the dataloader with tqdm for overall progress\n",
    "    test_iterator = tqdm(\n",
    "        dataloader,\n",
    "        total=min(num_images_to_process, len(dataloader)),\n",
    "        desc=f\"{model_name} Progress\"\n",
    "    )\n",
    "\n",
    "    for i, (images, labels) in enumerate(test_iterator):\n",
    "        if processed_count >= num_images_to_process:\n",
    "            print(f\"\\nReached limit of {num_images_to_process} images for {model_name}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        # Since BATCH_SIZE=1, images and labels contain single items\n",
    "        image_tensor = images[0].to(device) # Get the image tensor, move to device\n",
    "        label = labels[0]                   # Get the corresponding label tensor\n",
    "\n",
    "        print(f\"\\nProcessing image {processed_count + 1}/{num_images_to_process} (DataLoader index: {i}) for {model_name}\")\n",
    "\n",
    "        # 1. Get Original Model Score\n",
    "        with torch.no_grad():\n",
    "            original_output = model(image_tensor.unsqueeze(0))\n",
    "            if isinstance(original_output, tuple) and len(original_output) > 0:\n",
    "                original_score_tensor = original_output[0]\n",
    "            else:\n",
    "                original_score_tensor = original_output\n",
    "            original_score = original_score_tensor.item()\n",
    "\n",
    "        true_authenticity = label.item()\n",
    "        print(f\"  True Authenticity: {true_authenticity:.4f}\")\n",
    "        print(f\"  {model_name} Predicted Authenticity: {original_score:.4f}\")\n",
    "\n",
    "        if not sigma_list:\n",
    "             print(\"  Warning: No sigma values provided. Skipping saliency calculation.\")\n",
    "             processed_count += 1\n",
    "             test_iterator.set_postfix_str(f\"Image {processed_count}/{num_images_to_process} (Skipped)\")\n",
    "             continue\n",
    "\n",
    "        # 3. Calculate Saliency Map\n",
    "        print(f\"  Calculating saliency map...\")\n",
    "        saliency_map_np = calculate_saliency_map(\n",
    "            model=model,\n",
    "            image=image_tensor,\n",
    "            original_score=original_score,\n",
    "            sigma_list=sigma_list,\n",
    "            mask_value=mask_value\n",
    "            # Note: calculate_saliency_map implicitly uses the device the model/image are on\n",
    "        )\n",
    "        print(f\"  Saliency map calculated with shape: {saliency_map_np.shape}\")\n",
    "\n",
    "        # 4. Visualization and Saving\n",
    "        filename_prefix = f\"image_{processed_count:03d}_auth_{true_authenticity:.2f}_pred_{original_score:.2f}\"\n",
    "        print(f\"  Visualizing and saving results with prefix: {filename_prefix}...\")\n",
    "\n",
    "        image_to_visualize = image_tensor.cpu() # Ensure tensor is on CPU for visualization\n",
    "\n",
    "        visualize_and_save_saliency(\n",
    "            image_tensor=image_to_visualize,\n",
    "            saliency_map=saliency_map_np,\n",
    "            output_dir=output_dir, # Use the function's output_dir parameter\n",
    "            filename_prefix=filename_prefix,\n",
    "            overlay_alpha=vis_alpha,\n",
    "            cmap_name=vis_cmap\n",
    "            # mean/std are defaults in visualize_and_save_saliency\n",
    "        )\n",
    "        print(f\"  Visualization saved.\")\n",
    "\n",
    "        processed_count += 1\n",
    "        test_iterator.set_postfix_str(f\"Image {processed_count}/{num_images_to_process}\")\n",
    "\n",
    "    test_iterator.close()\n",
    "    print(f\"\\n--- Saliency Analysis for {model_name} Finished ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Main Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration (remains mostly the same) ---\n",
    "NUM_IMAGES_TO_PROCESS = 5 # Number of images to process from the test set\n",
    "# Sigma generation parameters (keep only one method active)\n",
    "# SIGMA_PERCENTAGES = [0.02, 0.05, 0.1, 0.15, 0.2] # Example if using percentages\n",
    "SIGMA_LIST = [3, 5, 9, 17, 33, 65] # Fixed list\n",
    "# MIN_SIGMA = 5 # Only needed if SIGMA_PERCENTAGES is used\n",
    "MASK_VALUE = 0.0 # Value for occluded pixels\n",
    "VIS_CMAP = 'bwr' # Colormap for visualization\n",
    "VIS_ALPHA = 0.6 # Overlay opacity\n",
    "\n",
    "# Define output directories\n",
    "baseline_output_dir = 'mask_saliency_visualizations_baseline'\n",
    "pruned_output_dir = 'mask_saliency_visualizations_pruned'\n",
    "\n",
    "\n",
    "# Run for Baseline Model\n",
    "run_saliency_analysis(\n",
    "    model=baseline_model,\n",
    "    dataloader=test_dataloader,\n",
    "    output_dir=baseline_output_dir,\n",
    "    num_images_to_process=NUM_IMAGES_TO_PROCESS,\n",
    "    sigma_list=SIGMA_LIST, # Pass the chosen sigma list\n",
    "    mask_value=MASK_VALUE,\n",
    "    vis_cmap=VIS_CMAP,\n",
    "    vis_alpha=VIS_ALPHA,\n",
    "    device=device,\n",
    "    model_name=\"Baseline Model\"\n",
    ")\n",
    "\n",
    "# Run for Pruned Model\n",
    "run_saliency_analysis(\n",
    "    model=pruned_model,\n",
    "    dataloader=test_dataloader,\n",
    "    output_dir=pruned_output_dir,\n",
    "    num_images_to_process=NUM_IMAGES_TO_PROCESS,\n",
    "    sigma_list=SIGMA_LIST, # Pass the same sigma list\n",
    "    mask_value=MASK_VALUE,\n",
    "    vis_cmap=VIS_CMAP,\n",
    "    vis_alpha=VIS_ALPHA,\n",
    "    device=device,\n",
    "    model_name=\"Pruned Model\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- All Saliency Analyses Completed ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
