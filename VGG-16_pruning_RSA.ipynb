{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import VGG16_Weights\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LAST_CONV_IDX = 29  # Index of last convolutional layer in VGG16\n",
    "NUM_CHANNELS = 512  # Number of channels in VGG16 last conv layer\n",
    "NUM_CLASSES = 2  # Number of output classes for our task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageQualityDataset(Dataset):\n",
    "    \"\"\"Dataset for image quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its labels by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (image, labels) where:\n",
    "                image (PIL.Image): The image.\n",
    "                labels (torch.Tensor): Tensor containing quality and authenticity scores.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(os.getcwd(), self.data.iloc[idx, 3])  # image_path column\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        quality = self.data.iloc[idx, 0]  # Quality column\n",
    "        authenticity = self.data.iloc[idx, 1]  # Authenticity column\n",
    "        labels = torch.tensor([quality, authenticity], dtype=torch.float)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_Feature_Extractor(nn.Module):\n",
    "    \"\"\"VGG16 model for image quality assessment with pruning capability.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        \"\"\"\n",
    "        Initializes the VGG16 model for image quality assessment.\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): Number of output classes.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16 model\n",
    "        self.features = models.vgg16(weights=VGG16_Weights.DEFAULT).features\n",
    "\n",
    "        # Freeze all feature extraction layers\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Store the last convolutional layer output index\n",
    "        self.last_conv_idx = LAST_CONV_IDX\n",
    "\n",
    "        # Classifier layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(NUM_CHANNELS * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        \n",
    "        self.final = nn.Linear(1000, num_classes)\n",
    "            \n",
    "    def forward(self, x, conv_masks=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model with optional channel pruning.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            conv_masks (torch.Tensor, optional): Binary mask to apply to last conv layer channels.\n",
    "                Should be shape (512,) for VGG16 where 1 keeps channel and 0 prunes it.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Tuple containing feature outputs at different stages\n",
    "        \"\"\"\n",
    "        # Process through the feature layers up to the last convolutional layer\n",
    "        for i in range(self.last_conv_idx + 1):\n",
    "            x = self.features[i](x)\n",
    "            \n",
    "            # Apply channel pruning to the last convolutional layer output\n",
    "            if i == self.last_conv_idx and conv_masks is not None:\n",
    "                # Apply mask to prune channels - expanding to match dimensions\n",
    "                mask = conv_masks.view(1, -1, 1, 1).to(x.device)\n",
    "                x = x * mask\n",
    "        \n",
    "        # Continue through the remaining feature layers\n",
    "        for i in range(self.last_conv_idx + 1, len(self.features)):\n",
    "            x = self.features[i](x)\n",
    "            \n",
    "        # Store last_conv output\n",
    "        last_conv = x\n",
    "        \n",
    "        # Continue through fully connected layers\n",
    "        flattened = last_conv.view(last_conv.size(0), -1)\n",
    "        fc1_out = self.fc1(flattened)\n",
    "        fc2_out = self.fc2(fc1_out)\n",
    "        fc3_out = self.fc3(fc2_out)\n",
    "        output = self.final(fc3_out)\n",
    "        \n",
    "        return last_conv, fc1_out, fc2_out, fc3_out, output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader, channels_to_prune=None):\n",
    "    \"\"\"\n",
    "    Extract features from the model with optional pruning of the last conv layer.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to extract features from\n",
    "        dataloader: DataLoader containing the data\n",
    "        channels_to_prune: List of channel indices to prune (set to zero) in the last conv layer (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing extracted features from different layers and labels\n",
    "    \"\"\"\n",
    "    # Create channel mask (1 = keep, 0 = prune)\n",
    "    if channels_to_prune is None:\n",
    "        conv_masks = torch.ones(NUM_CHANNELS)\n",
    "    else:\n",
    "        conv_masks = torch.ones(NUM_CHANNELS)\n",
    "        conv_masks[channels_to_prune] = 0\n",
    "    \n",
    "    # Pre-allocate lists for features and labels\n",
    "    last_conv_features = []\n",
    "    fc1_features = []\n",
    "    fc2_features = []\n",
    "    fc3_features = []\n",
    "    output_features = []\n",
    "    labels_list = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            # Move data to device\n",
    "            inputs = inputs.to(model.features[0].weight.device)\n",
    "            \n",
    "            # Forward pass with pruning mask\n",
    "            last_conv, fc1, fc2, fc3, output = model(inputs, conv_masks)\n",
    "            \n",
    "            # Collect features from different layers\n",
    "            last_conv_features.append(last_conv.cpu().numpy())\n",
    "            fc1_features.append(fc1.cpu().numpy())\n",
    "            fc2_features.append(fc2.cpu().numpy())\n",
    "            fc3_features.append(fc3.cpu().numpy())\n",
    "            output_features.append(output.cpu().numpy())\n",
    "            labels_list.append(targets.numpy())\n",
    "    \n",
    "    # Concatenate batched features\n",
    "    features_dict = {\n",
    "        'last_conv': np.concatenate(last_conv_features),\n",
    "        'fc1': np.concatenate(fc1_features),\n",
    "        'fc2': np.concatenate(fc2_features),\n",
    "        'fc3': np.concatenate(fc3_features),\n",
    "        'output': np.concatenate(output_features),\n",
    "        'labels': np.concatenate(labels_list)\n",
    "    }\n",
    "    \n",
    "    return features_dict\n",
    "\n",
    "# Cell 6: Similarity and Correlation Analysis Functions\n",
    "def compute_similarity_matrix(features):\n",
    "    \"\"\"\n",
    "    Compute a similarity matrix from feature embeddings.\n",
    "    Works with both convolutional features (4D) and FC features (2D).\n",
    "    \n",
    "    Args:\n",
    "        features: numpy array - either shape (n_samples, n_channels, height, width)\n",
    "                 or shape (n_samples, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        similarity_matrix: numpy array of shape (n_samples, n_samples)\n",
    "    \"\"\"\n",
    "    # Check the dimensionality of features\n",
    "    n_samples = features.shape[0]\n",
    "    \n",
    "    # If features are from convolutional layer (4D), reshape to 2D\n",
    "    if len(features.shape) == 4:\n",
    "        features_flat = features.reshape(n_samples, -1)\n",
    "    else:\n",
    "        # Features are already 2D (from FC layer)\n",
    "        features_flat = features\n",
    "    \n",
    "    # Compute cosine similarity between all pairs\n",
    "    similarity_matrix = cosine_similarity(features_flat)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def compute_quality_difference_matrix(quality_scores):\n",
    "    \"\"\"\n",
    "    Compute a matrix of quality differences between all pairs of samples.\n",
    "    \n",
    "    Args:\n",
    "        quality_scores: numpy array of shape (n_samples,) containing quality scores\n",
    "        \n",
    "    Returns:\n",
    "        difference_matrix: numpy array of shape (n_samples, n_samples)\n",
    "    \"\"\"\n",
    "    n_samples = quality_scores.shape[0]\n",
    "    difference_matrix = np.zeros((n_samples, n_samples))\n",
    "    \n",
    "    # Compute absolute differences between all pairs\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            difference_matrix[i, j] = abs(quality_scores[i] - quality_scores[j])\n",
    "            \n",
    "    return difference_matrix\n",
    "\n",
    "def get_upper_triangle(matrix):\n",
    "    \"\"\"\n",
    "    Extract the upper triangle of a matrix (excluding diagonal).\n",
    "    \n",
    "    Args:\n",
    "        matrix: numpy array of shape (n, n)\n",
    "        \n",
    "    Returns:\n",
    "        upper_triangle: flattened upper triangle values\n",
    "    \"\"\"\n",
    "    indices = np.triu_indices_from(matrix, k=1)\n",
    "    return matrix[indices]\n",
    "\n",
    "def calculate_fit(similarity_matrix, quality_diff_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the fit between similarity and quality difference matrices.\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: numpy array of shape (n_samples, n_samples)\n",
    "        quality_diff_matrix: numpy array of shape (n_samples, n_samples)\n",
    "        \n",
    "    Returns:\n",
    "        correlation: Spearman correlation coefficient between the matrices\n",
    "        p_value: p-value of the correlation\n",
    "    \"\"\"\n",
    "    # Extract upper triangles (excluding diagonal)\n",
    "    sim_upper = get_upper_triangle(similarity_matrix)\n",
    "    qual_upper = get_upper_triangle(quality_diff_matrix)\n",
    "    \n",
    "    # Compute correlation (negative since higher similarity should correspond to lower difference)\n",
    "    correlation, p_value = spearmanr(sim_upper, qual_upper)\n",
    "    \n",
    "    # We're expecting a negative correlation (higher similarity â†’ lower quality difference)\n",
    "    # so we return the negative correlation value for easier interpretation\n",
    "    return -correlation, p_value\n",
    "\n",
    "def extract_quality_scores(labels):\n",
    "    \"\"\"\n",
    "    Extract quality scores from the label tensor.\n",
    "    \n",
    "    Args:\n",
    "        labels: numpy array of shape (n_samples, 2) where the first column is quality\n",
    "        \n",
    "    Returns:\n",
    "        quality_scores: numpy array of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    return labels[:, 0]  # Assuming first column contains quality scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel pruning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_channel_impact(model, dataloader, num_channels=NUM_CHANNELS):\n",
    "    \"\"\"\n",
    "    Analyze how pruning each channel in the last convolutional layer impacts \n",
    "    the feature quality in the fc2 layer.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to analyze\n",
    "        dataloader: DataLoader containing the data\n",
    "        num_channels: Number of channels in the last convolutional layer\n",
    "        \n",
    "    Returns:\n",
    "        channel_impacts: numpy array of shape (num_channels) containing channel index and impact scores\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing impact of pruning last_conv channels on fc2 features...\")\n",
    "    # Check cached list\n",
    "    base_path = 'Ranking_arrays'\n",
    "    if os.path.exists(f\"{base_path}/sim_matrix_channel_importance.npy\"):\n",
    "        print(\"Loading cached channel impacts...\")\n",
    "        return np.load(f\"{base_path}/sim_matrix_channel_importance.npy\", allow_pickle=True)\n",
    "    \n",
    "    # First, get baseline features with no pruning\n",
    "    baseline_features = extract_features(model, dataloader)\n",
    "    \n",
    "    # Extract quality scores and compute baseline fit for fc2\n",
    "    quality_scores = extract_quality_scores(baseline_features['labels'])\n",
    "    quality_diff = compute_quality_difference_matrix(quality_scores)\n",
    "    fc2_similarity = compute_similarity_matrix(baseline_features['fc2'])\n",
    "    baseline_fit, _ = calculate_fit(fc2_similarity, quality_diff)\n",
    "    \n",
    "    print(f\"Baseline fit (no pruning): {baseline_fit:.4f}\")\n",
    "    \n",
    "    # Analyze each channel's impact by pruning it and measuring fc2 fit\n",
    "    channel_impacts = []\n",
    "    \n",
    "    for channel_idx in tqdm(range(num_channels), desc=\"Pruning channels\"):\n",
    "        # Prune one channel at a time\n",
    "        pruned_features = extract_features(\n",
    "            model, \n",
    "            dataloader, \n",
    "            channels_to_prune=[channel_idx] # Prune one channel at a time\n",
    "        )\n",
    "        \n",
    "        # Compute fit for pruned features\n",
    "        fc2_pruned_similarity = compute_similarity_matrix(pruned_features['fc2'])\n",
    "        pruned_fit, _ = calculate_fit(fc2_pruned_similarity, quality_diff)\n",
    "        print(f\"Pruned channel {channel_idx}: {pruned_fit:.4f}\")\n",
    "        \n",
    "        # Calculate impact: positive means removing the channel improves the fc2 fit\n",
    "        channel_impact = pruned_fit - baseline_fit\n",
    "        channel_impacts.append([channel_idx, channel_impact])\n",
    "        print(f\"Channel {channel_idx} impact: {channel_impact:.4f}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "    \n",
    "    # Summarize results\n",
    "    print(\"\\nChannel Pruning Impact Analysis Results:\")\n",
    "    print(f\"Baseline fit (no pruning): {baseline_fit:.4f}\")\n",
    "    \n",
    "    # Sort the channel impacts by the impact score\n",
    "    channel_impacts = np.array(channel_impacts)\n",
    "    channel_impacts = channel_impacts[np.argsort(channel_impacts[:, 1])]\n",
    "    \n",
    "    return channel_impacts\n",
    "\n",
    "def evaluate_multi_channel_pruning(model, dataloader, channels_to_prune):\n",
    "    \"\"\"\n",
    "    Evaluate the impact of pruning multiple channels together on fc2 features.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to analyze\n",
    "        dataloader: DataLoader containing the data\n",
    "        channels_to_prune: List of channel indices to prune\n",
    "        \n",
    "    Returns:\n",
    "        fc2_fit: Fit of fc2 features after pruning\n",
    "    \"\"\"\n",
    "    # Extract features with specified pruning\n",
    "    pruned_features = extract_features(\n",
    "        model, \n",
    "        dataloader, \n",
    "        channels_to_prune=channels_to_prune\n",
    "    )\n",
    "    \n",
    "    # Compute fit for pruned features\n",
    "    quality_scores = extract_quality_scores(pruned_features['labels'])\n",
    "    quality_diff = compute_quality_difference_matrix(quality_scores)\n",
    "    fc2_pruned_similarity = compute_similarity_matrix(pruned_features['fc2'])\n",
    "    fc2_fit, p_value = calculate_fit(fc2_pruned_similarity, quality_diff)\n",
    "    \n",
    "    return fc2_fit\n",
    "\n",
    "def find_optimal_pruning(model, dataloader, channel_impacts, method='greedy', original_fit=None):\n",
    "    \"\"\"\n",
    "    Find the optimal set of last_conv channels to prune to maximize fc2 fit.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to analyze\n",
    "        dataloader: DataLoader containing the data\n",
    "        channel_impacts: numpy array of impacts from pruning each channel\n",
    "        original_fit: Baseline fit with no pruning\n",
    "        method: Pruning strategy ('greedy' or 'threshold')\n",
    "        \n",
    "    Returns:\n",
    "        optimal_channels: List of channels to prune\n",
    "        optimal_fit: Fit achieved after pruning\n",
    "        fit_history: History of fit improvements (for greedy method)\n",
    "    \"\"\"\n",
    "    if original_fit is None:\n",
    "        # First, get baseline features with no pruning\n",
    "        baseline_features = extract_features(model, dataloader)\n",
    "        \n",
    "        # Extract quality scores and compute baseline fit for fc2\n",
    "        quality_scores = extract_quality_scores(baseline_features['labels'])\n",
    "        quality_diff = compute_quality_difference_matrix(quality_scores)\n",
    "        fc2_similarity = compute_similarity_matrix(baseline_features['fc2'])\n",
    "        baseline_fit, _ = calculate_fit(fc2_similarity, quality_diff)\n",
    "        \n",
    "    print(f\"Baseline fit (no pruning): {baseline_fit:.4f}\")\n",
    "    fit_history = []\n",
    "    \n",
    "    if method == 'threshold':\n",
    "        # Prune all channels with positive impact\n",
    "        channels_to_prune = np.where(channel_impacts > 0)[0].tolist()\n",
    "        print(f\"Threshold method selected {len(channels_to_prune)} channels to prune\")\n",
    "        \n",
    "        # Evaluate pruning\n",
    "        prune_fit = evaluate_multi_channel_pruning(model, dataloader, channels_to_prune)\n",
    "        \n",
    "    elif method == 'greedy':\n",
    "        # Start with no pruning\n",
    "        channels_to_prune = []\n",
    "        \n",
    "        best_fit = baseline_fit\n",
    "        \n",
    "        # Sort channels by descending impact\n",
    "        sorted_channels = np.argsort(channel_impacts)[::-1]\n",
    "        \n",
    "        # Greedily add channels to prune until fit no longer improves\n",
    "        for i in tqdm(range(len(sorted_channels)), desc=\"Greedy pruning\"):\n",
    "            test_channels = channels_to_prune + [sorted_channels[i]]\n",
    "            test_fit = evaluate_multi_channel_pruning(model, dataloader, test_channels)\n",
    "            \n",
    "            if test_fit > best_fit:\n",
    "                print(f\"Adding channel {sorted_channels[i]} to prune list improved fit to {test_fit:.4f}\")\n",
    "                channels_to_prune = test_channels\n",
    "                best_fit = test_fit\n",
    "                fit_history.append(test_fit)\n",
    "            \n",
    "        prune_fit = best_fit\n",
    "        print(f\"Greedy search found {len(channels_to_prune)} channels to prune\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    return channels_to_prune, prune_fit, fit_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_environment():\n",
    "    \"\"\"Initialize device, transformations, and datasets\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Data transformations\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Dataset setup\n",
    "    ANNOTATIONS_FILE = 'Dataset/AIGCIQA2023/mos_data.csv'\n",
    "    dataset = ImageQualityDataset(csv_file=ANNOTATIONS_FILE, transform=data_transforms)\n",
    "    \n",
    "    # Split the dataset\n",
    "    TRAIN_RATIO = 0.8\n",
    "    TEST_RATIO = 0.2\n",
    "    train_size = int(TRAIN_RATIO * len(dataset))\n",
    "    test_size = int(TEST_RATIO * len(dataset))\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_WORKERS = 10\n",
    "    analysis_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    return device, analysis_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Setup environment\n",
    "    device, analysis_dataloader, test_dataloader = setup_environment()\n",
    "    \n",
    "    # Create model\n",
    "    model = VGG16_Feature_Extractor(num_classes=NUM_CLASSES).to(device)\n",
    "    \n",
    "    # Analyze channel importance for fc2 fit\n",
    "    channel_impacts = compute_channel_impact(\n",
    "        model, \n",
    "        analysis_dataloader,\n",
    "        num_channels=NUM_CHANNELS\n",
    "    )\n",
    "    \n",
    "    # Save channel impacts if needed\n",
    "    # np.save('Ranking_arrays/sim_matrix_channel_importance.npy', channel_impacts)\n",
    "    \n",
    "    return model, analysis_dataloader, test_dataloader, channel_impacts\n",
    "\n",
    "# Execute main function to initialize variables\n",
    "model, analysis_dataloader, test_dataloader, channel_impacts = main()\n",
    "\n",
    "# Cell 10: Threshold-based Pruning Evaluation\n",
    "# Find channels to prune using threshold method\n",
    "negative_impact_pruning_set, neg_impact_pruned_fit, _ = find_optimal_pruning(\n",
    "    model, \n",
    "    analysis_dataloader, \n",
    "    channel_impacts[:, 1], \n",
    "    method='threshold'\n",
    ")\n",
    "\n",
    "print(f\"Negative impact pruning set: {negative_impact_pruning_set}\")\n",
    "print(f\"Negative impact pruning fit: {neg_impact_pruned_fit:.4f}\")\n",
    "\n",
    "# Save the negative impact pruning set\n",
    "np.save('Pruning_sets/negative_impact_pruning_set.npy', negative_impact_pruning_set)\n",
    "\n",
    "# Cell 11: Greedy Pruning Evaluation\n",
    "# Find channels to prune using greedy method\n",
    "greedy_search_pruning_set, greedy_pruned_fit, fit_history = find_optimal_pruning(\n",
    "    model, \n",
    "    analysis_dataloader, \n",
    "    channel_impacts[:, 1], \n",
    "    method='greedy'\n",
    ")\n",
    "\n",
    "print(f\"Greedy search pruning set: {greedy_search_pruning_set}\")\n",
    "print(f\"Greedy search pruning fit: {greedy_pruned_fit:.4f}\")\n",
    "print(f\"Greedy search fit history: {fit_history}\")\n",
    "\n",
    "# Cell 12: Model Evaluation with Different Pruning Strategies\n",
    "# Evaluate base model (no pruning)\n",
    "base_model = VGG16_Feature_Extractor(num_classes=NUM_CLASSES).to(device)\n",
    "base_model_eval = evaluate_multi_channel_pruning(\n",
    "    base_model,\n",
    "    test_dataloader,\n",
    "    channels_to_prune=None\n",
    ")\n",
    "del base_model\n",
    "\n",
    "# Evaluate model with negative impact pruning\n",
    "base_model_2 = VGG16_Feature_Extractor(num_classes=NUM_CLASSES).to(device)\n",
    "channels_to_prune = np.load('Ranking_arrays/sim_matrix_channel_importance.npy', allow_pickle=True)\n",
    "# Pick channels_to_prune[:,0] where channels_to_prune[:,1] is greater than 0\n",
    "negative_impact_channels_to_prune = channels_to_prune[channels_to_prune[:,1] > 0][:,0]\n",
    "# Transform the float64 to int\n",
    "negative_impact_channels_to_prune = negative_impact_channels_to_prune.astype(int)\n",
    "\n",
    "neg_impact_pruning_eval = evaluate_multi_channel_pruning(\n",
    "    base_model_2, \n",
    "    test_dataloader, \n",
    "    negative_impact_channels_to_prune\n",
    ")\n",
    "del base_model_2\n",
    "\n",
    "# Evaluate model with greedy pruning\n",
    "base_model_3 = VGG16_Feature_Extractor(num_classes=NUM_CLASSES).to(device)\n",
    "greedy_impact_channels_to_prune = np.load('Pruning_sets/RSA_greedy_search_pruning_set.npy', allow_pickle=True)\n",
    "\n",
    "greedy_impact_pruning_eval = evaluate_multi_channel_pruning(\n",
    "    base_model_3,\n",
    "    test_dataloader,\n",
    "    greedy_impact_channels_to_prune\n",
    ")\n",
    "\n",
    "print(f\"Base model fit: {base_model_eval:.4f}\")\n",
    "print(f\"Negative impact pruning fit: {neg_impact_pruning_eval:.4f}\")\n",
    "print(f\"Greedy search pruning fit: {greedy_impact_pruning_eval:.4f}\")\n",
    "\n",
    "# Cell 13: Visualization of Pruning Performance\n",
    "# Load pruning search history\n",
    "pruning_search_history = np.load('Fit_histories/RSA_greedy_search_fit_history.npy')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pruning_search_history, label='Greedy Search')\n",
    "plt.xlabel('Number of Channels Pruned')\n",
    "plt.ylabel('Fit Score')\n",
    "plt.title('Greedy Search Pruning Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
